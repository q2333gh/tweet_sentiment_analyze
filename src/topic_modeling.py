"""
ä¸»é¢˜å»ºæ¨¡æ¨¡å— - LDAä¸»é¢˜å»ºæ¨¡
å¯¹åº”notebook: 4_topic_modeling_lda.ipynb
"""
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from typing import Optional, Dict, Any, List
import pyspark.sql.functions as F
from pyspark.sql.types import BooleanType
from pyspark.ml.feature import CountVectorizer, IDF
from pyspark.ml.clustering import LDA
from wordcloud import WordCloud
from .config import DATA_PATHS, TOPIC_MODELING, CLIMATE_KEYWORDS
from .utils import (create_spark_session, setup_logging, create_climate_filter_udf, 
                   get_topic_words_from_model, print_data_info)

logger = setup_logging()

class TopicModeler:
    """ä¸»é¢˜å»ºæ¨¡å™¨"""
    
    def __init__(self, input_path: Optional[str] = None, output_path: Optional[str] = None):
        """
        åˆå§‹åŒ–ä¸»é¢˜å»ºæ¨¡å™¨
        
        Args:
            input_path: è¾“å…¥æ•°æ®è·¯å¾„
            output_path: è¾“å‡ºæ•°æ®è·¯å¾„
        """
        self.input_path = input_path or DATA_PATHS['sentiment_analyzed']
        self.output_path = output_path or DATA_PATHS['topic_analyzed']
        self.spark = create_spark_session("TopicModeling")
        self.df = None
        self.lda_model = None
        self.vocabulary = None
        self.topic_words = None
        
    def load_data(self) -> bool:
        """
        åŠ è½½æƒ…æ„Ÿåˆ†æåçš„æ•°æ®
        
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"åŠ è½½æƒ…æ„Ÿåˆ†ææ•°æ®: {self.input_path}")
            
            # å°è¯•åŠ è½½æƒ…æ„Ÿåˆ†æç»“æœ
            try:
                self.df = self.spark.read.parquet(self.input_path)
            except:
                # å¤‡é€‰æ–¹æ¡ˆï¼šåŠ è½½æ¸…æ´—åçš„æ•°æ®
                logger.warning("æƒ…æ„Ÿåˆ†ææ•°æ®åŠ è½½å¤±è´¥ï¼Œå°è¯•åŠ è½½æ¸…æ´—åæ•°æ®...")
                self.df = self.spark.read.parquet(DATA_PATHS['cleaned_data'])
            
            self.df.cache()
            record_count = self.df.count()
            logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {record_count:,} æ¡è®°å½•")
            
            print("\næ•°æ®ç»“æ„:")
            self.df.printSchema()
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'tokens_cleaned' not in self.df.columns:
                logger.error("âŒ æœªæ‰¾åˆ°tokens_cleanedåˆ—ï¼Œéœ€è¦é‡æ–°åˆ†è¯")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def preprocess_for_topic_modeling(self) -> bool:
        """
        ä¸ºä¸»é¢˜å»ºæ¨¡é¢„å¤„ç†æ•°æ®
        
        Returns:
            bool: é¢„å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        if self.df is None:
            logger.error("è¯·å…ˆåŠ è½½æ•°æ®")
            return False
        
        logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # 1. è¿‡æ»¤è¯æ±‡æ•°é‡è¿‡å°‘çš„æ–‡æ¡£
        min_tokens = TOPIC_MODELING.get('min_tokens', 5)
        df_filtered = self.df.filter(F.size(F.col("tokens_cleaned")) >= min_tokens)
        
        filtered_count = df_filtered.count()
        original_count = self.df.count()
        logger.info(f"è¿‡æ»¤åæ•°æ®é‡: {filtered_count:,} æ¡è®°å½•")
        logger.info(f"ä¿ç•™æ¯”ä¾‹: {filtered_count/original_count*100:.1f}%")
        
        # 2. è¿‡æ»¤Climate Changeç›¸å…³å…³é”®è¯
        climate_filter_udf = create_climate_filter_udf()
        df_climate = df_filtered.filter(climate_filter_udf(F.col("tokens_cleaned")))
        
        climate_count = df_climate.count()
        logger.info(f"Climateç›¸å…³è¯„è®º: {climate_count:,} æ¡è®°å½•")
        logger.info(f"å è¿‡æ»¤åæ•°æ®: {climate_count/filtered_count*100:.1f}%")
        
        # 3. é‡‡æ ·æ•°æ®ä»¥å‡å°‘è®¡ç®—è´Ÿæ‹…
        sample_fraction = TOPIC_MODELING['sample_fraction']
        self.df = df_climate.sample(fraction=sample_fraction, seed=42)
        sample_count = self.df.count()
        
        logger.info(f"é‡‡æ ·åæ•°æ®é‡: {sample_count:,} æ¡è®°å½• (åŸæ•°æ®çš„ {sample_fraction*100}%)")
        
        # ç¼“å­˜æœ€ç»ˆç”¨äºå»ºæ¨¡çš„æ•°æ®
        self.df.cache()
        
        return True
    
    def build_feature_vectors(self) -> bool:
        """
        æ„å»ºç‰¹å¾å‘é‡
        
        Returns:
            bool: æ„å»ºæ˜¯å¦æˆåŠŸ
        """
        if self.df is None:
            logger.error("è¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†")
            return False
        
        logger.info("å¼€å§‹æ„å»ºç‰¹å¾å‘é‡...")
        
        # 1. CountVectorizerï¼šå°†tokensè½¬æ¢ä¸ºè¯é¢‘å‘é‡
        vocab_size = TOPIC_MODELING['vocab_size']
        min_df = TOPIC_MODELING['min_doc_freq']
        
        count_vectorizer = CountVectorizer(
            inputCol="tokens_cleaned", 
            outputCol="raw_features",
            vocabSize=vocab_size,
            minDF=min_df
        )
        
        logger.info("è®­ç»ƒCountVectorizer...")
        count_model = count_vectorizer.fit(self.df)
        df_vectorized = count_model.transform(self.df)
        
        # 2. TF-IDFï¼šè®¡ç®—è¯æ±‡é‡è¦æ€§æƒé‡
        idf = IDF(inputCol="raw_features", outputCol="features")
        logger.info("è®­ç»ƒIDF...")
        idf_model = idf.fit(df_vectorized)
        self.df = idf_model.transform(df_vectorized)
        
        # ä¿å­˜è¯æ±‡è¡¨
        self.vocabulary = count_model.vocabulary
        
        logger.info(f"âœ… ç‰¹å¾å‘é‡æ„å»ºå®Œæˆ")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}")
        logger.info(f"ç‰¹å¾å‘é‡ç»´åº¦: {len(self.vocabulary)}")
        
        # æ˜¾ç¤ºè¯æ±‡è¡¨ç¤ºä¾‹
        print("\nè¯æ±‡è¡¨ç¤ºä¾‹ï¼ˆå‰20ä¸ªè¯ï¼‰:")
        for i, word in enumerate(self.vocabulary[:20]):
            print(f"{i}: {word}")
        
        return True
    
    def train_lda_model(self) -> bool:
        """
        è®­ç»ƒLDAæ¨¡å‹
        
        Returns:
            bool: è®­ç»ƒæ˜¯å¦æˆåŠŸ
        """
        if self.df is None or self.vocabulary is None:
            logger.error("è¯·å…ˆæ„å»ºç‰¹å¾å‘é‡")
            return False
        
        logger.info("å¼€å§‹è®­ç»ƒLDAæ¨¡å‹...")
        
        # è®¾ç½®LDAå‚æ•°
        num_topics = TOPIC_MODELING['num_topics']
        max_iter = TOPIC_MODELING['max_iterations']
        
        # åˆ›å»ºLDAæ¨¡å‹
        lda = LDA(
            featuresCol="features", 
            topicsCol="topic_distribution",
            k=num_topics,
            maxIter=max_iter,
            seed=42
        )
        
        logger.info(f"è®­ç»ƒLDAæ¨¡å‹ï¼ˆ{num_topics}ä¸ªä¸»é¢˜ï¼Œæœ€å¤§è¿­ä»£{max_iter}æ¬¡ï¼‰...")
        self.lda_model = lda.fit(self.df)
        
        # åº”ç”¨æ¨¡å‹å¾—åˆ°ä¸»é¢˜åˆ†å¸ƒ
        self.df = self.lda_model.transform(self.df)
        
        logger.info("âœ… LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        logger.info(f"æ¨¡å‹å›°æƒ‘åº¦: {self.lda_model.logPerplexity(self.df):.2f}")
        logger.info(f"æ¨¡å‹å¯¹æ•°ä¼¼ç„¶: {self.lda_model.logLikelihood(self.df):.2f}")
        
        return True
    
    def extract_topic_keywords(self) -> bool:
        """
        æå–ä¸»é¢˜å…³é”®è¯
        
        Returns:
            bool: æå–æ˜¯å¦æˆåŠŸ
        """
        if self.lda_model is None or self.vocabulary is None:
            logger.error("è¯·å…ˆè®­ç»ƒLDAæ¨¡å‹")
            return False
        
        logger.info("å¼€å§‹æå–ä¸»é¢˜å…³é”®è¯...")
        
        max_terms = TOPIC_MODELING['max_terms_per_topic']
        self.topic_words = get_topic_words_from_model(self.lda_model, self.vocabulary, max_terms)
        
        # æ˜¾ç¤ºä¸»é¢˜å…³é”®è¯
        print("\n=== ä¸»é¢˜å…³é”®è¯åˆ—è¡¨ ===")
        for topic in self.topic_words:
            topic_id = topic['topic_id']
            words = topic['words'][:10]  # æ˜¾ç¤ºå‰10ä¸ªå…³é”®è¯
            weights = topic['weights'][:10]
            
            print(f"\nğŸ” ä¸»é¢˜ {topic_id}:")
            for word, weight in zip(words, weights):
                print(f"  {word}: {weight:.4f}")
        
        logger.info("âœ… ä¸»é¢˜å…³é”®è¯æå–å®Œæˆï¼")
        return True
    
    def analyze_topic_distribution(self) -> Dict[str, Any]:
        """
        åˆ†æä¸»é¢˜åˆ†å¸ƒ
        
        Returns:
            dict: ä¸»é¢˜åˆ†å¸ƒåˆ†æç»“æœ
        """
        if self.df is None or 'topic_distribution' not in self.df.columns:
            logger.error("è¯·å…ˆè®­ç»ƒLDAæ¨¡å‹")
            return {}
        
        logger.info("å¼€å§‹åˆ†æä¸»é¢˜åˆ†å¸ƒ...")
        
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ä¸»å¯¼ä¸»é¢˜
        def get_dominant_topic(topic_dist):
            if topic_dist is None:
                return -1
            return int(np.argmax(topic_dist.toArray()))
        
        from pyspark.sql.types import IntegerType
        dominant_topic_udf = F.udf(get_dominant_topic, IntegerType())
        
        self.df = self.df.withColumn("dominant_topic", dominant_topic_udf("topic_distribution"))
        
        # ç»Ÿè®¡å„ä¸»é¢˜çš„æ–‡æ¡£æ•°é‡
        topic_dist = self.df.groupBy("dominant_topic").count().orderBy("dominant_topic")
        topic_dist_pd = topic_dist.toPandas()
        
        print("\n=== ä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡ ===")
        print(topic_dist_pd)
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        total_docs = topic_dist_pd['count'].sum()
        topic_dist_pd['percentage'] = (topic_dist_pd['count'] / total_docs * 100).round(2)
        
        return {
            'topic_distribution': topic_dist_pd,
            'total_documents': total_docs
        }
    
    def analyze_topic_sentiment(self) -> Dict[str, Any]:
        """
        åˆ†æå„ä¸»é¢˜çš„æƒ…æ„Ÿåˆ†å¸ƒ
        
        Returns:
            dict: ä¸»é¢˜æƒ…æ„Ÿåˆ†æç»“æœ
        """
        if self.df is None or 'dominant_topic' not in self.df.columns:
            logger.error("è¯·å…ˆåˆ†æä¸»é¢˜åˆ†å¸ƒ")
            return {}
        
        logger.info("å¼€å§‹åˆ†æä¸»é¢˜æƒ…æ„Ÿåˆ†å¸ƒ...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æƒ…æ„Ÿåˆ†ç±»åˆ—
        sentiment_col = None
        if 'sentiment_category' in self.df.columns:
            sentiment_col = 'sentiment_category'
        elif 'vader_sentiment' in self.df.columns:
            # å¦‚æœæ²¡æœ‰åˆ†ç±»ï¼ŒåŸºäºVADERåˆ†æ•°åˆ›å»ºåˆ†ç±»
            def categorize_sentiment(score):
                if score is None:
                    return "Unknown"
                elif score > 0.05:
                    return "Positive"
                elif score < -0.05:
                    return "Negative"
                else:
                    return "Neutral"
            
            from pyspark.sql.types import StringType
            categorize_udf = F.udf(categorize_sentiment, StringType())
            self.df = self.df.withColumn("sentiment_category", categorize_udf("vader_sentiment"))
            sentiment_col = 'sentiment_category'
        
        if sentiment_col is None:
            logger.warning("æœªæ‰¾åˆ°æƒ…æ„Ÿåˆ†æç»“æœï¼Œè·³è¿‡ä¸»é¢˜æƒ…æ„Ÿåˆ†æ")
            return {}
        
        # ç»Ÿè®¡å„ä¸»é¢˜çš„æƒ…æ„Ÿåˆ†å¸ƒ
        topic_sentiment = self.df.groupBy("dominant_topic", sentiment_col) \
                               .count() \
                               .orderBy("dominant_topic", sentiment_col)
        
        topic_sentiment_pd = topic_sentiment.toPandas()
        
        print("\n=== ä¸»é¢˜æƒ…æ„Ÿåˆ†å¸ƒ ===")
        print(topic_sentiment_pd)
        
        return {
            'topic_sentiment': topic_sentiment_pd
        }
    
    def create_topic_visualizations(self, save_plots: bool = False) -> None:
        """
        åˆ›å»ºä¸»é¢˜å»ºæ¨¡å¯è§†åŒ–
        
        Args:
            save_plots: æ˜¯å¦ä¿å­˜å›¾è¡¨
        """
        if self.topic_words is None:
            logger.error("è¯·å…ˆæå–ä¸»é¢˜å…³é”®è¯")
            return
        
        logger.info("å¼€å§‹ç”Ÿæˆä¸»é¢˜å»ºæ¨¡å¯è§†åŒ–...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. ä¸»é¢˜å…³é”®è¯å¯è§†åŒ–
        num_topics = len(self.topic_words)
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, topic in enumerate(self.topic_words):
            if i >= 6:  # æœ€å¤šæ˜¾ç¤º6ä¸ªä¸»é¢˜
                break
                
            topic_id = topic['topic_id']
            words = topic['words'][:10]
            weights = topic['weights'][:10]
            
            ax = axes[i]
            y_pos = np.arange(len(words))
            ax.barh(y_pos, weights, align='center')
            ax.set_yticks(y_pos)
            ax.set_yticklabels(words)
            ax.invert_yaxis()
            ax.set_xlabel('æƒé‡')
            ax.set_title(f'ä¸»é¢˜ {topic_id} å…³é”®è¯')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(num_topics, 6):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        if save_plots:
            plt.savefig('topic_keywords.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # 2. ç”Ÿæˆå„ä¸»é¢˜çš„è¯äº‘
        for topic in self.topic_words[:3]:  # å‰3ä¸ªä¸»é¢˜
            topic_id = topic['topic_id']
            words = topic['words'][:20]
            weights = topic['weights'][:20]
            
            word_freq_dict = dict(zip(words, weights))
            
            wordcloud = WordCloud(width=800, height=400, 
                                background_color='white',
                                max_words=50).generate_from_frequencies(word_freq_dict)
            
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(f'ä¸»é¢˜ {topic_id} è¯äº‘å›¾')
            
            if save_plots:
                plt.savefig(f'topic_{topic_id}_wordcloud.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # 3. ä¸»é¢˜åˆ†å¸ƒå›¾
        topic_dist_data = self.analyze_topic_distribution()
        if topic_dist_data:
            plt.figure(figsize=(10, 6))
            
            # é¥¼å›¾
            plt.subplot(1, 2, 1)
            plt.pie(topic_dist_data['topic_distribution']['count'], 
                   labels=[f"ä¸»é¢˜ {i}" for i in topic_dist_data['topic_distribution']['dominant_topic']],
                   autopct='%1.1f%%', startangle=90)
            plt.title('ä¸»é¢˜åˆ†å¸ƒ')
            
            # æŸ±çŠ¶å›¾
            plt.subplot(1, 2, 2)
            sns.barplot(data=topic_dist_data['topic_distribution'], 
                       x='dominant_topic', y='count')
            plt.title('å„ä¸»é¢˜æ–‡æ¡£æ•°é‡')
            plt.xlabel('ä¸»é¢˜ID')
            plt.ylabel('æ–‡æ¡£æ•°é‡')
            
            plt.tight_layout()
            if save_plots:
                plt.savefig('topic_distribution.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # 4. ä¸»é¢˜æƒ…æ„Ÿåˆ†æå›¾
        topic_sentiment_data = self.analyze_topic_sentiment()
        if topic_sentiment_data and not topic_sentiment_data['topic_sentiment'].empty:
            # é€è§†è¡¨ç”¨äºçƒ­åŠ›å›¾
            pivot_table = topic_sentiment_data['topic_sentiment'].pivot(
                index='dominant_topic', 
                columns='sentiment_category', 
                values='count'
            ).fillna(0)
            
            plt.figure(figsize=(10, 8))
            sns.heatmap(pivot_table, annot=True, fmt='g', cmap='YlOrRd')
            plt.title('ä¸»é¢˜-æƒ…æ„Ÿåˆ†å¸ƒçƒ­åŠ›å›¾')
            plt.xlabel('æƒ…æ„Ÿç±»åˆ«')
            plt.ylabel('ä¸»é¢˜ID')
            plt.tight_layout()
            
            if save_plots:
                plt.savefig('topic_sentiment_heatmap.png', dpi=300, bbox_inches='tight')
            plt.show()
    
    def save_results(self) -> bool:
        """
        ä¿å­˜ä¸»é¢˜å»ºæ¨¡ç»“æœ
        
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if self.df is None or 'dominant_topic' not in self.df.columns:
            logger.error("è¯·å…ˆå®Œæˆä¸»é¢˜å»ºæ¨¡")
            return False
        
        try:
            logger.info(f"ä¿å­˜ä¸»é¢˜å»ºæ¨¡ç»“æœåˆ°: {self.output_path}")
            
            # é€‰æ‹©éœ€è¦ä¿å­˜çš„åˆ—
            columns_to_save = [
                "id", 
                "`subreddit.name`" if "`subreddit.name`" in self.df.columns else "subreddit.name",
                "created_utc", 
                "timestamp",
                "body", 
                "cleaned_body", 
                "tokens_cleaned",
                "sentiment",
                "vader_sentiment" if "vader_sentiment" in self.df.columns else None,
                "sentiment_category" if "sentiment_category" in self.df.columns else None,
                "topic_distribution",
                "dominant_topic",
                "score"
            ]
            
            # è¿‡æ»¤å­˜åœ¨çš„åˆ—
            available_columns = [col for col in columns_to_save if col and col in self.df.columns]
            
            df_final = self.df.select(*available_columns)
            
            # ä¿å­˜ä¸ºParquetæ ¼å¼
            df_final.write.mode("overwrite").parquet(self.output_path)
            
            final_count = df_final.count()
            logger.info(f"âœ… ä¸»é¢˜å»ºæ¨¡ç»“æœä¿å­˜å®Œæˆï¼å…± {final_count:,} æ¡è®°å½•")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    def execute_full_topic_modeling(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸»é¢˜å»ºæ¨¡æµç¨‹
        
        Returns:
            dict: å»ºæ¨¡æŠ¥å‘Š
        """
        logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´ä¸»é¢˜å»ºæ¨¡æµç¨‹...")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data():
            return {'success': False, 'error': 'æ•°æ®åŠ è½½å¤±è´¥'}
        
        # 2. æ•°æ®é¢„å¤„ç†
        if not self.preprocess_for_topic_modeling():
            return {'success': False, 'error': 'æ•°æ®é¢„å¤„ç†å¤±è´¥'}
        
        # 3. æ„å»ºç‰¹å¾å‘é‡
        if not self.build_feature_vectors():
            return {'success': False, 'error': 'ç‰¹å¾å‘é‡æ„å»ºå¤±è´¥'}
        
        # 4. è®­ç»ƒLDAæ¨¡å‹
        if not self.train_lda_model():
            return {'success': False, 'error': 'LDAæ¨¡å‹è®­ç»ƒå¤±è´¥'}
        
        # 5. æå–ä¸»é¢˜å…³é”®è¯
        if not self.extract_topic_keywords():
            return {'success': False, 'error': 'ä¸»é¢˜å…³é”®è¯æå–å¤±è´¥'}
        
        # 6. åˆ†æä¸»é¢˜åˆ†å¸ƒ
        topic_distribution = self.analyze_topic_distribution()
        topic_sentiment = self.analyze_topic_sentiment()
        
        # 7. ç”Ÿæˆå¯è§†åŒ–
        self.create_topic_visualizations(save_plots=True)
        
        # 8. ä¿å­˜ç»“æœ
        save_success = self.save_results()
        
        # ç”Ÿæˆå»ºæ¨¡æŠ¥å‘Š
        modeling_report = {
            'success': save_success,
            'num_topics': TOPIC_MODELING['num_topics'],
            'vocabulary_size': len(self.vocabulary) if self.vocabulary else 0,
            'topic_words': self.topic_words,
            'topic_distribution': topic_distribution,
            'topic_sentiment': topic_sentiment,
            'model_perplexity': self.lda_model.logPerplexity(self.df) if self.lda_model else None,
            'model_likelihood': self.lda_model.logLikelihood(self.df) if self.lda_model else None,
            'output_path': self.output_path
        }
        
        logger.info("âœ… å®Œæ•´ä¸»é¢˜å»ºæ¨¡æµç¨‹æ‰§è¡Œå®Œæˆï¼")
        return modeling_report
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.spark:
            self.spark.stop()
            logger.info("Spark Sessionå·²å…³é—­")

def perform_topic_modeling(input_path: Optional[str] = None, 
                          output_path: Optional[str] = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ‰§è¡Œä¸»é¢˜å»ºæ¨¡
    
    Args:
        input_path: è¾“å…¥æ•°æ®è·¯å¾„
        output_path: è¾“å‡ºæ•°æ®è·¯å¾„
        
    Returns:
        dict: å»ºæ¨¡æŠ¥å‘Š
    """
    modeler = TopicModeler(input_path, output_path)
    
    try:
        report = modeler.execute_full_topic_modeling()
        return report
    finally:
        modeler.cleanup()

if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ­¤æ¨¡å—æ—¶æ‰§è¡Œä¸»é¢˜å»ºæ¨¡
    logger.info("å¼€å§‹æ‰§è¡Œä¸»é¢˜å»ºæ¨¡...")
    report = perform_topic_modeling()
    
    if report.get('success', False):
        print(f"\nâœ… ä¸»é¢˜å»ºæ¨¡æˆåŠŸå®Œæˆï¼")
        print(f"ä¸»é¢˜æ•°é‡: {report['num_topics']}")
        print(f"è¯æ±‡è¡¨å¤§å°: {report['vocabulary_size']}")
        print(f"æ¨¡å‹å›°æƒ‘åº¦: {report.get('model_perplexity', 'N/A')}")
        
        # æ˜¾ç¤ºä¸»é¢˜å…³é”®è¯æ‘˜è¦
        if report.get('topic_words'):
            print("\nä¸»é¢˜å…³é”®è¯æ‘˜è¦:")
            for topic in report['topic_words'][:3]:
                topic_id = topic['topic_id']
                words = topic['words'][:5]
                print(f"  ä¸»é¢˜ {topic_id}: {', '.join(words)}")
    else:
        logger.error("âŒ ä¸»é¢˜å»ºæ¨¡å¤±è´¥")
        if 'error' in report:
            print(f"é”™è¯¯ä¿¡æ¯: {report['error']}") 