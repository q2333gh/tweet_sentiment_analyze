"""
é€šç”¨å·¥å…·å‡½æ•°æ¨¡å—
"""
import re
import os
import logging
from typing import Optional, List, Dict, Any

# å°è¯•å¯¼å…¥PySparkï¼Œå¦‚æœå¤±è´¥åˆ™è®¾ç½®æ ‡å¿—
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import udf
    from pyspark.sql.types import StringType, BooleanType
    PYSPARK_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  PySparkå¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ“ å°†ä½¿ç”¨pandasä½œä¸ºåå¤‡æ–¹æ¡ˆ")
    PYSPARK_AVAILABLE = False
    SparkSession = None
    udf = None
    StringType = None
    BooleanType = None

from .config import SPARK_CONFIG, CLIMATE_KEYWORDS

def create_spark_session(app_name: str) -> SparkSession:
    """
    åˆ›å»ºSpark Session
    
    Args:
        app_name: åº”ç”¨åç§°
        
    Returns:
        SparkSessionå¯¹è±¡
    """
    spark = SparkSession.builder \
        .appName(f"{SPARK_CONFIG['app_name_prefix']}_{app_name}") \
        .master(SPARK_CONFIG['master']) \
        .config("spark.driver.memory", SPARK_CONFIG['driver_memory']) \
        .config("spark.sql.execution.arrow.pyspark.enabled", SPARK_CONFIG['arrow_enabled']) \
        .getOrCreate()
    
    print(f"Spark Session created: {spark.version}")
    print(f"Available cores: {spark.sparkContext.defaultParallelism}")
    
    return spark

def setup_logging(log_level: str = "INFO") -> logging.Logger:
    """
    è®¾ç½®æ—¥å¿—è®°å½•
    
    Args:
        log_level: æ—¥å¿—çº§åˆ«
        
    Returns:
        Loggerå¯¹è±¡
    """
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def create_text_cleaning_udf():
    """
    åˆ›å»ºæ–‡æœ¬æ¸…æ´—çš„UDFå‡½æ•°
    
    Returns:
        PySpark UDFå‡½æ•°
    """
    def clean_text_func(text: Optional[str]) -> Optional[str]:
        if text is None:
            return None
        
        text = str(text)
        
        # 1. è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # 2. å»é™¤URL
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # 3. å»é™¤Redditç‰¹æœ‰çš„æ ¼å¼
        text = re.sub(r'/u/\w+', '', text)  # å»é™¤ç”¨æˆ·å
        text = re.sub(r'/r/\w+', '', text)  # å»é™¤å­ç‰ˆå—å
        text = re.sub(r'&gt;', '', text)   # å»é™¤å¼•ç”¨ç¬¦å·
        text = re.sub(r'&lt;', '', text)
        text = re.sub(r'&amp;', 'and', text)
        
        # 4. å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<.*?>', '', text)
        
        # 5. å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼å’ŒåŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^a-zA-Z0-9\s\.\,\!\?\;\:]', ' ', text)
        
        # 6. å»é™¤å¤šä½™çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 7. è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬
        if len(text) < 10:
            return None
            
        return text
    
    return udf(clean_text_func, StringType())

def create_climate_filter_udf():
    """
    åˆ›å»ºæ°”å€™å˜åŒ–å…³é”®è¯è¿‡æ»¤çš„UDFå‡½æ•°
    
    Returns:
        PySpark UDFå‡½æ•°
    """
    def contains_climate_keywords(tokens: Optional[List[str]]) -> bool:
        if tokens is None:
            return False
        tokens_lower = [token.lower() for token in tokens]
        return any(keyword in tokens_lower for keyword in CLIMATE_KEYWORDS)
    
    return udf(contains_climate_keywords, BooleanType())

def print_data_info(df, title: str = "æ•°æ®ä¿¡æ¯"):
    """
    æ‰“å°æ•°æ®æ¡†çš„åŸºæœ¬ä¿¡æ¯
    
    Args:
        df: Spark DataFrame
        title: æ ‡é¢˜
    """
    print(f"\n=== {title} ===")
    print(f"è®°å½•æ•°: {df.count():,}")
    print(f"åˆ—æ•°: {len(df.columns)}")
    print(f"åˆ—å: {df.columns}")
    print("\næ•°æ®ç»“æ„:")
    df.printSchema()

def calculate_data_completeness(df) -> Dict[str, float]:
    """
    è®¡ç®—æ•°æ®å®Œæ•´æ€§
    
    Args:
        df: Spark DataFrame
        
    Returns:
        æ¯åˆ—çš„å®Œæ•´æ€§ç™¾åˆ†æ¯”å­—å…¸
    """
    total_count = df.count()
    completeness = {}
    
    for col_name in df.columns:
        # å¤„ç†åŒ…å«ç‚¹å·çš„åˆ—å
        if "." in col_name:
            null_count = df.filter(df[f"`{col_name}`"].isNull()).count()
        else:
            null_count = df.filter(df[col_name].isNull()).count()
        
        completeness[col_name] = ((total_count - null_count) / total_count * 100)
    
    return completeness

def safe_divide(a: float, b: float) -> float:
    """
    å®‰å…¨é™¤æ³•ï¼Œé¿å…é™¤é›¶é”™è¯¯
    
    Args:
        a: è¢«é™¤æ•°
        b: é™¤æ•°
        
    Returns:
        é™¤æ³•ç»“æœï¼Œå¦‚æœé™¤æ•°ä¸º0åˆ™è¿”å›0
    """
    return a / b if b != 0 else 0.0

def format_percentage(value: float, decimal_places: int = 2) -> str:
    """
    æ ¼å¼åŒ–ç™¾åˆ†æ¯”
    
    Args:
        value: æ•°å€¼
        decimal_places: å°æ•°ä½æ•°
        
    Returns:
        æ ¼å¼åŒ–çš„ç™¾åˆ†æ¯”å­—ç¬¦ä¸²
    """
    return f"{value:.{decimal_places}f}%"

def ensure_directory_exists(path: str) -> None:
    """
    ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    
    Args:
        path: ç›®å½•è·¯å¾„
    """
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)
        print(f"Created directory: {path}")

def get_topic_words_from_model(lda_model, vocabulary: List[str], max_terms: int = 10) -> List[Dict[str, Any]]:
    """
    ä»LDAæ¨¡å‹ä¸­æå–ä¸»é¢˜å…³é”®è¯
    
    Args:
        lda_model: è®­ç»ƒå¥½çš„LDAæ¨¡å‹
        vocabulary: è¯æ±‡è¡¨
        max_terms: æ¯ä¸ªä¸»é¢˜çš„æœ€å¤§è¯æ±‡æ•°
        
    Returns:
        ä¸»é¢˜å…³é”®è¯åˆ—è¡¨
    """
    topics = lda_model.describeTopics(maxTermsPerTopic=max_terms)
    topics_list = []
    
    for row in topics.collect():
        topic_id = row['topic']
        term_indices = row['termIndices']
        term_weights = row['termWeights']
        
        # è½¬æ¢ç´¢å¼•ä¸ºè¯æ±‡
        words = [vocabulary[idx] for idx in term_indices]
        
        topics_list.append({
            'topic_id': topic_id,
            'words': words,
            'weights': term_weights
        })
    
    return topics_list

def print_classification_metrics(y_true, y_pred, target_names: Optional[List[str]] = None):
    """
    æ‰“å°åˆ†ç±»æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        target_names: ç±»åˆ«åç§°åˆ—è¡¨
    """
    from sklearn.metrics import classification_report, confusion_matrix
    
    print("\n=== åˆ†ç±»æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ ===")
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred, target_names=target_names))
    
    print("\næ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y_true, y_pred)
    print(cm)
    
    return cm 