"""
ä¸»è¿è¡Œè„šæœ¬ - æ•´åˆæ‰€æœ‰åˆ†ææµç¨‹
"""
import argparse
import sys
import os
from typing import Optional, Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œæ”¯æŒç›´æ¥è¿è¡Œ
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆä½œä¸ºæ¨¡å—è¿è¡Œæ—¶ï¼‰
    from .utils import setup_logging, ensure_directory_exists
    from .config import ensure_output_dirs
    from .data_extraction import extract_ten_percent_sample
    from .data_ingestion import analyze_data_ingestion
    from .data_cleaning import clean_data
    from .sentiment_analysis import analyze_sentiment
    from .topic_modeling import perform_topic_modeling
    from .classification import perform_classification
except ImportError:
    # ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥è¿è¡Œæ—¶ï¼‰
    from src.utils import setup_logging, ensure_directory_exists
    from src.config import ensure_output_dirs
    from src.data_extraction import extract_ten_percent_sample
    from src.data_ingestion import analyze_data_ingestion
    from src.data_cleaning import clean_data
    from src.sentiment_analysis import analyze_sentiment
    from src.topic_modeling import perform_topic_modeling
    from src.classification import perform_classification

logger = setup_logging()

class TweetSentimentAnalysisPipeline:
    """Tweetæƒ…æ„Ÿåˆ†æå®Œæ•´æµæ°´çº¿"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        self.results = {}
        ensure_output_dirs()
        
    def run_data_extraction(self) -> bool:
        """
        æ­¥éª¤1: æ•°æ®æå–
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=== æ­¥éª¤1: æ•°æ®æå– ===")
        try:
            success = extract_ten_percent_sample()
            self.results['data_extraction'] = {'success': success}
            
            if success:
                logger.info("âœ… æ•°æ®æå–å®Œæˆ")
            else:
                logger.error("âŒ æ•°æ®æå–å¤±è´¥")
                
            return success
        except Exception as e:
            logger.error(f"æ•°æ®æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            self.results['data_extraction'] = {'success': False, 'error': str(e)}
            return False
    
    def run_data_ingestion(self) -> bool:
        """
        æ­¥éª¤2: æ•°æ®æ‘„å–å’Œç»Ÿè®¡åˆ†æ
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=== æ­¥éª¤2: æ•°æ®æ‘„å–å’Œç»Ÿè®¡åˆ†æ ===")
        try:
            report = analyze_data_ingestion(use_sample=True, save_plots=True)
            success = bool(report)
            
            self.results['data_ingestion'] = {
                'success': success,
                'report': report
            }
            
            if success:
                logger.info("âœ… æ•°æ®æ‘„å–åˆ†æå®Œæˆ")
            else:
                logger.error("âŒ æ•°æ®æ‘„å–åˆ†æå¤±è´¥")
                
            return success
        except Exception as e:
            logger.error(f"æ•°æ®æ‘„å–åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            self.results['data_ingestion'] = {'success': False, 'error': str(e)}
            return False
    
    def run_data_cleaning(self) -> bool:
        """
        æ­¥éª¤3: æ•°æ®æ¸…æ´—
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=== æ­¥éª¤3: æ•°æ®æ¸…æ´— ===")
        try:
            report = clean_data()
            success = report.get('success', False)
            
            self.results['data_cleaning'] = report
            
            if success:
                logger.info("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
                logger.info(f"æœ€ç»ˆæ•°æ®: {report.get('final_count', 0):,} æ¡è®°å½•")
            else:
                logger.error("âŒ æ•°æ®æ¸…æ´—å¤±è´¥")
                
            return success
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            self.results['data_cleaning'] = {'success': False, 'error': str(e)}
            return False
    
    def run_sentiment_analysis(self) -> bool:
        """
        æ­¥éª¤4: æƒ…æ„Ÿåˆ†æ
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=== æ­¥éª¤4: æƒ…æ„Ÿåˆ†æ ===")
        try:
            report = analyze_sentiment()
            success = report.get('success', False)
            
            self.results['sentiment_analysis'] = report
            
            if success:
                logger.info("âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
                overview = report.get('overview', {})
                logger.info(f"åˆ†æè®°å½•æ•°: {overview.get('total_count', 0):,}")
            else:
                logger.error("âŒ æƒ…æ„Ÿåˆ†æå¤±è´¥")
                
            return success
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            self.results['sentiment_analysis'] = {'success': False, 'error': str(e)}
            return False
    
    def run_topic_modeling(self) -> bool:
        """
        æ­¥éª¤5: ä¸»é¢˜å»ºæ¨¡
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=== æ­¥éª¤5: LDAä¸»é¢˜å»ºæ¨¡ ===")
        try:
            report = perform_topic_modeling()
            success = report.get('success', False)
            
            self.results['topic_modeling'] = report
            
            if success:
                logger.info("âœ… ä¸»é¢˜å»ºæ¨¡å®Œæˆ")
                logger.info(f"ä¸»é¢˜æ•°é‡: {report.get('num_topics', 0)}")
                logger.info(f"è¯æ±‡è¡¨å¤§å°: {report.get('vocabulary_size', 0)}")
            else:
                logger.error("âŒ ä¸»é¢˜å»ºæ¨¡å¤±è´¥")
                
            return success
        except Exception as e:
            logger.error(f"ä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            self.results['topic_modeling'] = {'success': False, 'error': str(e)}
            return False
    
    def run_classification(self) -> bool:
        """
        æ­¥éª¤6: åˆ†ç±»å»ºæ¨¡
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=== æ­¥éª¤6: åˆ†ç±»å»ºæ¨¡ ===")
        try:
            report = perform_classification()
            success = report.get('success', False)
            
            self.results['classification'] = report
            
            if success:
                logger.info("âœ… åˆ†ç±»å»ºæ¨¡å®Œæˆ")
                logger.info(f"è®­ç»ƒæ¨¡å‹: {', '.join(report.get('models_trained', []))}")
                
                # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹æ€§èƒ½
                evaluations = report.get('evaluations', {})
                if evaluations:
                    best_model = max(evaluations.items(), key=lambda x: x[1].get('accuracy', 0))
                    logger.info(f"æœ€ä½³æ¨¡å‹: {best_model[0]} (å‡†ç¡®ç‡: {best_model[1].get('accuracy', 0):.4f})")
            else:
                logger.error("âŒ åˆ†ç±»å»ºæ¨¡å¤±è´¥")
                
            return success
        except Exception as e:
            logger.error(f"åˆ†ç±»å»ºæ¨¡è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            self.results['classification'] = {'success': False, 'error': str(e)}
            return False
    
    def run_full_pipeline(self, skip_steps: Optional[list] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµæ°´çº¿
        
        Args:
            skip_steps: è·³è¿‡çš„æ­¥éª¤åˆ—è¡¨
            
        Returns:
            dict: å®Œæ•´çš„åˆ†æç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒTweetæƒ…æ„Ÿåˆ†æå®Œæ•´æµæ°´çº¿...")
        
        skip_steps = skip_steps or []
        
        # å®šä¹‰æµæ°´çº¿æ­¥éª¤
        pipeline_steps = [
            ('data_extraction', self.run_data_extraction),
            ('data_ingestion', self.run_data_ingestion),
            ('data_cleaning', self.run_data_cleaning),
            ('sentiment_analysis', self.run_sentiment_analysis),
            ('topic_modeling', self.run_topic_modeling),
            ('classification', self.run_classification)
        ]
        
        # æ‰§è¡Œæ¯ä¸ªæ­¥éª¤
        for step_name, step_func in pipeline_steps:
            if step_name in skip_steps:
                logger.info(f"â­ï¸  è·³è¿‡æ­¥éª¤: {step_name}")
                continue
                
            logger.info(f"â–¶ï¸  æ‰§è¡Œæ­¥éª¤: {step_name}")
            success = step_func()
            
            if not success:
                logger.error(f"âŒ æ­¥éª¤ {step_name} å¤±è´¥ï¼Œåœæ­¢æµæ°´çº¿æ‰§è¡Œ")
                break
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self.generate_final_report()
        
        logger.info("ğŸ‰ Tweetæƒ…æ„Ÿåˆ†ææµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        return final_report
    
    def generate_final_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š
        
        Returns:
            dict: æœ€ç»ˆæŠ¥å‘Š
        """
        logger.info("ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
        
        # ç»Ÿè®¡æˆåŠŸçš„æ­¥éª¤
        successful_steps = []
        failed_steps = []
        
        for step_name, result in self.results.items():
            if result.get('success', False):
                successful_steps.append(step_name)
            else:
                failed_steps.append(step_name)
        
        # æ±‡æ€»å…³é”®æŒ‡æ ‡
        key_metrics = {}
        
        # æ•°æ®é‡ç»Ÿè®¡
        if 'data_cleaning' in self.results and self.results['data_cleaning'].get('success'):
            cleaning_result = self.results['data_cleaning']
            key_metrics['initial_records'] = cleaning_result.get('initial_count', 0)
            key_metrics['final_records'] = cleaning_result.get('final_count', 0)
            key_metrics['data_retention_rate'] = cleaning_result.get('retention_rate', 0)
        
        # æƒ…æ„Ÿåˆ†æç»“æœ
        if 'sentiment_analysis' in self.results and self.results['sentiment_analysis'].get('success'):
            sentiment_result = self.results['sentiment_analysis']
            sentiment_dist = sentiment_result.get('sentiment_distribution', {})
            if sentiment_dist and 'distribution' in sentiment_dist:
                key_metrics['sentiment_distribution'] = sentiment_dist['distribution'].to_dict('records')
        
        # ä¸»é¢˜å»ºæ¨¡ç»“æœ
        if 'topic_modeling' in self.results and self.results['topic_modeling'].get('success'):
            topic_result = self.results['topic_modeling']
            key_metrics['num_topics'] = topic_result.get('num_topics', 0)
            key_metrics['vocabulary_size'] = topic_result.get('vocabulary_size', 0)
        
        # åˆ†ç±»æ¨¡å‹ç»“æœ
        if 'classification' in self.results and self.results['classification'].get('success'):
            classification_result = self.results['classification']
            key_metrics['models_trained'] = classification_result.get('models_trained', [])
            
            evaluations = classification_result.get('evaluations', {})
            if evaluations:
                best_model = max(evaluations.items(), key=lambda x: x[1].get('accuracy', 0))
                key_metrics['best_model'] = {
                    'name': best_model[0],
                    'accuracy': best_model[1].get('accuracy', 0),
                    'f1_score': best_model[1].get('f1', 0)
                }
        
        final_report = {
            'pipeline_summary': {
                'total_steps': len(self.results),
                'successful_steps': len(successful_steps),
                'failed_steps': len(failed_steps),
                'success_rate': len(successful_steps) / len(self.results) * 100 if self.results else 0
            },
            'successful_steps': successful_steps,
            'failed_steps': failed_steps,
            'key_metrics': key_metrics,
            'detailed_results': self.results
        }
        
        # æ‰“å°æœ€ç»ˆæŠ¥å‘Šæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ¯ TWEETæƒ…æ„Ÿåˆ†ææœ€ç»ˆæŠ¥å‘Š")
        print("="*60)
        
        print(f"\nğŸ“Š æµæ°´çº¿æ‰§è¡Œæ‘˜è¦:")
        print(f"  æ€»æ­¥éª¤æ•°: {final_report['pipeline_summary']['total_steps']}")
        print(f"  æˆåŠŸæ­¥éª¤: {final_report['pipeline_summary']['successful_steps']}")
        print(f"  å¤±è´¥æ­¥éª¤: {final_report['pipeline_summary']['failed_steps']}")
        print(f"  æˆåŠŸç‡: {final_report['pipeline_summary']['success_rate']:.1f}%")
        
        if key_metrics:
            print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
            if 'initial_records' in key_metrics:
                print(f"  åˆå§‹æ•°æ®: {key_metrics['initial_records']:,} æ¡è®°å½•")
                print(f"  æœ€ç»ˆæ•°æ®: {key_metrics['final_records']:,} æ¡è®°å½•")
                print(f"  æ•°æ®ä¿ç•™ç‡: {key_metrics['data_retention_rate']:.2f}%")
            
            if 'num_topics' in key_metrics:
                print(f"  è¯†åˆ«ä¸»é¢˜æ•°: {key_metrics['num_topics']}")
                print(f"  è¯æ±‡è¡¨å¤§å°: {key_metrics['vocabulary_size']:,}")
            
            if 'best_model' in key_metrics:
                best = key_metrics['best_model']
                print(f"  æœ€ä½³æ¨¡å‹: {best['name'].replace('_', ' ').title()}")
                print(f"  æ¨¡å‹å‡†ç¡®ç‡: {best['accuracy']:.4f}")
                print(f"  æ¨¡å‹F1åˆ†æ•°: {best['f1_score']:.4f}")
        
        if failed_steps:
            print(f"\nâŒ å¤±è´¥æ­¥éª¤: {', '.join(failed_steps)}")
            print("è¯·æ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        
        print("\n" + "="*60)
        
        return final_report

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Tweetæƒ…æ„Ÿåˆ†ææµæ°´çº¿')
    parser.add_argument('--step', choices=[
        'extraction', 'ingestion', 'cleaning', 'sentiment', 'topic', 'classification', 'all'
    ], default='all', help='è¦æ‰§è¡Œçš„æ­¥éª¤')
    parser.add_argument('--skip', nargs='*', choices=[
        'data_extraction', 'data_ingestion', 'data_cleaning', 
        'sentiment_analysis', 'topic_modeling', 'classification'
    ], help='è¦è·³è¿‡çš„æ­¥éª¤')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµæ°´çº¿å®ä¾‹
    pipeline = TweetSentimentAnalysisPipeline()
    
    try:
        if args.step == 'all':
            # è¿è¡Œå®Œæ•´æµæ°´çº¿
            final_report = pipeline.run_full_pipeline(skip_steps=args.skip)
        else:
            # è¿è¡Œå•ä¸ªæ­¥éª¤
            step_mapping = {
                'extraction': pipeline.run_data_extraction,
                'ingestion': pipeline.run_data_ingestion,
                'cleaning': pipeline.run_data_cleaning,
                'sentiment': pipeline.run_sentiment_analysis,
                'topic': pipeline.run_topic_modeling,
                'classification': pipeline.run_classification
            }
            
            if args.step in step_mapping:
                success = step_mapping[args.step]()
                if success:
                    logger.info(f"âœ… æ­¥éª¤ {args.step} æ‰§è¡ŒæˆåŠŸ")
                else:
                    logger.error(f"âŒ æ­¥éª¤ {args.step} æ‰§è¡Œå¤±è´¥")
                    sys.exit(1)
            else:
                logger.error(f"æœªçŸ¥æ­¥éª¤: {args.step}")
                sys.exit(1)
                
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸé”™è¯¯: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main() 