{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç¬¬ä¸€æ­¥ï¼šæ•°æ®åŠ è½½ä¸ŽåŸºæœ¬ç»Ÿè®¡åˆ†æž\n",
        "\n",
        "# æœ¬ notebook çš„ç›®æ ‡ï¼š\n",
        "# 1. åˆå§‹åŒ– Spark Session\n",
        "# 2. åŠ è½½åŽŸå§‹æ•°æ®\n",
        "# 3. æŸ¥çœ‹æ•°æ®ç»“æž„å’ŒåŸºæœ¬ä¿¡æ¯\n",
        "# 4. è¿›è¡Œæè¿°æ€§ç»Ÿè®¡åˆ†æž\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆå§‹åŒ– Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_DataIngestion\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {sc.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½æ•°æ®\n",
        "raw_data_path = \"/home/jovyan/work/data/raw/the-reddit-climate-change-dataset-comments.csv\"\n",
        "\n",
        "# åŠ è½½æ•°æ®ï¼Œè®©Sparkè‡ªåŠ¨æŽ¨æ–­Schema\n",
        "df_raw = spark.read.csv(raw_data_path, header=True, inferSchema=True, multiLine=True, escape='\"')\n",
        "\n",
        "# ç¼“å­˜DataFrameï¼ŒåŽç»­æ“ä½œä¼šæ›´å¿«\n",
        "df_raw.cache()\n",
        "\n",
        "print(\"æ•°æ®åŠ è½½å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹æ•°æ®ç»“æž„\n",
        "print(\"=== æ•°æ®ç»“æž„ (Schema) ===\")\n",
        "df_raw.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®\n",
        "print(\"=== å‰5è¡Œæ•°æ® ===\")\n",
        "df_raw.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
        "print(\"=== åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ ===\")\n",
        "\n",
        "# 1. æ•°æ®æ€»é‡\n",
        "total_count = df_raw.count()\n",
        "print(f\"æ€»è¯„è®ºæ•°é‡: {total_count:,}\")\n",
        "\n",
        "# 2. åˆ—æ•°\n",
        "num_columns = len(df_raw.columns)\n",
        "print(f\"åˆ—æ•°: {num_columns}\")\n",
        "\n",
        "# 3. åˆ—å\n",
        "print(f\"åˆ—å: {df_raw.columns}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥å„åˆ—çš„è¯¦ç»†ä¿¡æ¯å’Œæ ·æœ¬æ•°æ®\n",
        "print(\"=== å„åˆ—è¯¦ç»†ä¿¡æ¯ ===\")\n",
        "for col_name in df_raw.columns:\n",
        "    print(f\"\\nåˆ—å: {col_name}\")\n",
        "    print(f\"æ•°æ®ç±»åž‹: {dict(df_raw.dtypes)[col_name]}\")\n",
        "    \n",
        "    # æ˜¾ç¤ºéžç©ºå€¼æ•°é‡ - ä½¿ç”¨åå¼•å·å¤„ç†åŒ…å«ç‚¹å·çš„åˆ—å\n",
        "    if '.' in col_name:\n",
        "        non_null_count = df_raw.filter(F.col(f\"`{col_name}`\").isNotNull()).count()\n",
        "    else:\n",
        "        non_null_count = df_raw.filter(F.col(col_name).isNotNull()).count()\n",
        "    \n",
        "    null_count = total_count - non_null_count\n",
        "    print(f\"éžç©ºå€¼: {non_null_count:,} | ç©ºå€¼: {null_count:,} ({null_count/total_count*100:.2f}%)\")\n",
        "    \n",
        "    # æ˜¾ç¤ºæ ·æœ¬å€¼ï¼ˆå¯¹äºŽå­—ç¬¦ä¸²ç±»åž‹ï¼Œé™åˆ¶é•¿åº¦ï¼‰\n",
        "    if col_name in ['body', 'permalink']:\n",
        "        sample_values = df_raw.select(col_name).limit(2).collect()\n",
        "        for i, row in enumerate(sample_values):\n",
        "            value = str(row[0])[:100] + \"...\" if row[0] and len(str(row[0])) > 100 else row[0]\n",
        "            print(f\"  æ ·æœ¬{i+1}: {value}\")\n",
        "    else:\n",
        "        sample_values = df_raw.select(col_name).limit(3).collect()\n",
        "        for i, row in enumerate(sample_values):\n",
        "            print(f\"  æ ·æœ¬{i+1}: {row[0]}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ—¶é—´èŒƒå›´åˆ†æž\n",
        "print(\"=== æ—¶é—´èŒƒå›´åˆ†æž ===\")\n",
        "\n",
        "# è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼\n",
        "df_with_time = df_raw.withColumn(\"timestamp\", F.from_unixtime(F.col(\"created_utc\")))\n",
        "\n",
        "# èŽ·å–æ—¶é—´èŒƒå›´\n",
        "time_stats = df_with_time.select(\n",
        "    F.min(\"timestamp\").alias(\"earliest_time\"),\n",
        "    F.max(\"timestamp\").alias(\"latest_time\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"æœ€æ—©è¯„è®ºæ—¶é—´: {time_stats['earliest_time']}\")\n",
        "print(f\"æœ€æ™šè¯„è®ºæ—¶é—´: {time_stats['latest_time']}\")\n",
        "\n",
        "# æŒ‰å¹´ä»½ç»Ÿè®¡è¯„è®ºæ•°é‡\n",
        "yearly_stats = df_with_time.withColumn(\"year\", F.year(\"timestamp\")) \\\n",
        "                          .groupBy(\"year\") \\\n",
        "                          .count() \\\n",
        "                          .orderBy(\"year\")\n",
        "\n",
        "print(\"\\næŒ‰å¹´ä»½ç»Ÿè®¡è¯„è®ºæ•°é‡:\")\n",
        "yearly_stats.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­ç‰ˆå—åˆ†æž\n",
        "print(\"=== å­ç‰ˆå—åˆ†æž ===\")\n",
        "\n",
        "# ç»Ÿè®¡å„å­ç‰ˆå—çš„è¯„è®ºæ•°é‡ - ä½¿ç”¨åå¼•å·å¤„ç†åˆ—å\n",
        "subreddit_stats = df_raw.groupBy(F.col(\"`subreddit.name`\")) \\\n",
        "                        .count() \\\n",
        "                        .orderBy(F.desc(\"count\"))\n",
        "\n",
        "print(\"è¯„è®ºæ•°é‡æœ€å¤šçš„å‰20ä¸ªå­ç‰ˆå—:\")\n",
        "subreddit_stats.show(20)\n",
        "\n",
        "# è½¬æ¢ä¸º Pandas è¿›è¡Œå¯è§†åŒ–\n",
        "top_subreddits = subreddit_stats.limit(15).toPandas()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=top_subreddits, x='count', y='subreddit.name')\n",
        "plt.title('è¯„è®ºæ•°é‡æœ€å¤šçš„å‰15ä¸ªå­ç‰ˆå—')\n",
        "plt.xlabel('è¯„è®ºæ•°é‡')\n",
        "plt.ylabel('å­ç‰ˆå—åç§°')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æƒ…æ„Ÿåˆ†æ•°åˆ†æžï¼ˆæ•°æ®ä¸­å·²æœ‰sentimentåˆ—ï¼‰\n",
        "print(\"=== æƒ…æ„Ÿåˆ†æ•°åˆ†æž ===\")\n",
        "\n",
        "# åŸºæœ¬ç»Ÿè®¡\n",
        "sentiment_stats = df_raw.select(\"sentiment\").describe()\n",
        "sentiment_stats.show()\n",
        "\n",
        "# æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒ\n",
        "print(\"æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒ:\")\n",
        "sentiment_ranges = df_raw.withColumn(\"sentiment_range\",\n",
        "    F.when(F.col(\"sentiment\") >= 0.1, \"positive\")\n",
        "     .when(F.col(\"sentiment\") <= -0.1, \"negative\")\n",
        "     .otherwise(\"neutral\")\n",
        ").groupBy(\"sentiment_range\").count().orderBy(F.desc(\"count\"))\n",
        "\n",
        "sentiment_ranges.show()\n",
        "\n",
        "# å¯è§†åŒ–æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒ\n",
        "sentiment_sample = df_raw.select(\"sentiment\").sample(False, 0.01).toPandas()  # æŠ½æ ·1%\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(sentiment_sample['sentiment'], bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.title('æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾')\n",
        "plt.xlabel('æƒ…æ„Ÿåˆ†æ•°')\n",
        "plt.ylabel('é¢‘æ¬¡')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=sentiment_sample['sentiment'])\n",
        "plt.title('æƒ…æ„Ÿåˆ†æ•°ç®±çº¿å›¾')\n",
        "plt.ylabel('æƒ…æ„Ÿåˆ†æ•°')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¯„è®ºé•¿åº¦åˆ†æž\n",
        "print(\"=== è¯„è®ºé•¿åº¦åˆ†æž ===\")\n",
        "\n",
        "# è®¡ç®—è¯„è®ºé•¿åº¦\n",
        "df_with_length = df_raw.withColumn(\"body_length\", F.length(F.col(\"body\")))\n",
        "\n",
        "# é•¿åº¦ç»Ÿè®¡\n",
        "length_stats = df_with_length.select(\"body_length\").describe()\n",
        "length_stats.show()\n",
        "\n",
        "# é•¿åº¦åˆ†å¸ƒ\n",
        "print(\"è¯„è®ºé•¿åº¦åˆ†å¸ƒ:\")\n",
        "length_ranges = df_with_length.withColumn(\"length_range\",\n",
        "    F.when(F.col(\"body_length\") <= 50, \"very_short\")\n",
        "     .when(F.col(\"body_length\") <= 200, \"short\")\n",
        "     .when(F.col(\"body_length\") <= 500, \"medium\")\n",
        "     .when(F.col(\"body_length\") <= 1000, \"long\")\n",
        "     .otherwise(\"very_long\")\n",
        ").groupBy(\"length_range\").count().orderBy(F.desc(\"count\"))\n",
        "\n",
        "length_ranges.show()\n",
        "\n",
        "# å¯è§†åŒ–è¯„è®ºé•¿åº¦åˆ†å¸ƒ\n",
        "length_sample = df_with_length.select(\"body_length\").sample(False, 0.01).toPandas()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(length_sample['body_length'], bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.title('è¯„è®ºé•¿åº¦åˆ†å¸ƒç›´æ–¹å›¾')\n",
        "plt.xlabel('è¯„è®ºé•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰')\n",
        "plt.ylabel('é¢‘æ¬¡')\n",
        "plt.xlim(0, 2000)  # é™åˆ¶xè½´èŒƒå›´ä»¥ä¾¿æ›´å¥½åœ°è§‚å¯Ÿ\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“Š æ•°æ®æŽ¢ç´¢æ€»ç»“\n",
        "\n",
        "é€šè¿‡ä»¥ä¸Šåˆ†æžï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†æœ‰äº†å…¨é¢çš„äº†è§£ï¼š\n",
        "\n",
        "### æ•°æ®è§„æ¨¡\n",
        "- **æ€»é‡**: 460ä¸‡æ¡ Reddit è¯„è®º\n",
        "- **æ—¶é—´è·¨åº¦**: ä»Žæ•°æ®ä¸­çš„æ—¶é—´æˆ³å¯ä»¥çœ‹å‡ºè¦†ç›–çš„æ—¶é—´èŒƒå›´\n",
        "- **æ¥æº**: å¤šä¸ª Reddit å­ç‰ˆå—çš„æ°”å€™å˜åŒ–ç›¸å…³è®¨è®º\n",
        "\n",
        "### å…³é”®å‘çŽ°\n",
        "1. **æ•°æ®è´¨é‡**: æ£€æŸ¥äº†å„åˆ—çš„ç¼ºå¤±å€¼æƒ…å†µ\n",
        "2. **æƒ…æ„Ÿåˆ†å¸ƒ**: æ•°æ®ä¸­å·²åŒ…å«é¢„è®¡ç®—çš„æƒ…æ„Ÿåˆ†æ•°\n",
        "3. **å†…å®¹é•¿åº¦**: è¯„è®ºé•¿åº¦åˆ†å¸ƒæƒ…å†µ\n",
        "4. **ç¤¾åŒºåˆ†å¸ƒ**: ä¸åŒå­ç‰ˆå—çš„å‚ä¸Žåº¦\n",
        "\n",
        "### ä¸‹ä¸€æ­¥è®¡åˆ’\n",
        "1. **æ•°æ®æ¸…æ´—**: å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å’Œå™ªå£°æ•°æ®\n",
        "2. **æ–‡æœ¬é¢„å¤„ç†**: æ¸…ç†HTMLæ ‡ç­¾ã€URLã€ç‰¹æ®Šå­—ç¬¦ç­‰\n",
        "3. **æ·±åº¦åˆ†æž**: æƒ…æ„Ÿè¶‹åŠ¿ã€ä¸»é¢˜å»ºæ¨¡ã€åˆ†ç±»é¢„æµ‹\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
