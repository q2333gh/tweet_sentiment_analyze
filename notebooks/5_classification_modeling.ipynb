{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 第五步：分类建模 (Classification Modeling)\n",
        "\n",
        "## 本notebook的目标：\n",
        "1. 加载经过情感分析和主题建模的数据\n",
        "2. 准备特征工程（文本向量化和主题分布特征）\n",
        "3. 训练分类模型（Naive Bayes 和 Random Forest）\n",
        "4. 模型评估和比较\n",
        "5. 生成模型评估报告\n",
        "\n",
        "## 分类任务\n",
        "- **目标变量**: 情感标签（积极、中性、消极）\n",
        "- **特征**: TF-IDF向量 + 主题分布\n",
        "- **模型**: Naive Bayes、Random Forest\n",
        "- **评估指标**: 准确率、召回率、F1值、混淆矩阵\n",
        "\n",
        "## 模型流程\n",
        "1. 数据预处理和特征工程\n",
        "2. 训练/测试集划分\n",
        "3. 模型训练和调参\n",
        "4. 模型评估和对比\n",
        "5. 特征重要性分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "库导入完成！\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"库导入完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.0\n",
            "Available cores: 20\n"
          ]
        }
      ],
      "source": [
        "# 初始化Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_Classification\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 数据加载 ===\n",
            "❌ 数据加载失败: [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/processed/topic_analyzed_comments.parquet.\n",
            "使用清洗后的数据作为备选方案...\n",
            "✅ 使用清洗后数据，共 459,171 条记录\n"
          ]
        }
      ],
      "source": [
        "# 加载主题分析和情感分析的数据\n",
        "print(\"=== 数据加载 ===\")\n",
        "\n",
        "# 尝试加载主题分析结果\n",
        "topic_data_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_topic = spark.read.parquet(topic_data_path)\n",
        "    df_topic.cache()\n",
        "    record_count = df_topic.count()\n",
        "    print(f\"✅ 主题分析数据加载完成，共 {record_count:,} 条记录\")\n",
        "    \n",
        "    print(\"\\n数据结构:\")\n",
        "    df_topic.printSchema()\n",
        "    \n",
        "    # 检查必要的列\n",
        "    required_cols = ['sentiment_label', 'cleaned_body', 'dominant_topic']\n",
        "    missing_cols = [col for col in required_cols if col not in df_topic.columns]\n",
        "    \n",
        "    if missing_cols:\n",
        "        print(f\"❌ 缺少必要列: {missing_cols}\")\n",
        "        print(\"尝试加载其他数据源...\")\n",
        "        \n",
        "        # 备选方案：加载情感分析数据\n",
        "        sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "        df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "        \n",
        "        # 需要重新进行情感分类\n",
        "        def classify_sentiment(score):\n",
        "            if score is None:\n",
        "                return \"未知\"\n",
        "            elif score > 0.1:\n",
        "                return \"积极\"\n",
        "            elif score < -0.1:\n",
        "                return \"消极\"\n",
        "            else:\n",
        "                return \"中性\"\n",
        "        \n",
        "        classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "        \n",
        "        # 找到情感分数列\n",
        "        sentiment_col = None\n",
        "        for col in ['compound_score', 'vader_compound', 'sentiment']:\n",
        "            if col in df_sentiment.columns:\n",
        "                sentiment_col = col\n",
        "                break\n",
        "        \n",
        "        if sentiment_col:\n",
        "            df_topic = df_sentiment.withColumn(\n",
        "                \"sentiment_label\",\n",
        "                classify_sentiment_udf(F.col(sentiment_col))\n",
        "            )\n",
        "            df_topic = df_topic.withColumn(\"dominant_topic\", F.lit(0))  # 临时主题\n",
        "            record_count = df_topic.count()\n",
        "            print(f\"✅ 使用情感分析数据，共 {record_count:,} 条记录\")\n",
        "        else:\n",
        "            raise Exception(\"未找到情感分数列\")\n",
        "    else:\n",
        "        print(\"✅ 所有必要列都存在\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ 数据加载失败: {e}\")\n",
        "    print(\"使用清洗后的数据作为备选方案...\")\n",
        "    \n",
        "    # 最后备选：使用清洗后的数据\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_base = spark.read.parquet(cleaned_data_path)\n",
        "    \n",
        "    # 使用原始sentiment列\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"未知\"\n",
        "        elif score > 0.1:\n",
        "            return \"积极\"\n",
        "        elif score < -0.1:\n",
        "            return \"消极\"\n",
        "        else:\n",
        "            return \"中性\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    df_topic = df_base.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(\"sentiment\"))\n",
        "    ).withColumn(\"dominant_topic\", F.lit(0))  # 临时主题\n",
        "    \n",
        "    record_count = df_topic.count()\n",
        "    print(f\"✅ 使用清洗后数据，共 {record_count:,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 数据预处理和特征工程 ===\n",
            "1. 情感标签分布:\n",
            "+---------------+------+\n",
            "|sentiment_label| count|\n",
            "+---------------+------+\n",
            "|           消极|205850|\n",
            "|           积极|201235|\n",
            "|           中性| 47179|\n",
            "|           未知|  4907|\n",
            "+---------------+------+\n",
            "\n",
            "详细分布:\n",
            "  消极: 205,850 (44.83%)\n",
            "  积极: 201,235 (43.83%)\n",
            "  中性: 47,179 (10.27%)\n",
            "  未知: 4,907 (1.07%)\n",
            "\n",
            "过滤后数据量: 454,264 条记录\n",
            "3. ✅ 已有分词结果\n",
            "\n",
            "最终用于建模的数据: 454,264 条记录\n"
          ]
        }
      ],
      "source": [
        "# 数据预处理和特征工程\n",
        "print(\"=== 数据预处理和特征工程 ===\")\n",
        "\n",
        "# 1. 检查情感标签分布\n",
        "print(\"1. 情感标签分布:\")\n",
        "sentiment_dist = df_topic.groupBy(\"sentiment_label\").count().orderBy(F.desc(\"count\"))\n",
        "sentiment_dist.show()\n",
        "\n",
        "# 转换为pandas查看详细比例\n",
        "sentiment_dist_pd = sentiment_dist.toPandas()\n",
        "total_count = sentiment_dist_pd['count'].sum()\n",
        "sentiment_dist_pd['percentage'] = (sentiment_dist_pd['count'] / total_count * 100).round(2)\n",
        "\n",
        "print(\"详细分布:\")\n",
        "for _, row in sentiment_dist_pd.iterrows():\n",
        "    label = row['sentiment_label']\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    print(f\"  {label}: {count:,} ({pct}%)\")\n",
        "\n",
        "# 2. 过滤掉\"未知\"标签的数据（如果存在）\n",
        "df_filtered = df_topic.filter(F.col(\"sentiment_label\") != \"未知\")\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"\\n过滤后数据量: {filtered_count:,} 条记录\")\n",
        "\n",
        "# 3. 检查是否有分词结果，如果没有则重新分词\n",
        "if 'tokens_cleaned' not in df_filtered.columns:\n",
        "    print(\"3. 重新进行文本分词...\")\n",
        "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "    \n",
        "    # 分词\n",
        "    tokenizer = Tokenizer(inputCol=\"cleaned_body\", outputCol=\"tokens_raw\")\n",
        "    df_tokens = tokenizer.transform(df_filtered)\n",
        "    \n",
        "    # 去停用词\n",
        "    remover = StopWordsRemover(inputCol=\"tokens_raw\", outputCol=\"tokens_cleaned\")\n",
        "    df_filtered = remover.transform(df_tokens)\n",
        "    print(\"   分词完成\")\n",
        "else:\n",
        "    print(\"3. ✅ 已有分词结果\")\n",
        "\n",
        "print(f\"\\n最终用于建模的数据: {df_filtered.count():,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 构建特征向量 ===\n",
            "1. 构建TF-IDF特征...\n",
            "   TF-IDF词汇表大小: 3000\n",
            "2. 添加主题特征...\n"
          ]
        },
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: The input column topic_index should have at least two distinct values.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# One-hot编码\u001b[39;00m\n\u001b[1;32m     39\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_indexed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df_indexed)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 合并TF-IDF和主题特征\u001b[39;00m\n\u001b[1;32m     43\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(\n\u001b[1;32m     44\u001b[0m     inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidf_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_features\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     45\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m )\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The input column topic_index should have at least two distinct values."
          ]
        }
      ],
      "source": [
        "# 构建特征向量\n",
        "print(\"=== 构建特征向量 ===\")\n",
        "\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "\n",
        "# 1. 文本特征：TF-IDF向量化\n",
        "print(\"1. 构建TF-IDF特征...\")\n",
        "\n",
        "# CountVectorizer\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=3000,  # 减少特征维度提高训练速度\n",
        "    minDF=3.0        # 最小文档频率\n",
        ")\n",
        "\n",
        "count_model = count_vectorizer.fit(df_filtered)\n",
        "df_vectorized = count_model.transform(df_filtered)\n",
        "\n",
        "# TF-IDF\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"   TF-IDF词汇表大小: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# 2. 主题特征（如果有的话）\n",
        "if 'dominant_topic' in df_tfidf.columns:\n",
        "    print(\"2. 添加主题特征...\")\n",
        "    # 将主题ID转换为one-hot编码\n",
        "    from pyspark.ml.feature import OneHotEncoder\n",
        "    \n",
        "    # 先转换为数值索引\n",
        "    indexer = StringIndexer(inputCol=\"dominant_topic\", outputCol=\"topic_index\")\n",
        "    indexer_model = indexer.fit(df_tfidf)\n",
        "    df_indexed = indexer_model.transform(df_tfidf)\n",
        "    \n",
        "    # One-hot编码\n",
        "    encoder = OneHotEncoder(inputCol=\"topic_index\", outputCol=\"topic_features\")\n",
        "    df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "    \n",
        "    # 合并TF-IDF和主题特征\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"tfidf_features\", \"topic_features\"],\n",
        "        outputCol=\"features\"\n",
        "    )\n",
        "    df_features = assembler.transform(df_encoded)\n",
        "    print(\"   ✅ TF-IDF + 主题特征合并完成\")\n",
        "else:\n",
        "    print(\"2. 仅使用TF-IDF特征...\")\n",
        "    df_features = df_tfidf.withColumnRenamed(\"tfidf_features\", \"features\")\n",
        "\n",
        "print(f\"最终特征数据: {df_features.count():,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备训练数据和标签编码\n",
        "print(\"=== 准备训练数据 ===\")\n",
        "\n",
        "# 1. 将情感标签转换为数值索引\n",
        "label_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\n",
        "indexer_model = label_indexer.fit(df_features)\n",
        "df_labeled = indexer_model.transform(df_features)\n",
        "\n",
        "# 获取标签映射\n",
        "labels = indexer_model.labels\n",
        "print(f\"标签映射: {dict(enumerate(labels))}\")\n",
        "\n",
        "# 2. 划分训练集和测试集\n",
        "print(\"2. 划分训练集和测试集...\")\n",
        "train_data, test_data = df_labeled.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "train_count = train_data.count()\n",
        "test_count = test_data.count()\n",
        "\n",
        "print(f\"训练集: {train_count:,} 条记录\")\n",
        "print(f\"测试集: {test_count:,} 条记录\")\n",
        "\n",
        "# 缓存数据提高后续训练速度\n",
        "train_data.cache()\n",
        "test_data.cache()\n",
        "\n",
        "# 3. 检查训练集中的标签分布\n",
        "print(\"\\n3. 训练集标签分布:\")\n",
        "train_label_dist = train_data.groupBy(\"sentiment_label\").count().orderBy(F.desc(\"count\"))\n",
        "train_label_dist.show()\n",
        "\n",
        "print(\"测试集标签分布:\")\n",
        "test_label_dist = test_data.groupBy(\"sentiment_label\").count().orderBy(F.desc(\"count\"))\n",
        "test_label_dist.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练Naive Bayes模型\n",
        "print(\"=== 训练Naive Bayes模型 ===\")\n",
        "\n",
        "# 1. 创建Naive Bayes分类器\n",
        "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"nb_prediction\")\n",
        "\n",
        "print(\"1. 开始训练Naive Bayes模型...\")\n",
        "nb_model = nb.fit(train_data)\n",
        "print(\"   ✅ Naive Bayes训练完成\")\n",
        "\n",
        "# 2. 在测试集上进行预测\n",
        "print(\"2. 在测试集上预测...\")\n",
        "nb_predictions = nb_model.transform(test_data)\n",
        "\n",
        "# 3. 评估Naive Bayes模型\n",
        "print(\"3. 评估Naive Bayes模型:\")\n",
        "\n",
        "# 准确率\n",
        "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "nb_accuracy = evaluator_accuracy.evaluate(nb_predictions)\n",
        "\n",
        "# F1分数\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"f1\"\n",
        ")\n",
        "nb_f1 = evaluator_f1.evaluate(nb_predictions)\n",
        "\n",
        "# 加权精确率\n",
        "evaluator_precision = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"weightedPrecision\"\n",
        ")\n",
        "nb_precision = evaluator_precision.evaluate(nb_predictions)\n",
        "\n",
        "# 加权召回率\n",
        "evaluator_recall = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"weightedRecall\"\n",
        ")\n",
        "nb_recall = evaluator_recall.evaluate(nb_predictions)\n",
        "\n",
        "print(f\"   准确率: {nb_accuracy:.4f}\")\n",
        "print(f\"   F1分数: {nb_f1:.4f}\")\n",
        "print(f\"   加权精确率: {nb_precision:.4f}\")\n",
        "print(f\"   加权召回率: {nb_recall:.4f}\")\n",
        "\n",
        "# 保存Naive Bayes结果\n",
        "nb_results = {\n",
        "    'model': 'Naive Bayes',\n",
        "    'accuracy': nb_accuracy,\n",
        "    'f1': nb_f1,\n",
        "    'precision': nb_precision,\n",
        "    'recall': nb_recall\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练Random Forest模型\n",
        "print(\"=== 训练Random Forest模型 ===\")\n",
        "\n",
        "# 1. 创建Random Forest分类器\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\", \n",
        "    labelCol=\"label\", \n",
        "    predictionCol=\"rf_prediction\",\n",
        "    numTrees=50,        # 树的数量，可以根据需要调整\n",
        "    maxDepth=10,        # 最大深度\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"1. 开始训练Random Forest模型...\")\n",
        "rf_model = rf.fit(train_data)\n",
        "print(\"   ✅ Random Forest训练完成\")\n",
        "\n",
        "# 2. 在测试集上进行预测\n",
        "print(\"2. 在测试集上预测...\")\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "\n",
        "# 3. 评估Random Forest模型\n",
        "print(\"3. 评估Random Forest模型:\")\n",
        "\n",
        "# 准确率\n",
        "rf_accuracy = evaluator_accuracy.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "# F1分数\n",
        "rf_f1 = evaluator_f1.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "# 加权精确率\n",
        "rf_precision = evaluator_precision.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "# 加权召回率\n",
        "rf_recall = evaluator_recall.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "print(f\"   准确率: {rf_accuracy:.4f}\")\n",
        "print(f\"   F1分数: {rf_f1:.4f}\")\n",
        "print(f\"   加权精确率: {rf_precision:.4f}\")\n",
        "print(f\"   加权召回率: {rf_recall:.4f}\")\n",
        "\n",
        "# 保存Random Forest结果\n",
        "rf_results = {\n",
        "    'model': 'Random Forest',\n",
        "    'accuracy': rf_accuracy,\n",
        "    'f1': rf_f1,\n",
        "    'precision': rf_precision,\n",
        "    'recall': rf_recall\n",
        "}\n",
        "\n",
        "# 4. 特征重要性分析\n",
        "print(\"\\n4. 特征重要性分析:\")\n",
        "feature_importances = rf_model.featureImportances\n",
        "print(f\"特征重要性向量长度: {len(feature_importances)}\")\n",
        "\n",
        "# 获取最重要的特征（如果是文本特征）\n",
        "if len(feature_importances) > 0:\n",
        "    # 转换为pandas进行分析\n",
        "    importance_array = feature_importances.toArray()\n",
        "    \n",
        "    # 如果特征主要是词汇特征，显示最重要的词汇\n",
        "    vocab_size = len(count_model.vocabulary)\n",
        "    if vocab_size <= len(importance_array):\n",
        "        word_importance = [(count_model.vocabulary[i], importance_array[i]) \n",
        "                          for i in range(min(vocab_size, len(importance_array)))]\n",
        "        word_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        print(\"   最重要的10个词汇特征:\")\n",
        "        for word, importance in word_importance[:10]:\n",
        "            print(f\"     {word}: {importance:.6f}\")\n",
        "    \n",
        "    print(f\"   特征重要性总和: {sum(importance_array):.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模型比较和评估报告\n",
        "print(\"=== 模型比较和评估报告 ===\")\n",
        "\n",
        "# 1. 模型性能比较\n",
        "results_comparison = pd.DataFrame([nb_results, rf_results])\n",
        "print(\"1. 模型性能比较:\")\n",
        "print(results_comparison.round(4))\n",
        "\n",
        "# 2. 可视化模型比较\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
        "metric_names = ['准确率', 'F1分数', '加权精确率', '加权召回率']\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    ax = axes[i//2, i%2]\n",
        "    \n",
        "    values = [nb_results[metric], rf_results[metric]]\n",
        "    models = ['Naive Bayes', 'Random Forest']\n",
        "    colors = ['lightblue', 'lightgreen']\n",
        "    \n",
        "    bars = ax.bar(models, values, color=colors, alpha=0.8)\n",
        "    ax.set_title(f'{name}比较', fontweight='bold')\n",
        "    ax.set_ylabel(name)\n",
        "    ax.set_ylim(0, 1)\n",
        "    \n",
        "    # 在柱状图上添加数值标签\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('模型性能比较', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# 3. 确定最佳模型\n",
        "best_model_name = 'Naive Bayes' if nb_f1 > rf_f1 else 'Random Forest'\n",
        "best_predictions = nb_predictions if nb_f1 > rf_f1 else rf_predictions\n",
        "best_pred_col = 'nb_prediction' if nb_f1 > rf_f1 else 'rf_prediction'\n",
        "\n",
        "print(f\"\\n3. 最佳模型: {best_model_name} (F1分数: {max(nb_f1, rf_f1):.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 详细的分类报告和混淆矩阵\n",
        "print(\"=== 详细分类报告 ===\")\n",
        "\n",
        "# 1. 生成混淆矩阵数据\n",
        "def get_confusion_matrix_data(predictions, prediction_col):\n",
        "    \"\"\"生成混淆矩阵数据\"\"\"\n",
        "    # 收集预测结果\n",
        "    results = predictions.select(\"label\", prediction_col).collect()\n",
        "    \n",
        "    y_true = [row[\"label\"] for row in results]\n",
        "    y_pred = [row[prediction_col] for row in results]\n",
        "    \n",
        "    return y_true, y_pred\n",
        "\n",
        "# 2. 为最佳模型生成详细报告\n",
        "print(f\"为最佳模型 ({best_model_name}) 生成详细报告:\")\n",
        "\n",
        "y_true, y_pred = get_confusion_matrix_data(best_predictions, best_pred_col)\n",
        "\n",
        "# 转换数值标签回文字标签\n",
        "label_mapping = {i: label for i, label in enumerate(labels)}\n",
        "y_true_labels = [label_mapping[int(label)] for label in y_true]\n",
        "y_pred_labels = [label_mapping[int(pred)] for pred in y_pred]\n",
        "\n",
        "# 生成分类报告\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\n1. 分类报告:\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, zero_division=0))\n",
        "\n",
        "# 2. 混淆矩阵\n",
        "print(\"2. 混淆矩阵:\")\n",
        "cm = confusion_matrix(y_true_labels, y_pred_labels, labels=labels)\n",
        "print(cm)\n",
        "\n",
        "# 3. 可视化混淆矩阵\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.title(f'{best_model_name} - 混淆矩阵', fontweight='bold')\n",
        "plt.xlabel('预测标签')\n",
        "plt.ylabel('真实标签')\n",
        "plt.show()\n",
        "\n",
        "# 4. 按类别分析性能\n",
        "print(\"\\n3. 各类别性能分析:\")\n",
        "for i, label in enumerate(labels):\n",
        "    true_positives = cm[i, i]\n",
        "    false_positives = sum(cm[:, i]) - true_positives\n",
        "    false_negatives = sum(cm[i, :]) - true_positives\n",
        "    \n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    print(f\"  {label}:\")\n",
        "    print(f\"    精确率: {precision:.4f}\")\n",
        "    print(f\"    召回率: {recall:.4f}\")\n",
        "    print(f\"    F1分数: {f1:.4f}\")\n",
        "    print(f\"    支持样本数: {sum(cm[i, :])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型评估结果\n",
        "print(\"=== 保存模型评估结果 ===\")\n",
        "\n",
        "# 1. 创建完整的评估报告\n",
        "evaluation_report = {\n",
        "    \"experiment_info\": {\n",
        "        \"dataset_size\": df_filtered.count(),\n",
        "        \"train_size\": train_count,\n",
        "        \"test_size\": test_count,\n",
        "        \"feature_size\": len(count_model.vocabulary),\n",
        "        \"num_classes\": len(labels),\n",
        "        \"class_labels\": labels\n",
        "    },\n",
        "    \"models\": {\n",
        "        \"naive_bayes\": nb_results,\n",
        "        \"random_forest\": rf_results\n",
        "    },\n",
        "    \"best_model\": {\n",
        "        \"name\": best_model_name,\n",
        "        \"metrics\": nb_results if best_model_name == 'Naive Bayes' else rf_results\n",
        "    },\n",
        "    \"class_distribution\": {\n",
        "        \"train\": train_label_dist.toPandas().to_dict('records'),\n",
        "        \"test\": test_label_dist.toPandas().to_dict('records')\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. 保存评估报告\n",
        "import json\n",
        "report_path = \"/home/jovyan/work/data/processed/classification_evaluation_report.json\"\n",
        "\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(evaluation_report, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "print(f\"✅ 模型评估报告已保存到: {report_path}\")\n",
        "\n",
        "# 3. 保存最佳模型的预测结果（样本）\n",
        "sample_predictions = best_predictions.select(\n",
        "    \"id\", \"cleaned_body\", \"sentiment_label\", \"label\", best_pred_col\n",
        ").limit(1000)\n",
        "\n",
        "sample_path = \"/home/jovyan/work/data/processed/classification_sample_predictions.parquet\"\n",
        "sample_predictions.write.mode(\"overwrite\").parquet(sample_path)\n",
        "\n",
        "print(f\"✅ 样本预测结果已保存到: {sample_path}\")\n",
        "\n",
        "# 4. 生成最终总结\n",
        "print(\"\\n=== 分类建模总结 ===\")\n",
        "print(f\"🎯 实验完成概况:\")\n",
        "print(f\"   数据集大小: {df_filtered.count():,} 条记录\")\n",
        "print(f\"   特征维度: {len(count_model.vocabulary):,}\")\n",
        "print(f\"   类别数量: {len(labels)}\")\n",
        "print(f\"   训练集: {train_count:,} | 测试集: {test_count:,}\")\n",
        "\n",
        "print(f\"\\n📊 模型性能对比:\")\n",
        "print(f\"   Naive Bayes    - 准确率: {nb_accuracy:.4f} | F1: {nb_f1:.4f}\")\n",
        "print(f\"   Random Forest  - 准确率: {rf_accuracy:.4f} | F1: {rf_f1:.4f}\")\n",
        "\n",
        "print(f\"\\n🏆 最佳模型: {best_model_name}\")\n",
        "best_metrics = nb_results if best_model_name == 'Naive Bayes' else rf_results\n",
        "print(f\"   准确率: {best_metrics['accuracy']:.4f}\")\n",
        "print(f\"   F1分数: {best_metrics['f1']:.4f}\")\n",
        "print(f\"   精确率: {best_metrics['precision']:.4f}\")\n",
        "print(f\"   召回率: {best_metrics['recall']:.4f}\")\n",
        "\n",
        "print(f\"\\n💡 结果分析:\")\n",
        "if best_metrics['accuracy'] > 0.7:\n",
        "    print(\"   ✅ 模型性能良好，可以较好地区分情感类别\")\n",
        "elif best_metrics['accuracy'] > 0.6:\n",
        "    print(\"   ⚠️ 模型性能中等，有改进空间\")\n",
        "else:\n",
        "    print(\"   ❌ 模型性能较低，建议调整特征或算法\")\n",
        "\n",
        "print(f\"\\n🔧 改进建议:\")\n",
        "print(\"   1. 可以尝试更多的特征工程（如N-gram、词向量等）\")\n",
        "print(\"   2. 调整模型超参数进行优化\")\n",
        "print(\"   3. 考虑集成学习方法\")\n",
        "print(\"   4. 增加更多的预处理步骤\")\n",
        "\n",
        "print(\"\\n=== 分类建模实验完成！ ===\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
