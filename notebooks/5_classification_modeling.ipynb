{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Step 5: Classification Modeling (Classification Modeling)\n",
        "\n",
        "## Objectives of this notebook:\n",
        "1. Load data processed through sentiment analysis and topic modeling\n",
        "2. Prepare feature engineering (text vectorization and topic distribution features)\n",
        "3. Train classification models (Naive Bayes and Random Forest)\n",
        "4. Model evaluation and comparison\n",
        "5. Generate model evaluation report\n",
        "\n",
        "## Classification Task\n",
        "- **Target variable**: Sentiment labels (Positive, Neutral, Negative)\n",
        "- **feature**: TF-IDF vectors + Topic distribution\n",
        "- **model**: Naive Bayes, Random Forest\n",
        "- **Evaluation metrics**: accuracy, recall, F1 score, confusion matrix\n",
        "\n",
        "## Model Pipeline\n",
        "1. Data preprocessing and feature engineering\n",
        "2. Train/test set split\n",
        "3. Model training and hyperparameter tuning\n",
        "4. Model evaluation and comparison\n",
        "5. Feature importance analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up chart style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_Classification\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load topic analysis and sentiment analysis data\n",
        "print(\"=== Data Loading ===\")\n",
        "\n",
        "# Attempting to load topic analysis results\n",
        "topic_data_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_topic = spark.read.parquet(topic_data_path)\n",
        "    df_topic.cache()\n",
        "    record_count = df_topic.count()\n",
        "    print(f\"‚úÖ Topic analysis data loaded successfullyÔºåtotal {record_count:,} records\")\n",
        "    \n",
        "    print(\"\\nData structure:\")\n",
        "    df_topic.printSchema()\n",
        "    \n",
        "    # Checking required columns\n",
        "    required_cols = ['sentiment_label', 'cleaned_body', 'dominant_topic']\n",
        "    missing_cols = [col for col in required_cols if col not in df_topic.columns]\n",
        "    \n",
        "    if missing_cols:\n",
        "        print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
        "        print(\"Attempting to load other data sources...\")\n",
        "        \n",
        "        # Alternative: load sentiment analysis data\n",
        "        sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "        df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "        \n",
        "        # Need to re-perform sentiment classification\n",
        "        def classify_sentiment(score):\n",
        "            if score is None:\n",
        "                return \"Unknown\"\n",
        "            elif score > 0.1:\n",
        "                return \"Positive\"\n",
        "            elif score < -0.1:\n",
        "                return \"Negative\"\n",
        "            else:\n",
        "                return \"Neutral\"\n",
        "        \n",
        "        classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "        \n",
        "        # Found sentiment score columns\n",
        "        sentiment_col = None\n",
        "        for col in ['compound_score', 'vader_compound', 'sentiment']:\n",
        "            if col in df_sentiment.columns:\n",
        "                sentiment_col = col\n",
        "                break\n",
        "        \n",
        "        if sentiment_col:\n",
        "            df_topic = df_sentiment.withColumn(\n",
        "                \"sentiment_label\",\n",
        "                classify_sentiment_udf(F.col(sentiment_col))\n",
        "            )\n",
        "            df_topic = df_topic.withColumn(\"dominant_topic\", F.lit(0))  # Temporary topic\n",
        "            record_count = df_topic.count()\n",
        "            print(f\"‚úÖ Using sentiment analysis dataÔºåtotal {record_count:,} records\")\n",
        "        else:\n",
        "            raise Exception(\"Sentiment score column not found\")\n",
        "    else:\n",
        "        print(\"‚úÖ All required columns exist\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Data loading failed: {e}\")\n",
        "    print(\"Using cleaned data as fallback...\")\n",
        "    \n",
        "    # Final fallback: using cleaned data\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_base = spark.read.parquet(cleaned_data_path)\n",
        "    \n",
        "    # Using original sentiment column\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"Unknown\"\n",
        "        elif score > 0.1:\n",
        "            return \"Positive\"\n",
        "        elif score < -0.1:\n",
        "            return \"Negative\"\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    df_topic = df_base.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(\"sentiment\"))\n",
        "    ).withColumn(\"dominant_topic\", F.lit(0))  # Temporary topic\n",
        "    \n",
        "    record_count = df_topic.count()\n",
        "    print(f\"‚úÖ Using cleaned dataÔºåtotal {record_count:,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing and feature engineering\n",
        "print(\"=== Data preprocessing and feature engineering ===\")\n",
        "\n",
        "# 1. Checking sentiment label distribution\n",
        "print(\"1. Sentiment label distribution:\")\n",
        "sentiment_dist = df_topic.groupBy(\"sentiment_label\").count().orderBy(F.desc(\"count\"))\n",
        "sentiment_dist.show()\n",
        "\n",
        "# Convert to pandas to view detailed proportions\n",
        "sentiment_dist_pd = sentiment_dist.toPandas()\n",
        "total_count = sentiment_dist_pd['count'].sum()\n",
        "sentiment_dist_pd['percentage'] = (sentiment_dist_pd['count'] / total_count * 100).round(2)\n",
        "\n",
        "print(\"Detailed distribution:\")\n",
        "for _, row in sentiment_dist_pd.iterrows():\n",
        "    label = row['sentiment_label']\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    print(f\"  {label}: {count:,} ({pct}%)\")\n",
        "\n",
        "# 2. Filter out \"Unknown\" label data (if exists)\n",
        "df_filtered = df_topic.filter(F.col(\"sentiment_label\") != \"Unknown\")\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"\\nData size after filtering: {filtered_count:,} records\")\n",
        "\n",
        "# 3. Check if tokenization results exist, if not re-tokenize\n",
        "if 'tokens_cleaned' not in df_filtered.columns:\n",
        "    print(\"3. Re-perform text tokenization...\")\n",
        "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "    \n",
        "    # Tokenization\n",
        "    tokenizer = Tokenizer(inputCol=\"cleaned_body\", outputCol=\"tokens_raw\")\n",
        "    df_tokens = tokenizer.transform(df_filtered)\n",
        "    \n",
        "    # Remove stop words\n",
        "    remover = StopWordsRemover(inputCol=\"tokens_raw\", outputCol=\"tokens_cleaned\")\n",
        "    df_filtered = remover.transform(df_tokens)\n",
        "    print(\"   Tokenization completed\")\n",
        "else:\n",
        "    print(\"3. ‚úÖ Tokenization results already exist\")\n",
        "\n",
        "print(f\"\\nFinal data for modeling: {df_filtered.count():,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build feature vectors\n",
        "print(\"=== Build feature vectors ===\")\n",
        "\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "\n",
        "# 1. Text features: TF-IDF vectorization\n",
        "print(\"1. Build TF-IDF features...\")\n",
        "\n",
        "# CountVectorizer\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=3000,  # Reduce feature dimensions to improve training speed\n",
        "    minDF=3.0        # Minimum document frequency\n",
        ")\n",
        "\n",
        "count_model = count_vectorizer.fit(df_filtered)\n",
        "df_vectorized = count_model.transform(df_filtered)\n",
        "\n",
        "# TF-IDF\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"   TF-IDF vocabulary size: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# 2. Topic feature processing (simplified to avoid OneHotEncoder issues)\n",
        "if 'dominant_topic' in df_tfidf.columns:\n",
        "    print(\"2. Checking topic features...\")\n",
        "    # Checking uniqueness of topic values\n",
        "    unique_topics = df_tfidf.select(\"dominant_topic\").distinct().count()\n",
        "    print(f\"   Found {unique_topics} different topic values\")\n",
        "    \n",
        "    if unique_topics > 1:\n",
        "        print(\"   Adding topic features...\")\n",
        "        from pyspark.ml.feature import OneHotEncoder\n",
        "        \n",
        "        # First convert to numerical index\n",
        "        indexer = StringIndexer(inputCol=\"dominant_topic\", outputCol=\"topic_index\")\n",
        "        indexer_model = indexer.fit(df_tfidf)\n",
        "        df_indexed = indexer_model.transform(df_tfidf)\n",
        "        \n",
        "        # One-hotencoding\n",
        "        encoder = OneHotEncoder(inputCol=\"topic_index\", outputCol=\"topic_features\")\n",
        "        df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "        \n",
        "        # Merge TF-IDF and topic features\n",
        "        assembler = VectorAssembler(\n",
        "            inputCols=[\"tfidf_features\", \"topic_features\"],\n",
        "            outputCol=\"features\"\n",
        "        )\n",
        "        df_features = assembler.transform(df_encoded)\n",
        "        print(\"   ‚úÖ TF-IDF + Topic feature merging completed\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è Too few topic values, skip topic features, use only TF-IDF features\")\n",
        "        df_features = df_tfidf.withColumnRenamed(\"tfidf_features\", \"features\")\n",
        "else:\n",
        "    print(\"2. Using only TF-IDF features...\")\n",
        "    df_features = df_tfidf.withColumnRenamed(\"tfidf_features\", \"features\")\n",
        "\n",
        "print(f\"Final feature data: {df_features.count():,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data and label encoding\n",
        "print(\"=== Preparing Training Data ===\")\n",
        "\n",
        "# Checkingdf_featuresareÂê¶existÔºåif‰∏çexistthenusingdf_tfidf\n",
        "if 'df_features' not in locals():\n",
        "    print(\"‚ö†Ô∏è df_features undefined, using df_tfidf as feature data\")\n",
        "    df_features = df_tfidf.withColumnRenamed(\"tfidf_features\", \"features\")\n",
        "\n",
        "# 1. Convert sentiment labels to numerical indices\n",
        "label_indexer = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\n",
        "indexer_model = label_indexer.fit(df_features)\n",
        "df_labeled = indexer_model.transform(df_features)\n",
        "\n",
        "# Get label mapping\n",
        "labels = indexer_model.labels\n",
        "print(f\"Label mapping: {dict(enumerate(labels))}\")\n",
        "\n",
        "# 2. Split training and test sets\n",
        "print(\"2. Split training and test sets...\")\n",
        "train_data, test_data = df_labeled.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "train_count = train_data.count()\n",
        "test_count = test_data.count()\n",
        "\n",
        "print(f\"Training set: {train_count:,} records\")\n",
        "print(f\"Test set: {test_count:,} records\")\n",
        "\n",
        "# Cache data to improve subsequent training speed\n",
        "train_data.cache()\n",
        "test_data.cache()\n",
        "\n",
        "# 3. Check label distribution in training set\n",
        "print(\"\\n3. Training set label distribution:\")\n",
        "train_label_dist = train_data.groupBy(\"sentiment_label\").count().orderBy(F.desc(\"count\"))\n",
        "train_label_dist.show()\n",
        "\n",
        "print(\"Test set label distribution:\")\n",
        "test_label_dist = test_data.groupBy(\"sentiment_label\").count().orderBy(F.desc(\"count\"))\n",
        "test_label_dist.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Naive Bayes Model\n",
        "print(\"=== Training Naive Bayes Model ===\")\n",
        "\n",
        "# CheckingrequiredvariableareÂê¶exist\n",
        "if 'train_data' not in locals() or 'test_data' not in locals():\n",
        "    print(\"‚ùå Training/test data not prepared, please run the previous cell first\")\n",
        "else:\n",
        "    # 1. create Naive Bayes classifier\n",
        "    nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"nb_prediction\")\n",
        "\n",
        "    print(\"1. Start training Naive Bayes model...\")\n",
        "    nb_model = nb.fit(train_data)\n",
        "    print(\"   ‚úÖ Naive Bayes training completed\")\n",
        "\n",
        "    # 2. Perform prediction on test set\n",
        "    print(\"2. Prediction on test set...\")\n",
        "    nb_predictions = nb_model.transform(test_data)\n",
        "\n",
        "    # 3. Evaluate Naive Bayes model\n",
        "    print(\"3. Evaluate Naive Bayes model:\")\n",
        "\n",
        "    # accuracy\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"accuracy\"\n",
        "    )\n",
        "    nb_accuracy = evaluator_accuracy.evaluate(nb_predictions)\n",
        "\n",
        "    # F1 score\n",
        "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"f1\"\n",
        "    )\n",
        "    nb_f1 = evaluator_f1.evaluate(nb_predictions)\n",
        "\n",
        "    # Weighted precision\n",
        "    evaluator_precision = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"weightedPrecision\"\n",
        "    )\n",
        "    nb_precision = evaluator_precision.evaluate(nb_predictions)\n",
        "\n",
        "    # Weighted recall\n",
        "    evaluator_recall = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"nb_prediction\", metricName=\"weightedRecall\"\n",
        "    )\n",
        "    nb_recall = evaluator_recall.evaluate(nb_predictions)\n",
        "\n",
        "    print(f\"   accuracy: {nb_accuracy:.4f}\")\n",
        "    print(f\"   F1 score: {nb_f1:.4f}\")\n",
        "    print(f\"   Weighted precision: {nb_precision:.4f}\")\n",
        "    print(f\"   Weighted recall: {nb_recall:.4f}\")\n",
        "\n",
        "    # Saving Naive Bayes results\n",
        "    nb_results = {\n",
        "        'model': 'Naive Bayes',\n",
        "        'accuracy': nb_accuracy,\n",
        "        'f1': nb_f1,\n",
        "        'precision': nb_precision,\n",
        "        'recall': nb_recall\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Random Forest Model\n",
        "print(\"=== Training Random Forest Model ===\")\n",
        "\n",
        "# 1. create Random Forest classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\", \n",
        "    labelCol=\"label\", \n",
        "    predictionCol=\"rf_prediction\",\n",
        "    numTrees=50,        # Number of trees, can be adjusted according to needs\n",
        "    maxDepth=10,        # Maximum depth\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"1. Start training Random Forest model...\")\n",
        "rf_model = rf.fit(train_data)\n",
        "print(\"   ‚úÖ Random Forest training completed\")\n",
        "\n",
        "# 2. Perform prediction on test set\n",
        "print(\"2. Prediction on test set...\")\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "\n",
        "# 3. Evaluate Random Forest model\n",
        "print(\"3. Evaluate Random Forest model:\")\n",
        "\n",
        "# accuracy\n",
        "rf_accuracy = evaluator_accuracy.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "# F1 score\n",
        "rf_f1 = evaluator_f1.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "# Weighted precision\n",
        "rf_precision = evaluator_precision.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "# Weighted recall\n",
        "rf_recall = evaluator_recall.setParams(predictionCol=\"rf_prediction\").evaluate(rf_predictions)\n",
        "\n",
        "print(f\"   accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"   F1 score: {rf_f1:.4f}\")\n",
        "print(f\"   Weighted precision: {rf_precision:.4f}\")\n",
        "print(f\"   Weighted recall: {rf_recall:.4f}\")\n",
        "\n",
        "# Saving Random Forest results\n",
        "rf_results = {\n",
        "    'model': 'Random Forest',\n",
        "    'accuracy': rf_accuracy,\n",
        "    'f1': rf_f1,\n",
        "    'precision': rf_precision,\n",
        "    'recall': rf_recall\n",
        "}\n",
        "\n",
        "# 4. Feature importance analysis\n",
        "print(\"\\n4. Feature importance analysis:\")\n",
        "feature_importances = rf_model.featureImportances\n",
        "print(f\"Feature importance vector length: {len(feature_importances)}\")\n",
        "\n",
        "# Get most important features (if text features)\n",
        "if len(feature_importances) > 0:\n",
        "    # Convert to pandas for analysis\n",
        "    importance_array = feature_importances.toArray()\n",
        "    \n",
        "    # If features are mainlylyly vocabulary features, display most important words\n",
        "    vocab_size = len(count_model.vocabulary)\n",
        "    if vocab_size <= len(importance_array):\n",
        "        word_importance = [(count_model.vocabulary[i], importance_array[i]) \n",
        "                          for i in range(min(vocab_size, len(importance_array)))]\n",
        "        word_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        print(\"   Top 10 most important vocabulary features:\")\n",
        "        for word, importance in word_importance[:10]:\n",
        "            print(f\"     {word}: {importance:.6f}\")\n",
        "    \n",
        "    print(f\"   Sum of feature importance: {sum(importance_array):.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparison and Evaluation Report\n",
        "print(\"=== Model Comparison and Evaluation Report ===\")\n",
        "\n",
        "# 1. Model performance comparison\n",
        "results_comparison = pd.DataFrame([nb_results, rf_results])\n",
        "print(\"1. Model performance comparison:\")\n",
        "print(results_comparison.round(4))\n",
        "\n",
        "# 2. Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
        "metric_names = ['accuracy', 'F1 score', 'Weighted precision', 'Weighted recall']\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    ax = axes[i//2, i%2]\n",
        "    \n",
        "    values = [nb_results[metric], rf_results[metric]]\n",
        "    models = ['Naive Bayes', 'Random Forest']\n",
        "    colors = ['lightblue', 'lightgreen']\n",
        "    \n",
        "    bars = ax.bar(models, values, color=colors, alpha=0.8)\n",
        "    ax.set_title(f'{name}comparison', fontweight='bold')\n",
        "    ax.set_ylabel(name)\n",
        "    ax.set_ylim(0, 1)\n",
        "    \n",
        "    # Add numerical labels on bar chart\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Model performance comparison', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# 3. Determine best model\n",
        "best_model_name = 'Naive Bayes' if nb_f1 > rf_f1 else 'Random Forest'\n",
        "best_predictions = nb_predictions if nb_f1 > rf_f1 else rf_predictions\n",
        "best_pred_col = 'nb_prediction' if nb_f1 > rf_f1 else 'rf_prediction'\n",
        "\n",
        "print(f\"\\n3. Best model: {best_model_name} (F1 score: {max(nb_f1, rf_f1):.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed Classification Report and Confusion Matrix\n",
        "print(\"=== Detailed Classification Report ===\")\n",
        "\n",
        "# 1. Generate confusion matrix data\n",
        "def get_confusion_matrix_data(predictions, prediction_col):\n",
        "    \"\"\"Generate confusion matrix data\"\"\"\n",
        "    # Collect prediction results\n",
        "    results = predictions.select(\"label\", prediction_col).collect()\n",
        "    \n",
        "    y_true = [row[\"label\"] for row in results]\n",
        "    y_pred = [row[prediction_col] for row in results]\n",
        "    \n",
        "    return y_true, y_pred\n",
        "\n",
        "# 2. Generate detailed report for best model\n",
        "print(f\"forBest model ({best_model_name}) Generatedetailedreport:\")\n",
        "\n",
        "y_true, y_pred = get_confusion_matrix_data(best_predictions, best_pred_col)\n",
        "\n",
        "# Convert numerical labels back to text labels\n",
        "label_mapping = {i: label for i, label in enumerate(labels)}\n",
        "y_true_labels = [label_mapping[int(label)] for label in y_true]\n",
        "y_pred_labels = [label_mapping[int(pred)] for pred in y_pred]\n",
        "\n",
        "# Generate classification report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\n1. classification report:\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, zero_division=0))\n",
        "\n",
        "# 2. Confusion matrix\n",
        "print(\"2. Confusion matrix:\")\n",
        "cm = confusion_matrix(y_true_labels, y_pred_labels, labels=labels)\n",
        "print(cm)\n",
        "\n",
        "# 3. Visualize confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.title(f'{best_model_name} - Confusion matrix', fontweight='bold')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()\n",
        "\n",
        "# 4. Analyze performance by class\n",
        "print(\"\\n3. Performance analysis by class:\")\n",
        "for i, label in enumerate(labels):\n",
        "    true_positives = cm[i, i]\n",
        "    false_positives = sum(cm[:, i]) - true_positives\n",
        "    false_negatives = sum(cm[i, :]) - true_positives\n",
        "    \n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    print(f\"  {label}:\")\n",
        "    print(f\"    precision: {precision:.4f}\")\n",
        "    print(f\"    recall: {recall:.4f}\")\n",
        "    print(f\"    F1 score: {f1:.4f}\")\n",
        "    print(f\"    Support sample count: {sum(cm[i, :])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Model Evaluation Results\n",
        "print(\"=== Save Model Evaluation Results ===\")\n",
        "\n",
        "# 1. Create complete evaluation report\n",
        "evaluation_report = {\n",
        "    \"experiment_info\": {\n",
        "        \"dataset_size\": df_filtered.count(),\n",
        "        \"train_size\": train_count,\n",
        "        \"test_size\": test_count,\n",
        "        \"feature_size\": len(count_model.vocabulary),\n",
        "        \"num_classes\": len(labels),\n",
        "        \"class_labels\": labels\n",
        "    },\n",
        "    \"models\": {\n",
        "        \"naive_bayes\": nb_results,\n",
        "        \"random_forest\": rf_results\n",
        "    },\n",
        "    \"best_model\": {\n",
        "        \"name\": best_model_name,\n",
        "        \"metrics\": nb_results if best_model_name == 'Naive Bayes' else rf_results\n",
        "    },\n",
        "    \"class_distribution\": {\n",
        "        \"train\": train_label_dist.toPandas().to_dict('records'),\n",
        "        \"test\": test_label_dist.toPandas().to_dict('records')\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. Save evaluation report\n",
        "import json\n",
        "report_path = \"/home/jovyan/work/data/processed/classification_evaluation_report.json\"\n",
        "\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(evaluation_report, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "print(f\"‚úÖ Model evaluation report saved to: {report_path}\")\n",
        "\n",
        "# 3. Save best model prediction results (samples)\n",
        "sample_predictions = best_predictions.select(\n",
        "    \"id\", \"cleaned_body\", \"sentiment_label\", \"label\", best_pred_col\n",
        ").limit(1000)\n",
        "\n",
        "sample_path = \"/home/jovyan/work/data/processed/classification_sample_predictions.parquet\"\n",
        "sample_predictions.write.mode(\"overwrite\").parquet(sample_path)\n",
        "\n",
        "print(f\"‚úÖ Sample prediction results saved to: {sample_path}\")\n",
        "\n",
        "# 4. Generate final summary\n",
        "print(\"\\n=== Classification modeling summary ===\")\n",
        "print(f\"üéØ Experiment completion overview:\")\n",
        "print(f\"   Dataset size: {df_filtered.count():,} records\")\n",
        "print(f\"   Feature dimensions: {len(count_model.vocabulary):,}\")\n",
        "print(f\"   Number of classes: {len(labels)}\")\n",
        "print(f\"   Training set: {train_count:,} | Test set: {test_count:,}\")\n",
        "\n",
        "print(f\"\\nüìä Model performance comparison:\")\n",
        "print(f\"   Naive Bayes    - accuracy: {nb_accuracy:.4f} | F1: {nb_f1:.4f}\")\n",
        "print(f\"   Random Forest  - accuracy: {rf_accuracy:.4f} | F1: {rf_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nüèÜ Best model: {best_model_name}\")\n",
        "best_metrics = nb_results if best_model_name == 'Naive Bayes' else rf_results\n",
        "print(f\"   accuracy: {best_metrics['accuracy']:.4f}\")\n",
        "print(f\"   F1 score: {best_metrics['f1']:.4f}\")\n",
        "print(f\"   precision: {best_metrics['precision']:.4f}\")\n",
        "print(f\"   recall: {best_metrics['recall']:.4f}\")\n",
        "\n",
        "print(f\"\\nüí° Results analysis:\")\n",
        "if best_metrics['accuracy'] > 0.7:\n",
        "    print(\"   ‚úÖ Model performance is good, can distinguish sentiment classes well\")\n",
        "elif best_metrics['accuracy'] > 0.6:\n",
        "    print(\"   ‚ö†Ô∏è Model performance is moderate, room for improvement\")\n",
        "else:\n",
        "    print(\"   ‚ùå Model performance is low, suggest adjust features or algorithms\")\n",
        "\n",
        "print(f\"\\nüîß Improvement suggestions:\")\n",
        "print(\"   1. Can try more feature engineering (such as N-gram, word vectors, etc.)\")\n",
        "print(\"   2. Adjust model hyperparameters for optimization\")\n",
        "print(\"   3. Consider ensemble learning methodsss\")\n",
        "print(\"   4. Add more preprocessing steps\")\n",
        "\n",
        "print(\"\\n=== Classification modeling experiment completed! ===\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
