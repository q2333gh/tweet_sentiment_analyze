{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 第三步：探索性数据分析与情感分析 (EDA & Sentiment Analysis)\n",
        "\n",
        "本 notebook 的目标：\n",
        "1. 加载清洗后的数据\n",
        "2. 进行深度的探索性数据分析\n",
        "3. 使用VADER进行情感分析\n",
        "4. 分析情感趋势和分布\n",
        "5. 生成词云和可视化图表\n",
        "\n",
        "## 分析重点\n",
        "- 时间序列分析：情感随时间的变化\n",
        "- 子版块分析：不同社区的情感倾向\n",
        "- 词频分析：高频词汇和情感关联\n",
        "- 情感分布：正面、负面、中性评论的比例\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"库导入完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 初始化 Spark Session 并加载清洗后的数据\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_EDA_Sentiment\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "\n",
        "# 加载清洗后的数据\n",
        "cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "df_cleaned = spark.read.parquet(cleaned_data_path)\n",
        "df_cleaned.cache()\n",
        "\n",
        "print(f\"清洗后数据加载完成，共 {df_cleaned.count():,} 条记录\")\n",
        "print(\"\\n数据结构:\")\n",
        "df_cleaned.printSchema()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
