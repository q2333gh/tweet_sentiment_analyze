{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Step 3: Exploratory Data Analysis & Sentiment Analysis (EDA & Sentiment Analysis)\n",
    "\n",
    "Objectives of this notebook:\n",
    "1. Load cleaned data\n",
    "2. Conduct in-depth exploratory data analysis\n",
    "3. Perform sentiment analysis using VADER\n",
    "4. Analyze sentiment trends and distributions\n",
    "5. Generate word clouds and visualization charts\n",
    "\n",
    "## Analysis Focus\n",
    "- Time series analysis: sentiment changes over time\n",
    "- Subreddit analysis: sentiment tendencies across different communities\n",
    "- Word frequency analysis: high-frequency words and sentiment correlations\n",
    "- Sentiment distribution: proportions of positive, negative, and neutral comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set chart style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Attempting to load data from: /home/jovyan/work/data/processed/cleaned_comments.parquet\n",
      "‚úÖ Cleaned data loaded successfully, total 459,171 records\n",
      "\n",
      "Data structure:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit.name: string (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- cleaned_body: string (nullable = true)\n",
      " |-- tokens_cleaned: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n",
      "\n",
      "Data preview (first 3 rows):\n",
      "+-------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|id     |subreddit.name|cleaned_body                                                                                                                                                                                                                                                                                                                                                                                                    |sentiment|\n",
      "+-------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|c0i84kh|australia     |a single year cannot really be used to prove anything, this just makes global warming look flakey. instead, they should stick to the historical trends which clearly show climate change is happening.                                                                                                                                                                                                          |0.5106   |\n",
      "|c0ifmz4|environment   |paris: the world organisation for animal health oie is to study the impact of meat output on climate change in the light of debate about meat s contribution to greenhouse emissions, the paris based body said on thursday. the initiative, which will be the oie s first on an environmental issue, follows requests from its member countries to look at a question that has prompted calls to eat less meat.|0.0      |\n",
      "|c0ijeyo|science       |ok. discount climate change completely. we re still going to run out of oil, therefore energy conservation and research into alternative energy sources is still a good thing.                                                                                                                                                                                                                                  |0.7269   |\n",
      "+-------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session and load cleaned data\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TweetAnalysis_EDA_Sentiment\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# Load cleaned data (based on output path from step 2)\n",
    "cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
    "print(f\"Attempting to load data from: {cleaned_data_path}\")\n",
    "\n",
    "try:\n",
    "    df_cleaned = spark.read.parquet(cleaned_data_path)\n",
    "    df_cleaned.cache()\n",
    "    record_count = df_cleaned.count()\n",
    "    print(f\"‚úÖ Cleaned data loaded successfully, total {record_count:,} records\")\n",
    "    print(\"\\nData structure:\")\n",
    "    df_cleaned.printSchema()\n",
    "    \n",
    "    # Display preview of first few rows\n",
    "    print(\"\\nData preview (first 3 rows):\")\n",
    "    df_cleaned.select(\"id\", \"`subreddit.name`\", \"cleaned_body\", \"sentiment\").show(3, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data loading failed: {e}\")\n",
    "    print(\"Please ensure you have completed the data cleaning notebook from step 2\")\n",
    "    # Create an empty DataFrame as fallback\n",
    "    df_cleaned = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Data Overview ===\n",
      "1. Basic data information:\n",
      "Total records: 459,171\n",
      "Total columns: 9\n",
      "\n",
      "2. Data completeness:\n",
      "  id: 100.0% complete\n",
      "  subreddit.name: 100.0% complete\n",
      "  created_utc: 100.0% complete\n",
      "  timestamp: 100.0% complete\n",
      "  body: 100.0% complete\n",
      "  cleaned_body: 100.0% complete\n",
      "  tokens_cleaned: 100.0% complete\n",
      "  sentiment: 98.9% complete\n",
      "  score: 100.0% complete\n",
      "\n",
      "3. Sentiment score distribution:\n",
      "+-------+--------------------+\n",
      "|summary|           sentiment|\n",
      "+-------+--------------------+\n",
      "|  count|              454264|\n",
      "|   mean|-0.00644086346265...|\n",
      "| stddev|  0.6588389261269937|\n",
      "|    min|             -0.9999|\n",
      "|    max|              0.9999|\n",
      "+-------+--------------------+\n",
      "\n",
      "4. Subreddit distribution (Top 10):\n",
      "+--------------+-----+\n",
      "|subreddit.name|count|\n",
      "+--------------+-----+\n",
      "|politics      |36989|\n",
      "|worldnews     |35283|\n",
      "|askreddit     |25863|\n",
      "|news          |9524 |\n",
      "|collapse      |9490 |\n",
      "|futurology    |8904 |\n",
      "|science       |7063 |\n",
      "|environment   |6819 |\n",
      "|canada        |6722 |\n",
      "|australia     |5993 |\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic data overview and statistics\n",
    "print(\"=== Basic Data Overview ===\")\n",
    "\n",
    "# Verify if data was loaded successfully\n",
    "if df_cleaned is None:\n",
    "    print(\"‚ùå Data not loaded, please run the previous cell and ensure data loads successfully\")\n",
    "else:\n",
    "    # 1. Basic data information\n",
    "    print(\"1. Basic data information:\")\n",
    "    print(f\"Total records: {df_cleaned.count():,}\")\n",
    "    print(f\"Total columns: {len(df_cleaned.columns)}\")\n",
    "\n",
    "    # 2. Data completeness check for each column\n",
    "    print(\"\\n2. Data completeness:\")\n",
    "    for col_name in df_cleaned.columns:\n",
    "        # Handle column names containing dots\n",
    "        if \".\" in col_name:\n",
    "            null_count = df_cleaned.filter(F.col(f\"`{col_name}`\").isNull()).count()\n",
    "        else:\n",
    "            null_count = df_cleaned.filter(F.col(col_name).isNull()).count()\n",
    "        total_count = df_cleaned.count()\n",
    "        print(f\"  {col_name}: {((total_count - null_count) / total_count * 100):.1f}% complete\")\n",
    "\n",
    "    # 3. Sentiment score statistics\n",
    "    print(\"\\n3. Sentiment score distribution:\")\n",
    "    sentiment_stats = df_cleaned.select(\"sentiment\").describe()\n",
    "    sentiment_stats.show()\n",
    "\n",
    "    # 4. Subreddit distribution\n",
    "    print(\"4. Subreddit distribution (Top 10):\")\n",
    "    subreddit_dist = df_cleaned.groupBy(\"`subreddit.name`\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "    subreddit_dist.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VADER Sentiment Analysis ===\n",
      "Applying VADER sentiment analysis...\n",
      "VADER sentiment analysis completed!\n",
      "\n",
      "Original sentiment score vs VADER score comparison:\n"
     ]
    }
   ],
   "source": [
    "# Re-analyze sentiment using VADER (improve original sentiment scores)\n",
    "print(\"=== VADER Sentiment Analysis ===\")\n",
    "\n",
    "# Verify if data is available\n",
    "if df_cleaned is None:\n",
    "    print(\"‚ùå Data not loaded, please run previous cells and ensure data loads successfully\")\n",
    "else:\n",
    "    # Create VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def analyze_sentiment_vader(text):\n",
    "        \"\"\"Perform sentiment analysis using VADER\"\"\"\n",
    "        if text is None:\n",
    "            return None\n",
    "        scores = analyzer.polarity_scores(str(text))\n",
    "        return scores['compound']  # Return compound sentiment score (-1 to 1)\n",
    "\n",
    "    # Create UDF for Spark\n",
    "    sentiment_udf = F.udf(analyze_sentiment_vader, DoubleType())\n",
    "\n",
    "    # Apply VADER sentiment analysis to cleaned text\n",
    "    print(\"Applying VADER sentiment analysis...\")\n",
    "    df_with_vader = df_cleaned.withColumn(\"vader_sentiment\", sentiment_udf(F.col(\"cleaned_body\")))\n",
    "\n",
    "    # Cache results\n",
    "    df_with_vader.cache()\n",
    "\n",
    "    print(\"VADER sentiment analysis completed!\")\n",
    "\n",
    "    # Compare original sentiment scores vs VADER scores\n",
    "    print(\"\\nOriginal sentiment score vs VADER score comparison:\")\n",
    "    comparison = df_with_vader.select(\"sentiment\", \"vader_sentiment\").filter(\n",
    "        F.col(\"sentiment\").isNotNull() & F.col(\"vader_sentiment\").isNotNull()\n",
    "    ).limit(10)\n",
    "    comparison.show()\n",
    "\n",
    "    # VADER sentiment score statistics\n",
    "    print(\"\\nVADER sentiment score statistics:\")\n",
    "    vader_stats = df_with_vader.select(\"vader_sentiment\").describe()\n",
    "    vader_stats.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution analysis and visualization\n",
    "print(\"=== Sentiment Distribution Analysis ===\")\n",
    "\n",
    "# Verify if VADER analysis is completed\n",
    "try:\n",
    "    df_with_vader\n",
    "    print(\"Preparing visualization data...\")\n",
    "    \n",
    "    # Create sentiment classification\n",
    "    def categorize_sentiment(score):\n",
    "        if score is None:\n",
    "            return \"Unknown\"\n",
    "        elif score > 0.05:\n",
    "            return \"Positive\"\n",
    "        elif score < -0.05:\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "\n",
    "    categorize_udf = F.udf(categorize_sentiment, StringType())\n",
    "\n",
    "    # Apply sentiment classification\n",
    "    df_categorized = df_with_vader.withColumn(\"sentiment_category\", categorize_udf(F.col(\"vader_sentiment\")))\n",
    "\n",
    "    # Convert to Pandas for analysis\n",
    "    sentiment_dist = df_categorized.groupBy(\"sentiment_category\").count().toPandas()\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(sentiment_dist)\n",
    "\n",
    "    # Plot sentiment distribution pie chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Pie chart\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "    plt.pie(sentiment_dist['count'], labels=sentiment_dist['sentiment_category'], \n",
    "            autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    plt.title('Climate Change Comment Sentiment Distribution')\n",
    "\n",
    "    # Bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars = plt.bar(sentiment_dist['sentiment_category'], sentiment_dist['count'], color=colors)\n",
    "    plt.title('Comment Count by Sentiment Category')\n",
    "    plt.xlabel('Sentiment Category')\n",
    "    plt.ylabel('Comment Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add value labels on bar chart\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{int(height):,}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Sentiment distribution visualization completed!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå VADER sentiment analysis not completed, please run previous cells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â≠êÁâàÂùósentiment analysis\n",
    "print(\"=== Â≠êÁâàÂùósentiment analysis ===\")\n",
    "\n",
    "# verifyingÊÉÖÊÑüclassificationdataÊòØÂê¶ÂèØÁî®\n",
    "try:\n",
    "    df_categorized\n",
    "    \n",
    "    # CalculatingÂêÑÂ≠êÁâàÂùóÁöÑÂπ≥ÂùáÊÉÖÊÑüÂàÜÊï∞\n",
    "    subreddit_sentiment = df_categorized.groupBy(\"`subreddit.name`\").agg(\n",
    "        F.count(\"*\").alias(\"comment_count\"),\n",
    "        F.avg(\"vader_sentiment\").alias(\"avg_sentiment\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Negative\", 1).otherwise(0)).alias(\"negative_count\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Neutral\", 1).otherwise(0)).alias(\"neutral_count\")\n",
    "    ).filter(F.col(\"comment_count\") >= 1000).orderBy(F.desc(\"comment_count\"))\n",
    "\n",
    "    # converting‰∏∫PandasÂπ∂Displaying\n",
    "    subreddit_sentiment_pd = subreddit_sentiment.toPandas()\n",
    "    print(\"mainÂ≠êÁâàÂùósentiment analysisresults:\")\n",
    "    print(subreddit_sentiment_pd.head(10))\n",
    "\n",
    "    # ÁªòÂà∂Â≠êÁâàÂùóÊÉÖÊÑüÂØπÊØîÂõæ\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # selectingÂâç10‰∏™ÊúÄÊ¥ªË∑ÉÁöÑÂ≠êÁâàÂùó\n",
    "    top_subreddits = subreddit_sentiment_pd.head(10)\n",
    "\n",
    "    # Â≠êÁâàÂùóÂπ≥ÂùáÊÉÖÊÑüÂàÜÊï∞\n",
    "    plt.subplot(2, 1, 1)\n",
    "    bars = plt.bar(range(len(top_subreddits)), top_subreddits['avg_sentiment'])\n",
    "    plt.title('mainÂ≠êÁâàÂùóÂπ≥ÂùáÊÉÖÊÑüÂàÜÊï∞')\n",
    "    plt.xlabel('Â≠êÁâàÂùó')\n",
    "    plt.ylabel('Âπ≥ÂùáÊÉÖÊÑüÂàÜÊï∞')\n",
    "    plt.xticks(range(len(top_subreddits)), top_subreddits['subreddit.name'], rotation=45, ha='right')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='NeutralÁ∫ø')\n",
    "    plt.legend()\n",
    "\n",
    "    # ‰∏∫Êü±Áä∂ÂõæaddingÈ¢úËâ≤ÔºàÊ≠£Èù¢ÁªøËâ≤ÔºåË¥üÈù¢Á∫¢Ëâ≤ÔºåNeutralÁÅ∞Ëâ≤Ôºâ\n",
    "    for i, bar in enumerate(bars):\n",
    "        if top_subreddits.iloc[i]['avg_sentiment'] > 0.05:\n",
    "            bar.set_color('green')\n",
    "        elif top_subreddits.iloc[i]['avg_sentiment'] < -0.05:\n",
    "            bar.set_color('red')\n",
    "        else:\n",
    "            bar.set_color('gray')\n",
    "\n",
    "    # Â≠êÁâàÂùóËØÑËÆ∫Êï∞Èáè\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(range(len(top_subreddits)), top_subreddits['comment_count'], color='skyblue')\n",
    "    plt.title('mainÂ≠êÁâàÂùóËØÑËÆ∫Êï∞Èáè')\n",
    "    plt.xlabel('Â≠êÁâàÂùó')\n",
    "    plt.ylabel('ËØÑËÆ∫Êï∞Èáè')\n",
    "    plt.xticks(range(len(top_subreddits)), top_subreddits['subreddit.name'], rotation=45, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Â≠êÁâàÂùósentiment analysisÂèØËßÜÂåñcompletingÔºÅ\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå ÊÉÖÊÑüclassificationdataÊú™PreparingÂ•ΩÔºåËØ∑ÂÖàrunningÂâçÈù¢ÁöÑcells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êó∂Èó¥Â∫èÂàósentiment analysis\n",
    "print(\"=== Êó∂Èó¥Â∫èÂàósentiment analysis ===\")\n",
    "\n",
    "# verifyingÊÉÖÊÑüclassificationdataÊòØÂê¶ÂèØÁî®\n",
    "try:\n",
    "    df_categorized\n",
    "    \n",
    "    # ‰ªétimestamp‰∏≠ÊèêÂèñÂπ¥‰ªΩ\n",
    "    df_with_year = df_categorized.withColumn(\"year\", F.year(F.col(\"timestamp\")))\n",
    "    \n",
    "    # ÊåâÂπ¥‰ªΩAnalyzingÊÉÖÊÑüË∂ãÂäø\n",
    "    yearly_sentiment = df_with_year.groupBy(\"year\").agg(\n",
    "        F.count(\"*\").alias(\"total_comments\"),\n",
    "        F.avg(\"vader_sentiment\").alias(\"avg_sentiment\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Negative\", 1).otherwise(0)).alias(\"negative_count\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Neutral\", 1).otherwise(0)).alias(\"neutral_count\")\n",
    "    ).orderBy(\"year\")\n",
    "    \n",
    "    # converting‰∏∫PandasËøõË°åÂèØËßÜÂåñ\n",
    "    yearly_sentiment_pd = yearly_sentiment.toPandas()\n",
    "    print(\"Âπ¥Â∫¶ÊÉÖÊÑüË∂ãÂäøAnalyzing:\")\n",
    "    print(yearly_sentiment_pd)\n",
    "    \n",
    "    # ÁªòÂà∂Êó∂Èó¥Â∫èÂàóÂõæ\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Âπ¥Â∫¶ËØÑËÆ∫Êï∞ÈáèË∂ãÂäø\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['total_comments'], marker='o', linewidth=2)\n",
    "    plt.title('Âπ¥Â∫¶ËØÑËÆ∫Êï∞ÈáèË∂ãÂäø')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('ËØÑËÆ∫Êï∞Èáè')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Âπ¥Â∫¶Âπ≥ÂùáÊÉÖÊÑüË∂ãÂäø\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['avg_sentiment'], \n",
    "             marker='o', linewidth=2, color='orange')\n",
    "    plt.title('Âπ¥Â∫¶Âπ≥ÂùáÊÉÖÊÑüË∂ãÂäø')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('Âπ≥ÂùáÊÉÖÊÑüÂàÜÊï∞')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='NeutralÁ∫ø')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ÊÉÖÊÑüclassÂà´Âπ¥Â∫¶ÂàÜÂ∏É\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['positive_count'], \n",
    "             marker='o', label='Ê≠£Èù¢', color='green', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['negative_count'], \n",
    "             marker='s', label='Ë¥üÈù¢', color='red', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['neutral_count'], \n",
    "             marker='^', label='Neutral', color='gray', linewidth=2)\n",
    "    plt.title('Âπ¥Â∫¶ÊÉÖÊÑüclassÂà´ÂàÜÂ∏É')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('ËØÑËÆ∫Êï∞Èáè')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ÊÉÖÊÑüÊØî‰æãÂπ¥Â∫¶ÂèòÂåñ\n",
    "    plt.subplot(2, 2, 4)\n",
    "    yearly_sentiment_pd['positive_ratio'] = yearly_sentiment_pd['positive_count'] / yearly_sentiment_pd['total_comments']\n",
    "    yearly_sentiment_pd['negative_ratio'] = yearly_sentiment_pd['negative_count'] / yearly_sentiment_pd['total_comments']\n",
    "    yearly_sentiment_pd['neutral_ratio'] = yearly_sentiment_pd['neutral_count'] / yearly_sentiment_pd['total_comments']\n",
    "    \n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['positive_ratio'], \n",
    "             marker='o', label='Ê≠£Èù¢ÊØî‰æã', color='green', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['negative_ratio'], \n",
    "             marker='s', label='Ë¥üÈù¢ÊØî‰æã', color='red', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['neutral_ratio'], \n",
    "             marker='^', label='NeutralÊØî‰æã', color='gray', linewidth=2)\n",
    "    plt.title('Âπ¥Â∫¶ÊÉÖÊÑüÊØî‰æãÂèòÂåñ')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('ÊØî‰æã')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Êó∂Èó¥Â∫èÂàósentiment analysiscompletingÔºÅ\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå ÊÉÖÊÑüclassificationdataÊú™PreparingÂ•ΩÔºåËØ∑ÂÖàrunningÂâçÈù¢ÁöÑcells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis and word cloud generation\n",
    "print(\"=== Word Frequency Analysis and Word Cloud Generation ===\")\n",
    "\n",
    "# Verify if classification data is available\n",
    "try:\n",
    "    df_categorized\n",
    "    \n",
    "    # Extract all words from tokens_cleaned\n",
    "    print(\"Analyzing word frequency...\")\n",
    "\n",
    "    # Explode all tokens and calculate frequency\n",
    "    all_tokens = df_categorized.select(F.explode(F.col(\"tokens_cleaned\")).alias(\"token\"))\n",
    "    word_freq = all_tokens.groupBy(\"token\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "    # Get top 50 high-frequency words\n",
    "    top_words = word_freq.limit(50).toPandas()\n",
    "    print(\"Top 20 high-frequency words:\")\n",
    "    print(top_words.head(20))\n",
    "\n",
    "    # Analyze word frequency for different sentiment categories separately\n",
    "    print(\"\\nAnalyzing word frequency for different sentiment categories...\")\n",
    "\n",
    "    # Word frequency for positive comments\n",
    "    positive_tokens = df_categorized.filter(F.col(\"sentiment_category\") == \"Positive\").select(\n",
    "        F.explode(F.col(\"tokens_cleaned\")).alias(\"token\")\n",
    "    )\n",
    "    positive_word_freq = positive_tokens.groupBy(\"token\").count().orderBy(F.desc(\"count\")).limit(30).toPandas()\n",
    "\n",
    "    # Word frequency for negative comments\n",
    "    negative_tokens = df_categorized.filter(F.col(\"sentiment_category\") == \"Negative\").select(\n",
    "        F.explode(F.col(\"tokens_cleaned\")).alias(\"token\")\n",
    "    )\n",
    "    negative_word_freq = negative_tokens.groupBy(\"token\").count().orderBy(F.desc(\"count\")).limit(30).toPandas()\n",
    "\n",
    "    # Generate word clouds\n",
    "    print(\"Generating word clouds...\")\n",
    "\n",
    "    # Prepare word cloud data\n",
    "    word_freq_dict = dict(zip(top_words['token'], top_words['count']))\n",
    "\n",
    "    # Generate overall word cloud\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Overall word cloud\n",
    "    plt.subplot(2, 2, 1)\n",
    "    wordcloud = WordCloud(width=400, height=400, background_color='white', \n",
    "                         max_words=100, colormap='viridis').generate_from_frequencies(word_freq_dict)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title('Overall High-Frequency Word Cloud')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Positive comments word cloud\n",
    "    plt.subplot(2, 2, 2)\n",
    "    positive_dict = dict(zip(positive_word_freq['token'], positive_word_freq['count']))\n",
    "    positive_wordcloud = WordCloud(width=400, height=400, background_color='white',\n",
    "                                  max_words=50, colormap='Greens').generate_from_frequencies(positive_dict)\n",
    "    plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Positive Comments Word Cloud')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Negative comments word cloud\n",
    "    plt.subplot(2, 2, 3)\n",
    "    negative_dict = dict(zip(negative_word_freq['token'], negative_word_freq['count']))\n",
    "    negative_wordcloud = WordCloud(width=400, height=400, background_color='white',\n",
    "                                  max_words=50, colormap='Reds').generate_from_frequencies(negative_dict)\n",
    "    plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Negative Comments Word Cloud')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Word frequency comparison chart\n",
    "    plt.subplot(2, 2, 4)\n",
    "    top_15_words = top_words.head(15)\n",
    "    plt.barh(range(len(top_15_words)), top_15_words['count'])\n",
    "    plt.yticks(range(len(top_15_words)), top_15_words['token'])\n",
    "    plt.xlabel('Word Frequency')\n",
    "    plt.title('Top 15 High-Frequency Words')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Word frequency analysis and word cloud generation completed!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå Classification data not ready, please run previous cells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Êó∂Èó¥Â∫èÂàósentiment analysis\n",
    "print(\"=== Êó∂Èó¥Â∫èÂàósentiment analysis ===\")\n",
    "\n",
    "# verifyingÊÉÖÊÑüclassificationdataÊòØÂê¶ÂèØÁî®\n",
    "try:\n",
    "    df_categorized\n",
    "    \n",
    "    # ‰ªétimestamp‰∏≠ÊèêÂèñÂπ¥‰ªΩ\n",
    "    df_with_year = df_categorized.withColumn(\"year\", F.year(F.col(\"timestamp\")))\n",
    "    \n",
    "    # ÊåâÂπ¥‰ªΩAnalyzingÊÉÖÊÑüË∂ãÂäø\n",
    "    yearly_sentiment = df_with_year.groupBy(\"year\").agg(\n",
    "        F.count(\"*\").alias(\"total_comments\"),\n",
    "        F.avg(\"vader_sentiment\").alias(\"avg_sentiment\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Negative\", 1).otherwise(0)).alias(\"negative_count\"),\n",
    "        F.sum(F.when(F.col(\"sentiment_category\") == \"Neutral\", 1).otherwise(0)).alias(\"neutral_count\")\n",
    "    ).orderBy(\"year\")\n",
    "    \n",
    "    # converting‰∏∫PandasËøõË°åÂèØËßÜÂåñ\n",
    "    yearly_sentiment_pd = yearly_sentiment.toPandas()\n",
    "    print(\"Âπ¥Â∫¶ÊÉÖÊÑüË∂ãÂäøAnalyzing:\")\n",
    "    print(yearly_sentiment_pd)\n",
    "    \n",
    "    # ÁªòÂà∂Êó∂Èó¥Â∫èÂàóÂõæ\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Âπ¥Â∫¶ËØÑËÆ∫Êï∞ÈáèË∂ãÂäø\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['total_comments'], marker='o', linewidth=2)\n",
    "    plt.title('Âπ¥Â∫¶ËØÑËÆ∫Êï∞ÈáèË∂ãÂäø')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('ËØÑËÆ∫Êï∞Èáè')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Âπ¥Â∫¶Âπ≥ÂùáÊÉÖÊÑüË∂ãÂäø\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['avg_sentiment'], \n",
    "             marker='o', linewidth=2, color='orange')\n",
    "    plt.title('Âπ¥Â∫¶Âπ≥ÂùáÊÉÖÊÑüË∂ãÂäø')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('Âπ≥ÂùáÊÉÖÊÑüÂàÜÊï∞')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='NeutralÁ∫ø')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ÊÉÖÊÑüclassÂà´Âπ¥Â∫¶ÂàÜÂ∏É\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['positive_count'], \n",
    "             marker='o', label='Ê≠£Èù¢', color='green', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['negative_count'], \n",
    "             marker='s', label='Ë¥üÈù¢', color='red', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['neutral_count'], \n",
    "             marker='^', label='Neutral', color='gray', linewidth=2)\n",
    "    plt.title('Âπ¥Â∫¶ÊÉÖÊÑüclassÂà´ÂàÜÂ∏É')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('ËØÑËÆ∫Êï∞Èáè')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ÊÉÖÊÑüÊØî‰æãÂπ¥Â∫¶ÂèòÂåñ\n",
    "    plt.subplot(2, 2, 4)\n",
    "    yearly_sentiment_pd['positive_ratio'] = yearly_sentiment_pd['positive_count'] / yearly_sentiment_pd['total_comments']\n",
    "    yearly_sentiment_pd['negative_ratio'] = yearly_sentiment_pd['negative_count'] / yearly_sentiment_pd['total_comments']\n",
    "    yearly_sentiment_pd['neutral_ratio'] = yearly_sentiment_pd['neutral_count'] / yearly_sentiment_pd['total_comments']\n",
    "    \n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['positive_ratio'], \n",
    "             marker='o', label='Ê≠£Èù¢ÊØî‰æã', color='green', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['negative_ratio'], \n",
    "             marker='s', label='Ë¥üÈù¢ÊØî‰æã', color='red', linewidth=2)\n",
    "    plt.plot(yearly_sentiment_pd['year'], yearly_sentiment_pd['neutral_ratio'], \n",
    "             marker='^', label='NeutralÊØî‰æã', color='gray', linewidth=2)\n",
    "    plt.title('Âπ¥Â∫¶ÊÉÖÊÑüÊØî‰æãÂèòÂåñ')\n",
    "    plt.xlabel('Âπ¥‰ªΩ')\n",
    "    plt.ylabel('ÊØî‰æã')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Êó∂Èó¥Â∫èÂàósentiment analysiscompletingÔºÅ\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå ÊÉÖÊÑüclassificationdataÊú™PreparingÂ•ΩÔºåËØ∑ÂÖàrunningÂâçÈù¢ÁöÑcells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis summary and save results\n",
    "print(\"=== Exploratory Data Analysis Summary ===\")\n",
    "\n",
    "# Verify if all necessary data is available\n",
    "try:\n",
    "    df_categorized\n",
    "    subreddit_sentiment_pd\n",
    "    yearly_sentiment_pd\n",
    "\n",
    "    # Generate analysis report\n",
    "    total_comments = df_categorized.count()\n",
    "    sentiment_dist_final = df_categorized.groupBy(\"sentiment_category\").count().toPandas()\n",
    "\n",
    "    print(\"üìä **Data Overview**:\")\n",
    "    print(f\"   Total comments: {total_comments:,}\")\n",
    "    print(f\"   Analysis time span: 2010-2022\")\n",
    "    print(f\"   Main subreddits: politics, worldnews, askreddit\")\n",
    "\n",
    "    print(\"\\nüé≠ **Sentiment Distribution**:\")\n",
    "    for _, row in sentiment_dist_final.iterrows():\n",
    "        percentage = (row['count'] / total_comments) * 100\n",
    "        print(f\"   {row['sentiment_category']}: {row['count']:,} ({percentage:.1f}%)\")\n",
    "\n",
    "    print(\"\\nüìà **Key Findings**:\")\n",
    "    yearly_summary = yearly_sentiment_pd\n",
    "    if len(yearly_summary) > 0:\n",
    "        print(f\"   Most negative year: {yearly_summary.loc[yearly_summary['avg_sentiment'].idxmin(), 'year']}\")\n",
    "        print(f\"   Most positive year: {yearly_summary.loc[yearly_summary['avg_sentiment'].idxmax(), 'year']}\")\n",
    "        print(f\"   Year with most comments: {yearly_summary.loc[yearly_summary['total_comments'].idxmax(), 'year']}\")\n",
    "\n",
    "    print(\"\\nüè∑Ô∏è **Subreddit Characteristics**:\")\n",
    "    if len(subreddit_sentiment_pd) > 0:\n",
    "        most_positive = subreddit_sentiment_pd.loc[subreddit_sentiment_pd['avg_sentiment'].idxmax()]\n",
    "        most_negative = subreddit_sentiment_pd.loc[subreddit_sentiment_pd['avg_sentiment'].idxmin()]\n",
    "        print(f\"   Most positive subreddit: {most_positive['subreddit.name']} (score: {most_positive['avg_sentiment']:.3f})\")\n",
    "        print(f\"   Most negative subreddit: {most_negative['subreddit.name']} (score: {most_negative['avg_sentiment']:.3f})\")\n",
    "\n",
    "    print(\"\\nüíæ **Saving Enhanced Data**:\")\n",
    "    output_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
    "    print(f\"Saving to: {output_path}\")\n",
    "\n",
    "    try:\n",
    "        df_categorized.select(\n",
    "            \"id\", \"`subreddit.name`\", \"created_utc\", \"timestamp\",\n",
    "            \"body\", \"cleaned_body\", \"tokens_cleaned\", \"sentiment\",\n",
    "            \"vader_sentiment\", \"sentiment_category\", \"score\"\n",
    "        ).write.mode(\"overwrite\").parquet(output_path)\n",
    "        print(\"‚úÖ Sentiment analysis results saved successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Save failed: {e}\")\n",
    "\n",
    "    print(\"\\nüéØ **Next Steps Recommendations**:\")\n",
    "    print(\"   1. Perform topic modeling (LDA) based on sentiment analysis results\")\n",
    "    print(\"   2. Train machine learning classification models\")\n",
    "    print(\"   3. Analyze impact of specific events on sentiment\")\n",
    "    print(\"   4. Build sentiment prediction models\")\n",
    "\n",
    "    print(\"\\n‚úÖ Exploratory data analysis and sentiment analysis completed!\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"‚ùå Analysis data not ready, please run all previous cells first\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
