{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 第二步：数据清洗 (Data Cleaning)\n",
        "\n",
        "本 notebook 的目标：\n",
        "1. 加载10%采样数据（提高处理速度）\n",
        "2. 实施全面的数据清洗流程\n",
        "3. 处理缺失值、重复值和异常值\n",
        "4. 文本预处理和标准化\n",
        "5. 保存清洗后的数据\n",
        "\n",
        "## 清洗策略\n",
        "- 去除重复记录\n",
        "- 处理缺失值\n",
        "- 文本清洗：去除URL、HTML标签、特殊字符\n",
        "- 标准化时间格式\n",
        "- 过滤异常数据\n",
        "\n",
        "## 数据源\n",
        "使用10%采样数据：`/home/jovyan/work/data/processed/the-reddit-climate-change-dataset-comments-ten-percent.parquet`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "库导入完成！\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, TimestampType, ArrayType, BooleanType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"库导入完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.0\n",
            "Available cores: 20\n",
            "10%采样数据加载完成，共 460,070 条记录\n"
          ]
        }
      ],
      "source": [
        "# 初始化 Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_DataCleaning\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {sc.defaultParallelism}\")\n",
        "\n",
        "# 加载10%采样数据（速度更快）\n",
        "sample_data_path = \"/home/jovyan/work/data/processed/the-reddit-climate-change-dataset-comments-ten-percent.parquet\"\n",
        "df_raw = spark.read.parquet(sample_data_path)\n",
        "df_raw.cache()\n",
        "\n",
        "print(f\"10%采样数据加载完成，共 {df_raw.count():,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "文本清洗UDF创建完成！\n"
          ]
        }
      ],
      "source": [
        "# 定义文本清洗函数\n",
        "def create_text_cleaning_udf():\n",
        "    \"\"\"创建文本清洗的UDF函数\"\"\"\n",
        "    def clean_text_func(text):\n",
        "        if text is None:\n",
        "            return None\n",
        "        \n",
        "        text = str(text)\n",
        "        \n",
        "        # 1. 转换为小写\n",
        "        text = text.lower()\n",
        "        \n",
        "        # 2. 去除URL\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "        \n",
        "        # 3. 去除Reddit特有的格式\n",
        "        text = re.sub(r'/u/\\w+', '', text)  # 去除用户名\n",
        "        text = re.sub(r'/r/\\w+', '', text)  # 去除子版块名\n",
        "        text = re.sub(r'&gt;', '', text)   # 去除引用符号\n",
        "        text = re.sub(r'&lt;', '', text)\n",
        "        text = re.sub(r'&amp;', 'and', text)\n",
        "        \n",
        "        # 4. 去除HTML标签\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        \n",
        "        # 5. 去除特殊字符，保留字母、数字、空格和基本标点\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:]', ' ', text)\n",
        "        \n",
        "        # 6. 去除多余的空格\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        \n",
        "        # 7. 过滤过短的文本\n",
        "        if len(text) < 10:\n",
        "            return None\n",
        "            \n",
        "        return text\n",
        "    \n",
        "    return F.udf(clean_text_func, StringType())\n",
        "\n",
        "# 创建UDF\n",
        "clean_text_udf = create_text_cleaning_udf()\n",
        "print(\"文本清洗UDF创建完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 开始数据清洗流程 ===\n",
            "1. 处理时间戳...\n",
            "2. 去除重复记录...\n",
            "   去重前: 460,070 条\n",
            "   去重后: 460,070 条\n",
            "   删除了 0 条重复记录\n",
            "3. 处理缺失值...\n",
            "   删除空评论后: 460,070 条\n",
            "4. 应用文本清洗...\n",
            "   文本清洗后: 459,175 条\n",
            "\n",
            "总计删除了 895 条记录\n",
            "清洗完成率: 99.81%\n"
          ]
        }
      ],
      "source": [
        "# 执行数据清洗流程\n",
        "print(\"=== 开始数据清洗流程 ===\")\n",
        "\n",
        "# 1. 添加时间戳列\n",
        "print(\"1. 处理时间戳...\")\n",
        "df_cleaned = df_raw.withColumn(\"timestamp\", F.from_unixtime(F.col(\"created_utc\")))\n",
        "\n",
        "# 2. 去除重复记录（基于id列）\n",
        "print(\"2. 去除重复记录...\")\n",
        "initial_count = df_cleaned.count()\n",
        "df_cleaned = df_cleaned.dropDuplicates(['id'])\n",
        "after_dedup_count = df_cleaned.count()\n",
        "print(f\"   去重前: {initial_count:,} 条\")\n",
        "print(f\"   去重后: {after_dedup_count:,} 条\")\n",
        "print(f\"   删除了 {initial_count - after_dedup_count:,} 条重复记录\")\n",
        "\n",
        "# 3. 处理缺失值\n",
        "print(\"3. 处理缺失值...\")\n",
        "# 删除body为空的记录（这是我们分析的核心字段）\n",
        "df_cleaned = df_cleaned.filter(F.col(\"body\").isNotNull())\n",
        "df_cleaned = df_cleaned.filter(F.col(\"body\") != \"\")\n",
        "after_null_count = df_cleaned.count()\n",
        "print(f\"   删除空评论后: {after_null_count:,} 条\")\n",
        "\n",
        "# 4. 应用文本清洗\n",
        "print(\"4. 应用文本清洗...\")\n",
        "df_cleaned = df_cleaned.withColumn(\"cleaned_body\", clean_text_udf(F.col(\"body\")))\n",
        "\n",
        "# 5. 过滤清洗后为空的记录\n",
        "df_cleaned = df_cleaned.filter(F.col(\"cleaned_body\").isNotNull())\n",
        "after_text_clean_count = df_cleaned.count()\n",
        "print(f\"   文本清洗后: {after_text_clean_count:,} 条\")\n",
        "\n",
        "print(f\"\\n总计删除了 {initial_count - after_text_clean_count:,} 条记录\")\n",
        "print(f\"清洗完成率: {after_text_clean_count/initial_count*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 文本分词和停用词处理 ===\n",
            "1. 执行分词...\n",
            "2. 去除停用词...\n",
            "分词处理后剩余: 459,171 条记录\n",
            "数据已缓存到内存中\n"
          ]
        }
      ],
      "source": [
        "# 文本分词和停用词处理\n",
        "print(\"=== 文本分词和停用词处理 ===\")\n",
        "\n",
        "# 1. 分词\n",
        "print(\"1. 执行分词...\")\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_body\", outputCol=\"tokens_raw\")\n",
        "df_tokenized = tokenizer.transform(df_cleaned)\n",
        "\n",
        "# 2. 去除停用词\n",
        "print(\"2. 去除停用词...\")\n",
        "remover = StopWordsRemover(inputCol=\"tokens_raw\", outputCol=\"tokens_cleaned\")\n",
        "df_tokenized = remover.transform(df_tokenized)\n",
        "\n",
        "# 3. 过滤掉分词后为空的记录\n",
        "df_tokenized = df_tokenized.filter(F.size(F.col(\"tokens_cleaned\")) > 0)\n",
        "\n",
        "print(f\"分词处理后剩余: {df_tokenized.count():,} 条记录\")\n",
        "\n",
        "# 缓存结果\n",
        "df_tokenized.cache()\n",
        "print(\"数据已缓存到内存中\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 数据质量检查 ===\n",
            "1. 清洗前后对比样本:\n",
            "\n",
            "样本 1:\n",
            "原文: Maybe I was unclear, what I mean is, the Republicans lies include terrorists trying to kill us, gays molesting children and breaking up marriages, government g-men taking our guns and creating a polic...\n",
            "清洗后: maybe i was unclear, what i mean is, the republicans lies include terrorists trying to kill us, gays molesting children and breaking up marriages, government g men taking our guns and creating a polic...\n",
            "分词结果: ['maybe', 'unclear,', 'mean', 'is,', 'republicans', 'lies', 'include', 'terrorists', 'trying', 'kill']...\n",
            "\n",
            "样本 2:\n",
            "原文: Aaarrggh!!!  I be the copyright pyrate.  Free the information ya salty pups.\n",
            "\n",
            "\n",
            "\n",
            "&lt;&lt;&lt;America is losing the free world\" by Gideon Rachman\n",
            "\n",
            "Published: January 4 2010 20:11 | Last updated: January...\n",
            "清洗后: aaarrggh!!! i be the copyright pyrate. free the information ya salty pups. america is losing the free world by gideon rachman published: january 4 2010 20:11 last updated: january 4 2010 20:11 ever si...\n",
            "分词结果: ['aaarrggh!!!', 'copyright', 'pyrate.', 'free', 'information', 'ya', 'salty', 'pups.', 'america', 'losing']...\n",
            "\n",
            "样本 3:\n",
            "原文: How did I know \"climate change\" would be featured so prominently on a socialist website? Just a hunch, I guess!...\n",
            "清洗后: how did i know climate change would be featured so prominently on a socialist website? just a hunch, i guess!...\n",
            "分词结果: ['know', 'climate', 'change', 'featured', 'prominently', 'socialist', 'website?', 'hunch,', 'guess!']...\n",
            "\n",
            "2. 清洗后数据统计:\n",
            "总记录数: 459,171\n",
            "\n",
            "清洗后文本长度统计:\n",
            "+-------+-----------------+\n",
            "|summary|   cleaned_length|\n",
            "+-------+-----------------+\n",
            "|  count|           459171|\n",
            "|   mean| 682.247600567109|\n",
            "| stddev|973.4686817574221|\n",
            "|    min|               10|\n",
            "|    max|            39091|\n",
            "+-------+-----------------+\n",
            "\n",
            "分词后词汇数量统计:\n",
            "+-------+-----------------+\n",
            "|summary|      token_count|\n",
            "+-------+-----------------+\n",
            "|  count|           459171|\n",
            "|   mean|65.26154308525582|\n",
            "| stddev|93.37891752988807|\n",
            "|    min|                1|\n",
            "|    max|             3657|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 数据质量检查\n",
        "print(\"=== 数据质量检查 ===\")\n",
        "\n",
        "# 1. 查看清洗前后的对比样本\n",
        "print(\"1. 清洗前后对比样本:\")\n",
        "comparison_sample = df_tokenized.select(\"body\", \"cleaned_body\", \"tokens_cleaned\").limit(3).collect()\n",
        "\n",
        "for i, row in enumerate(comparison_sample):\n",
        "    print(f\"\\n样本 {i+1}:\")\n",
        "    print(f\"原文: {row['body'][:200]}...\")\n",
        "    print(f\"清洗后: {row['cleaned_body'][:200]}...\")\n",
        "    print(f\"分词结果: {row['tokens_cleaned'][:10]}...\")\n",
        "\n",
        "# 2. 统计信息\n",
        "print(\"\\n2. 清洗后数据统计:\")\n",
        "print(f\"总记录数: {df_tokenized.count():,}\")\n",
        "\n",
        "# 文本长度分布\n",
        "length_stats = df_tokenized.withColumn(\"cleaned_length\", F.length(\"cleaned_body\")) \\\n",
        "                          .select(\"cleaned_length\") \\\n",
        "                          .describe()\n",
        "print(\"\\n清洗后文本长度统计:\")\n",
        "length_stats.show()\n",
        "\n",
        "# 词汇数量分布\n",
        "token_stats = df_tokenized.withColumn(\"token_count\", F.size(\"tokens_cleaned\")) \\\n",
        "                         .select(\"token_count\") \\\n",
        "                         .describe()\n",
        "print(\"分词后词汇数量统计:\")\n",
        "token_stats.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 保存清洗后的数据 ===\n",
            "正在保存到: /home/jovyan/work/data/processed/cleaned_comments.parquet\n",
            "数据保存完成！\n",
            "最终数据集包含 459,171 条清洗后的记录\n",
            "\n",
            "最终数据结构:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- subreddit.name: string (nullable = true)\n",
            " |-- created_utc: long (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- cleaned_body: string (nullable = true)\n",
            " |-- tokens_cleaned: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- sentiment: double (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 保存清洗后的数据\n",
        "print(\"=== 保存清洗后的数据 ===\")\n",
        "\n",
        "# 选择需要保存的列\n",
        "columns_to_save = [\n",
        "    \"id\", \n",
        "    \"`subreddit.name`\",  # 使用反引号处理包含点号的列名\n",
        "    \"created_utc\", \n",
        "    \"timestamp\",\n",
        "    \"body\", \n",
        "    \"cleaned_body\", \n",
        "    \"tokens_cleaned\",\n",
        "    \"sentiment\", \n",
        "    \"score\"\n",
        "]\n",
        "\n",
        "df_final = df_tokenized.select(*columns_to_save)\n",
        "\n",
        "# 保存为Parquet格式\n",
        "output_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "print(f\"正在保存到: {output_path}\")\n",
        "\n",
        "df_final.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(\"数据保存完成！\")\n",
        "print(f\"最终数据集包含 {df_final.count():,} 条清洗后的记录\")\n",
        "\n",
        "# 显示最终数据结构\n",
        "print(\"\\n最终数据结构:\")\n",
        "df_final.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 最终清洗结果验证 ===\n",
            "1. 验证保存的数据文件...\n",
            "保存的数据记录数: 459,171\n",
            "\n",
            "2. 数据完整性检查:\n",
            "各列的非空值统计:\n",
            "  id: 459,171 非空值 (0 空值)\n",
            "  subreddit.name: 459,171 非空值 (0 空值)\n",
            "  created_utc: 459,171 非空值 (0 空值)\n",
            "  timestamp: 459,171 非空值 (0 空值)\n",
            "  body: 459,171 非空值 (0 空值)\n",
            "  cleaned_body: 459,171 非空值 (0 空值)\n",
            "  tokens_cleaned: 459,171 非空值 (0 空值)\n",
            "  sentiment: 454,264 非空值 (4,907 空值)\n",
            "  score: 459,171 非空值 (0 空值)\n",
            "\n",
            "3. 随机抽样查看清洗结果:\n",
            "\n",
            "样本 1 (ID: ebia1dn):\n",
            "  子版块: politics\n",
            "  原文: Climate change is a chinese hoax!!\n",
            "\n",
            "...\n",
            "  清洗后: climate change is a chinese hoax!!...\n",
            "  分词数量: 4 个词\n",
            "  情感得分: -0.3987\n",
            "\n",
            "样本 2 (ID: ec7hk8x):\n",
            "  子版块: politicalhumor\n",
            "  原文: Frankly, America's love affair with moderates in general is what's scaring the shit out of me with c...\n",
            "  清洗后: frankly, america s love affair with moderates in general is what s scaring the shit out of me with c...\n",
            "  分词数量: 56 个词\n",
            "  情感得分: 0.8381\n",
            "\n",
            "样本 3 (ID: een386u):\n",
            "  子版块: viennacircle\n",
            "  原文: Climate change rates drop to 0%...\n",
            "  清洗后: climate change rates drop to 0...\n",
            "  分词数量: 5 个词\n",
            "  情感得分: -0.2732\n",
            "\n",
            "4. 关键统计信息:\n",
            "  时间范围: 1262393359 到 1661990368\n",
            "  平均情感得分: -0.0064\n",
            "  平均分词数量: 65.3\n",
            "  平均文本长度: 682.2 字符\n",
            "\n",
            "5. 子版块数据分布:\n",
            "Top 10 子版块:\n",
            "+--------------+-----+\n",
            "|subreddit.name|count|\n",
            "+--------------+-----+\n",
            "|politics      |36989|\n",
            "|worldnews     |35283|\n",
            "|askreddit     |25863|\n",
            "|news          |9524 |\n",
            "|collapse      |9490 |\n",
            "|futurology    |8904 |\n",
            "|science       |7063 |\n",
            "|environment   |6819 |\n",
            "|canada        |6722 |\n",
            "|australia     |5993 |\n",
            "+--------------+-----+\n",
            "\n",
            "✅ 数据清洗流程完成！数据已准备好进行后续分析。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 49566)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 最终结果验证和检查\n",
        "print(\"=== 最终清洗结果验证 ===\")\n",
        "\n",
        "# 1. 重新加载保存的数据进行验证\n",
        "print(\"1. 验证保存的数据文件...\")\n",
        "df_saved = spark.read.parquet(\"/home/jovyan/work/data/processed/cleaned_comments.parquet\")\n",
        "saved_count = df_saved.count()\n",
        "print(f\"保存的数据记录数: {saved_count:,}\")\n",
        "\n",
        "# 2. 检查数据完整性\n",
        "print(\"\\n2. 数据完整性检查:\")\n",
        "print(\"各列的非空值统计:\")\n",
        "for col_name in df_saved.columns:\n",
        "    # 处理包含点号的列名\n",
        "    if \".\" in col_name:\n",
        "        null_count = df_saved.filter(F.col(f\"`{col_name}`\").isNull()).count()\n",
        "    else:\n",
        "        null_count = df_saved.filter(F.col(col_name).isNull()).count()\n",
        "    print(f\"  {col_name}: {saved_count - null_count:,} 非空值 ({null_count:,} 空值)\")\n",
        "\n",
        "# 3. 随机抽样查看清洗结果\n",
        "print(\"\\n3. 随机抽样查看清洗结果:\")\n",
        "sample_data = df_saved.sample(0.001, seed=42).select(\n",
        "    \"id\", \"`subreddit.name`\", \"body\", \"cleaned_body\", \"tokens_cleaned\", \"sentiment\"\n",
        ").limit(3).collect()\n",
        "\n",
        "for i, row in enumerate(sample_data):\n",
        "    print(f\"\\n样本 {i+1} (ID: {row['id']}):\")\n",
        "    print(f\"  子版块: {row['subreddit.name']}\")\n",
        "    print(f\"  原文: {row['body'][:100]}...\")\n",
        "    print(f\"  清洗后: {row['cleaned_body'][:100]}...\")\n",
        "    print(f\"  分词数量: {len(row['tokens_cleaned'])} 个词\")\n",
        "    print(f\"  情感得分: {row['sentiment']}\")\n",
        "\n",
        "# 4. 关键统计信息\n",
        "print(\"\\n4. 关键统计信息:\")\n",
        "stats = df_saved.select(\n",
        "    F.min(\"created_utc\").alias(\"earliest_timestamp\"),\n",
        "    F.max(\"created_utc\").alias(\"latest_timestamp\"),\n",
        "    F.avg(\"sentiment\").alias(\"avg_sentiment\"),\n",
        "    F.avg(F.size(\"tokens_cleaned\")).alias(\"avg_tokens\"),\n",
        "    F.avg(F.length(\"cleaned_body\")).alias(\"avg_text_length\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"  时间范围: {stats['earliest_timestamp']} 到 {stats['latest_timestamp']}\")\n",
        "print(f\"  平均情感得分: {stats['avg_sentiment']:.4f}\")\n",
        "print(f\"  平均分词数量: {stats['avg_tokens']:.1f}\")\n",
        "print(f\"  平均文本长度: {stats['avg_text_length']:.1f} 字符\")\n",
        "\n",
        "# 5. 子版块分布\n",
        "print(\"\\n5. 子版块数据分布:\")\n",
        "subreddit_dist = df_saved.groupBy(\"`subreddit.name`\").count().orderBy(F.desc(\"count\")).limit(10)\n",
        "print(\"Top 10 子版块:\")\n",
        "subreddit_dist.show(10, False)\n",
        "\n",
        "print(\"✅ 数据清洗流程完成！数据已准备好进行后续分析。\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 数据清洗总结\n",
        "\n",
        "### 清洗流程完成情况\n",
        "✅ **已完成的清洗步骤**：\n",
        "1. **重复数据处理** - 基于ID去除重复记录\n",
        "2. **缺失值处理** - 删除空评论内容\n",
        "3. **文本标准化** - 小写转换、URL清理、特殊字符处理\n",
        "4. **Reddit格式清理** - 去除用户名、子版块引用、HTML实体\n",
        "5. **分词处理** - 使用Spark ML的Tokenizer\n",
        "6. **停用词移除** - 使用Spark ML的StopWordsRemover\n",
        "7. **数据持久化** - 保存为高效的Parquet格式\n",
        "\n",
        "### 数据质量提升\n",
        "- **数据完整性**: 确保所有记录都有有效的文本内容\n",
        "- **文本标准化**: 统一格式，便于后续分析\n",
        "- **存储优化**: Parquet格式提供更好的压缩和查询性能\n",
        "\n",
        "### 下一步计划\n",
        "1. **探索性数据分析** - 基于清洗后的数据进行深度分析\n",
        "2. **情感分析** - 使用VADER或其他工具进行情感打分\n",
        "3. **主题建模** - 使用LDA发现潜在主题\n",
        "4. **分类建模** - 训练机器学习模型\n",
        "\n",
        "### 文件输出\n",
        "- **清洗后数据**: `/data/processed/cleaned_comments.parquet`\n",
        "- **包含字段**: id, subreddit.name, timestamp, body, cleaned_body, tokens_cleaned, sentiment, score\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
