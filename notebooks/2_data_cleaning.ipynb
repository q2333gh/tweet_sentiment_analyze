{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ç¬¬äºŒæ­¥ï¼šæ•°æ®æ¸…æ´— (Data Cleaning)\n",
        "\n",
        "æœ¬ notebook çš„ç›®æ ‡ï¼š\n",
        "1. åŠ è½½10%é‡‡æ ·æ•°æ®ï¼ˆæé«˜å¤„ç†é€Ÿåº¦ï¼‰\n",
        "2. å®æ–½å…¨é¢çš„æ•°æ®æ¸…æ´—æµç¨‹\n",
        "3. å¤„ç†ç¼ºå¤±å€¼ã€é‡å¤å€¼å’Œå¼‚å¸¸å€¼\n",
        "4. æ–‡æœ¬é¢„å¤„ç†å’Œæ ‡å‡†åŒ–\n",
        "5. ä¿å­˜æ¸…æ´—åçš„æ•°æ®\n",
        "\n",
        "## æ¸…æ´—ç­–ç•¥\n",
        "- å»é™¤é‡å¤è®°å½•\n",
        "- å¤„ç†ç¼ºå¤±å€¼\n",
        "- æ–‡æœ¬æ¸…æ´—ï¼šå»é™¤URLã€HTMLæ ‡ç­¾ã€ç‰¹æ®Šå­—ç¬¦\n",
        "- æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼\n",
        "- è¿‡æ»¤å¼‚å¸¸æ•°æ®\n",
        "\n",
        "## æ•°æ®æº\n",
        "ä½¿ç”¨10%é‡‡æ ·æ•°æ®ï¼š`/home/jovyan/work/data/processed/the-reddit-climate-change-dataset-comments-ten-percent.parquet`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "åº“å¯¼å…¥å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, TimestampType, ArrayType, BooleanType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"åº“å¯¼å…¥å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.0\n",
            "Available cores: 20\n",
            "10%é‡‡æ ·æ•°æ®åŠ è½½å®Œæˆï¼Œå…± 460,070 æ¡è®°å½•\n"
          ]
        }
      ],
      "source": [
        "# åˆå§‹åŒ– Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_DataCleaning\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {sc.defaultParallelism}\")\n",
        "\n",
        "# åŠ è½½10%é‡‡æ ·æ•°æ®ï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰\n",
        "sample_data_path = \"/home/jovyan/work/data/processed/the-reddit-climate-change-dataset-comments-ten-percent.parquet\"\n",
        "df_raw = spark.read.parquet(sample_data_path)\n",
        "df_raw.cache()\n",
        "\n",
        "print(f\"10%é‡‡æ ·æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {df_raw.count():,} æ¡è®°å½•\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ–‡æœ¬æ¸…æ´—UDFåˆ›å»ºå®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# å®šä¹‰æ–‡æœ¬æ¸…æ´—å‡½æ•°\n",
        "def create_text_cleaning_udf():\n",
        "    \"\"\"åˆ›å»ºæ–‡æœ¬æ¸…æ´—çš„UDFå‡½æ•°\"\"\"\n",
        "    def clean_text_func(text):\n",
        "        if text is None:\n",
        "            return None\n",
        "        \n",
        "        text = str(text)\n",
        "        \n",
        "        # 1. è½¬æ¢ä¸ºå°å†™\n",
        "        text = text.lower()\n",
        "        \n",
        "        # 2. å»é™¤URL\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "        \n",
        "        # 3. å»é™¤Redditç‰¹æœ‰çš„æ ¼å¼\n",
        "        text = re.sub(r'/u/\\w+', '', text)  # å»é™¤ç”¨æˆ·å\n",
        "        text = re.sub(r'/r/\\w+', '', text)  # å»é™¤å­ç‰ˆå—å\n",
        "        text = re.sub(r'&gt;', '', text)   # å»é™¤å¼•ç”¨ç¬¦å·\n",
        "        text = re.sub(r'&lt;', '', text)\n",
        "        text = re.sub(r'&amp;', 'and', text)\n",
        "        \n",
        "        # 4. å»é™¤HTMLæ ‡ç­¾\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        \n",
        "        # 5. å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼å’ŒåŸºæœ¬æ ‡ç‚¹\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:]', ' ', text)\n",
        "        \n",
        "        # 6. å»é™¤å¤šä½™çš„ç©ºæ ¼\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        \n",
        "        # 7. è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬\n",
        "        if len(text) < 10:\n",
        "            return None\n",
        "            \n",
        "        return text\n",
        "    \n",
        "    return F.udf(clean_text_func, StringType())\n",
        "\n",
        "# åˆ›å»ºUDF\n",
        "clean_text_udf = create_text_cleaning_udf()\n",
        "print(\"æ–‡æœ¬æ¸…æ´—UDFåˆ›å»ºå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== å¼€å§‹æ•°æ®æ¸…æ´—æµç¨‹ ===\n",
            "1. å¤„ç†æ—¶é—´æˆ³...\n",
            "2. å»é™¤é‡å¤è®°å½•...\n",
            "   å»é‡å‰: 460,070 æ¡\n",
            "   å»é‡å: 460,070 æ¡\n",
            "   åˆ é™¤äº† 0 æ¡é‡å¤è®°å½•\n",
            "3. å¤„ç†ç¼ºå¤±å€¼...\n",
            "   åˆ é™¤ç©ºè¯„è®ºå: 460,070 æ¡\n",
            "4. åº”ç”¨æ–‡æœ¬æ¸…æ´—...\n",
            "   æ–‡æœ¬æ¸…æ´—å: 459,175 æ¡\n",
            "\n",
            "æ€»è®¡åˆ é™¤äº† 895 æ¡è®°å½•\n",
            "æ¸…æ´—å®Œæˆç‡: 99.81%\n"
          ]
        }
      ],
      "source": [
        "# æ‰§è¡Œæ•°æ®æ¸…æ´—æµç¨‹\n",
        "print(\"=== å¼€å§‹æ•°æ®æ¸…æ´—æµç¨‹ ===\")\n",
        "\n",
        "# 1. æ·»åŠ æ—¶é—´æˆ³åˆ—\n",
        "print(\"1. å¤„ç†æ—¶é—´æˆ³...\")\n",
        "df_cleaned = df_raw.withColumn(\"timestamp\", F.from_unixtime(F.col(\"created_utc\")))\n",
        "\n",
        "# 2. å»é™¤é‡å¤è®°å½•ï¼ˆåŸºäºidåˆ—ï¼‰\n",
        "print(\"2. å»é™¤é‡å¤è®°å½•...\")\n",
        "initial_count = df_cleaned.count()\n",
        "df_cleaned = df_cleaned.dropDuplicates(['id'])\n",
        "after_dedup_count = df_cleaned.count()\n",
        "print(f\"   å»é‡å‰: {initial_count:,} æ¡\")\n",
        "print(f\"   å»é‡å: {after_dedup_count:,} æ¡\")\n",
        "print(f\"   åˆ é™¤äº† {initial_count - after_dedup_count:,} æ¡é‡å¤è®°å½•\")\n",
        "\n",
        "# 3. å¤„ç†ç¼ºå¤±å€¼\n",
        "print(\"3. å¤„ç†ç¼ºå¤±å€¼...\")\n",
        "# åˆ é™¤bodyä¸ºç©ºçš„è®°å½•ï¼ˆè¿™æ˜¯æˆ‘ä»¬åˆ†æçš„æ ¸å¿ƒå­—æ®µï¼‰\n",
        "df_cleaned = df_cleaned.filter(F.col(\"body\").isNotNull())\n",
        "df_cleaned = df_cleaned.filter(F.col(\"body\") != \"\")\n",
        "after_null_count = df_cleaned.count()\n",
        "print(f\"   åˆ é™¤ç©ºè¯„è®ºå: {after_null_count:,} æ¡\")\n",
        "\n",
        "# 4. åº”ç”¨æ–‡æœ¬æ¸…æ´—\n",
        "print(\"4. åº”ç”¨æ–‡æœ¬æ¸…æ´—...\")\n",
        "df_cleaned = df_cleaned.withColumn(\"cleaned_body\", clean_text_udf(F.col(\"body\")))\n",
        "\n",
        "# 5. è¿‡æ»¤æ¸…æ´—åä¸ºç©ºçš„è®°å½•\n",
        "df_cleaned = df_cleaned.filter(F.col(\"cleaned_body\").isNotNull())\n",
        "after_text_clean_count = df_cleaned.count()\n",
        "print(f\"   æ–‡æœ¬æ¸…æ´—å: {after_text_clean_count:,} æ¡\")\n",
        "\n",
        "print(f\"\\næ€»è®¡åˆ é™¤äº† {initial_count - after_text_clean_count:,} æ¡è®°å½•\")\n",
        "print(f\"æ¸…æ´—å®Œæˆç‡: {after_text_clean_count/initial_count*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== æ–‡æœ¬åˆ†è¯å’Œåœç”¨è¯å¤„ç† ===\n",
            "1. æ‰§è¡Œåˆ†è¯...\n",
            "2. å»é™¤åœç”¨è¯...\n",
            "åˆ†è¯å¤„ç†åå‰©ä½™: 459,171 æ¡è®°å½•\n",
            "æ•°æ®å·²ç¼“å­˜åˆ°å†…å­˜ä¸­\n"
          ]
        }
      ],
      "source": [
        "# æ–‡æœ¬åˆ†è¯å’Œåœç”¨è¯å¤„ç†\n",
        "print(\"=== æ–‡æœ¬åˆ†è¯å’Œåœç”¨è¯å¤„ç† ===\")\n",
        "\n",
        "# 1. åˆ†è¯\n",
        "print(\"1. æ‰§è¡Œåˆ†è¯...\")\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_body\", outputCol=\"tokens_raw\")\n",
        "df_tokenized = tokenizer.transform(df_cleaned)\n",
        "\n",
        "# 2. å»é™¤åœç”¨è¯\n",
        "print(\"2. å»é™¤åœç”¨è¯...\")\n",
        "remover = StopWordsRemover(inputCol=\"tokens_raw\", outputCol=\"tokens_cleaned\")\n",
        "df_tokenized = remover.transform(df_tokenized)\n",
        "\n",
        "# 3. è¿‡æ»¤æ‰åˆ†è¯åä¸ºç©ºçš„è®°å½•\n",
        "df_tokenized = df_tokenized.filter(F.size(F.col(\"tokens_cleaned\")) > 0)\n",
        "\n",
        "print(f\"åˆ†è¯å¤„ç†åå‰©ä½™: {df_tokenized.count():,} æ¡è®°å½•\")\n",
        "\n",
        "# ç¼“å­˜ç»“æœ\n",
        "df_tokenized.cache()\n",
        "print(\"æ•°æ®å·²ç¼“å­˜åˆ°å†…å­˜ä¸­\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== æ•°æ®è´¨é‡æ£€æŸ¥ ===\n",
            "1. æ¸…æ´—å‰åå¯¹æ¯”æ ·æœ¬:\n",
            "\n",
            "æ ·æœ¬ 1:\n",
            "åŸæ–‡: Maybe I was unclear, what I mean is, the Republicans lies include terrorists trying to kill us, gays molesting children and breaking up marriages, government g-men taking our guns and creating a polic...\n",
            "æ¸…æ´—å: maybe i was unclear, what i mean is, the republicans lies include terrorists trying to kill us, gays molesting children and breaking up marriages, government g men taking our guns and creating a polic...\n",
            "åˆ†è¯ç»“æœ: ['maybe', 'unclear,', 'mean', 'is,', 'republicans', 'lies', 'include', 'terrorists', 'trying', 'kill']...\n",
            "\n",
            "æ ·æœ¬ 2:\n",
            "åŸæ–‡: Aaarrggh!!!  I be the copyright pyrate.  Free the information ya salty pups.\n",
            "\n",
            "\n",
            "\n",
            "&lt;&lt;&lt;America is losing the free world\" by Gideon Rachman\n",
            "\n",
            "Published: January 4 2010 20:11 | Last updated: January...\n",
            "æ¸…æ´—å: aaarrggh!!! i be the copyright pyrate. free the information ya salty pups. america is losing the free world by gideon rachman published: january 4 2010 20:11 last updated: january 4 2010 20:11 ever si...\n",
            "åˆ†è¯ç»“æœ: ['aaarrggh!!!', 'copyright', 'pyrate.', 'free', 'information', 'ya', 'salty', 'pups.', 'america', 'losing']...\n",
            "\n",
            "æ ·æœ¬ 3:\n",
            "åŸæ–‡: How did I know \"climate change\" would be featured so prominently on a socialist website? Just a hunch, I guess!...\n",
            "æ¸…æ´—å: how did i know climate change would be featured so prominently on a socialist website? just a hunch, i guess!...\n",
            "åˆ†è¯ç»“æœ: ['know', 'climate', 'change', 'featured', 'prominently', 'socialist', 'website?', 'hunch,', 'guess!']...\n",
            "\n",
            "2. æ¸…æ´—åæ•°æ®ç»Ÿè®¡:\n",
            "æ€»è®°å½•æ•°: 459,171\n",
            "\n",
            "æ¸…æ´—åæ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\n",
            "+-------+-----------------+\n",
            "|summary|   cleaned_length|\n",
            "+-------+-----------------+\n",
            "|  count|           459171|\n",
            "|   mean| 682.247600567109|\n",
            "| stddev|973.4686817574221|\n",
            "|    min|               10|\n",
            "|    max|            39091|\n",
            "+-------+-----------------+\n",
            "\n",
            "åˆ†è¯åè¯æ±‡æ•°é‡ç»Ÿè®¡:\n",
            "+-------+-----------------+\n",
            "|summary|      token_count|\n",
            "+-------+-----------------+\n",
            "|  count|           459171|\n",
            "|   mean|65.26154308525582|\n",
            "| stddev|93.37891752988807|\n",
            "|    min|                1|\n",
            "|    max|             3657|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# æ•°æ®è´¨é‡æ£€æŸ¥\n",
        "print(\"=== æ•°æ®è´¨é‡æ£€æŸ¥ ===\")\n",
        "\n",
        "# 1. æŸ¥çœ‹æ¸…æ´—å‰åçš„å¯¹æ¯”æ ·æœ¬\n",
        "print(\"1. æ¸…æ´—å‰åå¯¹æ¯”æ ·æœ¬:\")\n",
        "comparison_sample = df_tokenized.select(\"body\", \"cleaned_body\", \"tokens_cleaned\").limit(3).collect()\n",
        "\n",
        "for i, row in enumerate(comparison_sample):\n",
        "    print(f\"\\næ ·æœ¬ {i+1}:\")\n",
        "    print(f\"åŸæ–‡: {row['body'][:200]}...\")\n",
        "    print(f\"æ¸…æ´—å: {row['cleaned_body'][:200]}...\")\n",
        "    print(f\"åˆ†è¯ç»“æœ: {row['tokens_cleaned'][:10]}...\")\n",
        "\n",
        "# 2. ç»Ÿè®¡ä¿¡æ¯\n",
        "print(\"\\n2. æ¸…æ´—åæ•°æ®ç»Ÿè®¡:\")\n",
        "print(f\"æ€»è®°å½•æ•°: {df_tokenized.count():,}\")\n",
        "\n",
        "# æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ\n",
        "length_stats = df_tokenized.withColumn(\"cleaned_length\", F.length(\"cleaned_body\")) \\\n",
        "                          .select(\"cleaned_length\") \\\n",
        "                          .describe()\n",
        "print(\"\\næ¸…æ´—åæ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\")\n",
        "length_stats.show()\n",
        "\n",
        "# è¯æ±‡æ•°é‡åˆ†å¸ƒ\n",
        "token_stats = df_tokenized.withColumn(\"token_count\", F.size(\"tokens_cleaned\")) \\\n",
        "                         .select(\"token_count\") \\\n",
        "                         .describe()\n",
        "print(\"åˆ†è¯åè¯æ±‡æ•°é‡ç»Ÿè®¡:\")\n",
        "token_stats.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ä¿å­˜æ¸…æ´—åçš„æ•°æ® ===\n",
            "æ­£åœ¨ä¿å­˜åˆ°: /home/jovyan/work/data/processed/cleaned_comments.parquet\n",
            "æ•°æ®ä¿å­˜å®Œæˆï¼\n",
            "æœ€ç»ˆæ•°æ®é›†åŒ…å« 459,171 æ¡æ¸…æ´—åçš„è®°å½•\n",
            "\n",
            "æœ€ç»ˆæ•°æ®ç»“æ„:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- subreddit.name: string (nullable = true)\n",
            " |-- created_utc: long (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- cleaned_body: string (nullable = true)\n",
            " |-- tokens_cleaned: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- sentiment: double (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ä¿å­˜æ¸…æ´—åçš„æ•°æ®\n",
        "print(\"=== ä¿å­˜æ¸…æ´—åçš„æ•°æ® ===\")\n",
        "\n",
        "# é€‰æ‹©éœ€è¦ä¿å­˜çš„åˆ—\n",
        "columns_to_save = [\n",
        "    \"id\", \n",
        "    \"`subreddit.name`\",  # ä½¿ç”¨åå¼•å·å¤„ç†åŒ…å«ç‚¹å·çš„åˆ—å\n",
        "    \"created_utc\", \n",
        "    \"timestamp\",\n",
        "    \"body\", \n",
        "    \"cleaned_body\", \n",
        "    \"tokens_cleaned\",\n",
        "    \"sentiment\", \n",
        "    \"score\"\n",
        "]\n",
        "\n",
        "df_final = df_tokenized.select(*columns_to_save)\n",
        "\n",
        "# ä¿å­˜ä¸ºParquetæ ¼å¼\n",
        "output_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "print(f\"æ­£åœ¨ä¿å­˜åˆ°: {output_path}\")\n",
        "\n",
        "df_final.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(\"æ•°æ®ä¿å­˜å®Œæˆï¼\")\n",
        "print(f\"æœ€ç»ˆæ•°æ®é›†åŒ…å« {df_final.count():,} æ¡æ¸…æ´—åçš„è®°å½•\")\n",
        "\n",
        "# æ˜¾ç¤ºæœ€ç»ˆæ•°æ®ç»“æ„\n",
        "print(\"\\næœ€ç»ˆæ•°æ®ç»“æ„:\")\n",
        "df_final.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== æœ€ç»ˆæ¸…æ´—ç»“æœéªŒè¯ ===\n",
            "1. éªŒè¯ä¿å­˜çš„æ•°æ®æ–‡ä»¶...\n",
            "ä¿å­˜çš„æ•°æ®è®°å½•æ•°: 459,171\n",
            "\n",
            "2. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:\n",
            "å„åˆ—çš„éç©ºå€¼ç»Ÿè®¡:\n",
            "  id: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "  subreddit.name: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "  created_utc: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "  timestamp: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "  body: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "  cleaned_body: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "  tokens_cleaned: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "  sentiment: 454,264 éç©ºå€¼ (4,907 ç©ºå€¼)\n",
            "  score: 459,171 éç©ºå€¼ (0 ç©ºå€¼)\n",
            "\n",
            "3. éšæœºæŠ½æ ·æŸ¥çœ‹æ¸…æ´—ç»“æœ:\n",
            "\n",
            "æ ·æœ¬ 1 (ID: ebia1dn):\n",
            "  å­ç‰ˆå—: politics\n",
            "  åŸæ–‡: Climate change is a chinese hoax!!\n",
            "\n",
            "...\n",
            "  æ¸…æ´—å: climate change is a chinese hoax!!...\n",
            "  åˆ†è¯æ•°é‡: 4 ä¸ªè¯\n",
            "  æƒ…æ„Ÿå¾—åˆ†: -0.3987\n",
            "\n",
            "æ ·æœ¬ 2 (ID: ec7hk8x):\n",
            "  å­ç‰ˆå—: politicalhumor\n",
            "  åŸæ–‡: Frankly, America's love affair with moderates in general is what's scaring the shit out of me with c...\n",
            "  æ¸…æ´—å: frankly, america s love affair with moderates in general is what s scaring the shit out of me with c...\n",
            "  åˆ†è¯æ•°é‡: 56 ä¸ªè¯\n",
            "  æƒ…æ„Ÿå¾—åˆ†: 0.8381\n",
            "\n",
            "æ ·æœ¬ 3 (ID: een386u):\n",
            "  å­ç‰ˆå—: viennacircle\n",
            "  åŸæ–‡: Climate change rates drop to 0%...\n",
            "  æ¸…æ´—å: climate change rates drop to 0...\n",
            "  åˆ†è¯æ•°é‡: 5 ä¸ªè¯\n",
            "  æƒ…æ„Ÿå¾—åˆ†: -0.2732\n",
            "\n",
            "4. å…³é”®ç»Ÿè®¡ä¿¡æ¯:\n",
            "  æ—¶é—´èŒƒå›´: 1262393359 åˆ° 1661990368\n",
            "  å¹³å‡æƒ…æ„Ÿå¾—åˆ†: -0.0064\n",
            "  å¹³å‡åˆ†è¯æ•°é‡: 65.3\n",
            "  å¹³å‡æ–‡æœ¬é•¿åº¦: 682.2 å­—ç¬¦\n",
            "\n",
            "5. å­ç‰ˆå—æ•°æ®åˆ†å¸ƒ:\n",
            "Top 10 å­ç‰ˆå—:\n",
            "+--------------+-----+\n",
            "|subreddit.name|count|\n",
            "+--------------+-----+\n",
            "|politics      |36989|\n",
            "|worldnews     |35283|\n",
            "|askreddit     |25863|\n",
            "|news          |9524 |\n",
            "|collapse      |9490 |\n",
            "|futurology    |8904 |\n",
            "|science       |7063 |\n",
            "|environment   |6819 |\n",
            "|canada        |6722 |\n",
            "|australia     |5993 |\n",
            "+--------------+-----+\n",
            "\n",
            "âœ… æ•°æ®æ¸…æ´—æµç¨‹å®Œæˆï¼æ•°æ®å·²å‡†å¤‡å¥½è¿›è¡Œåç»­åˆ†æã€‚\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 49566)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# æœ€ç»ˆç»“æœéªŒè¯å’Œæ£€æŸ¥\n",
        "print(\"=== æœ€ç»ˆæ¸…æ´—ç»“æœéªŒè¯ ===\")\n",
        "\n",
        "# 1. é‡æ–°åŠ è½½ä¿å­˜çš„æ•°æ®è¿›è¡ŒéªŒè¯\n",
        "print(\"1. éªŒè¯ä¿å­˜çš„æ•°æ®æ–‡ä»¶...\")\n",
        "df_saved = spark.read.parquet(\"/home/jovyan/work/data/processed/cleaned_comments.parquet\")\n",
        "saved_count = df_saved.count()\n",
        "print(f\"ä¿å­˜çš„æ•°æ®è®°å½•æ•°: {saved_count:,}\")\n",
        "\n",
        "# 2. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
        "print(\"\\n2. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:\")\n",
        "print(\"å„åˆ—çš„éç©ºå€¼ç»Ÿè®¡:\")\n",
        "for col_name in df_saved.columns:\n",
        "    # å¤„ç†åŒ…å«ç‚¹å·çš„åˆ—å\n",
        "    if \".\" in col_name:\n",
        "        null_count = df_saved.filter(F.col(f\"`{col_name}`\").isNull()).count()\n",
        "    else:\n",
        "        null_count = df_saved.filter(F.col(col_name).isNull()).count()\n",
        "    print(f\"  {col_name}: {saved_count - null_count:,} éç©ºå€¼ ({null_count:,} ç©ºå€¼)\")\n",
        "\n",
        "# 3. éšæœºæŠ½æ ·æŸ¥çœ‹æ¸…æ´—ç»“æœ\n",
        "print(\"\\n3. éšæœºæŠ½æ ·æŸ¥çœ‹æ¸…æ´—ç»“æœ:\")\n",
        "sample_data = df_saved.sample(0.001, seed=42).select(\n",
        "    \"id\", \"`subreddit.name`\", \"body\", \"cleaned_body\", \"tokens_cleaned\", \"sentiment\"\n",
        ").limit(3).collect()\n",
        "\n",
        "for i, row in enumerate(sample_data):\n",
        "    print(f\"\\næ ·æœ¬ {i+1} (ID: {row['id']}):\")\n",
        "    print(f\"  å­ç‰ˆå—: {row['subreddit.name']}\")\n",
        "    print(f\"  åŸæ–‡: {row['body'][:100]}...\")\n",
        "    print(f\"  æ¸…æ´—å: {row['cleaned_body'][:100]}...\")\n",
        "    print(f\"  åˆ†è¯æ•°é‡: {len(row['tokens_cleaned'])} ä¸ªè¯\")\n",
        "    print(f\"  æƒ…æ„Ÿå¾—åˆ†: {row['sentiment']}\")\n",
        "\n",
        "# 4. å…³é”®ç»Ÿè®¡ä¿¡æ¯\n",
        "print(\"\\n4. å…³é”®ç»Ÿè®¡ä¿¡æ¯:\")\n",
        "stats = df_saved.select(\n",
        "    F.min(\"created_utc\").alias(\"earliest_timestamp\"),\n",
        "    F.max(\"created_utc\").alias(\"latest_timestamp\"),\n",
        "    F.avg(\"sentiment\").alias(\"avg_sentiment\"),\n",
        "    F.avg(F.size(\"tokens_cleaned\")).alias(\"avg_tokens\"),\n",
        "    F.avg(F.length(\"cleaned_body\")).alias(\"avg_text_length\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"  æ—¶é—´èŒƒå›´: {stats['earliest_timestamp']} åˆ° {stats['latest_timestamp']}\")\n",
        "print(f\"  å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {stats['avg_sentiment']:.4f}\")\n",
        "print(f\"  å¹³å‡åˆ†è¯æ•°é‡: {stats['avg_tokens']:.1f}\")\n",
        "print(f\"  å¹³å‡æ–‡æœ¬é•¿åº¦: {stats['avg_text_length']:.1f} å­—ç¬¦\")\n",
        "\n",
        "# 5. å­ç‰ˆå—åˆ†å¸ƒ\n",
        "print(\"\\n5. å­ç‰ˆå—æ•°æ®åˆ†å¸ƒ:\")\n",
        "subreddit_dist = df_saved.groupBy(\"`subreddit.name`\").count().orderBy(F.desc(\"count\")).limit(10)\n",
        "print(\"Top 10 å­ç‰ˆå—:\")\n",
        "subreddit_dist.show(10, False)\n",
        "\n",
        "print(\"âœ… æ•°æ®æ¸…æ´—æµç¨‹å®Œæˆï¼æ•°æ®å·²å‡†å¤‡å¥½è¿›è¡Œåç»­åˆ†æã€‚\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“Š æ•°æ®æ¸…æ´—æ€»ç»“\n",
        "\n",
        "### æ¸…æ´—æµç¨‹å®Œæˆæƒ…å†µ\n",
        "âœ… **å·²å®Œæˆçš„æ¸…æ´—æ­¥éª¤**ï¼š\n",
        "1. **é‡å¤æ•°æ®å¤„ç†** - åŸºäºIDå»é™¤é‡å¤è®°å½•\n",
        "2. **ç¼ºå¤±å€¼å¤„ç†** - åˆ é™¤ç©ºè¯„è®ºå†…å®¹\n",
        "3. **æ–‡æœ¬æ ‡å‡†åŒ–** - å°å†™è½¬æ¢ã€URLæ¸…ç†ã€ç‰¹æ®Šå­—ç¬¦å¤„ç†\n",
        "4. **Redditæ ¼å¼æ¸…ç†** - å»é™¤ç”¨æˆ·åã€å­ç‰ˆå—å¼•ç”¨ã€HTMLå®ä½“\n",
        "5. **åˆ†è¯å¤„ç†** - ä½¿ç”¨Spark MLçš„Tokenizer\n",
        "6. **åœç”¨è¯ç§»é™¤** - ä½¿ç”¨Spark MLçš„StopWordsRemover\n",
        "7. **æ•°æ®æŒä¹…åŒ–** - ä¿å­˜ä¸ºé«˜æ•ˆçš„Parquetæ ¼å¼\n",
        "\n",
        "### æ•°æ®è´¨é‡æå‡\n",
        "- **æ•°æ®å®Œæ•´æ€§**: ç¡®ä¿æ‰€æœ‰è®°å½•éƒ½æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹\n",
        "- **æ–‡æœ¬æ ‡å‡†åŒ–**: ç»Ÿä¸€æ ¼å¼ï¼Œä¾¿äºåç»­åˆ†æ\n",
        "- **å­˜å‚¨ä¼˜åŒ–**: Parquetæ ¼å¼æä¾›æ›´å¥½çš„å‹ç¼©å’ŒæŸ¥è¯¢æ€§èƒ½\n",
        "\n",
        "### ä¸‹ä¸€æ­¥è®¡åˆ’\n",
        "1. **æ¢ç´¢æ€§æ•°æ®åˆ†æ** - åŸºäºæ¸…æ´—åçš„æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æ\n",
        "2. **æƒ…æ„Ÿåˆ†æ** - ä½¿ç”¨VADERæˆ–å…¶ä»–å·¥å…·è¿›è¡Œæƒ…æ„Ÿæ‰“åˆ†\n",
        "3. **ä¸»é¢˜å»ºæ¨¡** - ä½¿ç”¨LDAå‘ç°æ½œåœ¨ä¸»é¢˜\n",
        "4. **åˆ†ç±»å»ºæ¨¡** - è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹\n",
        "\n",
        "### æ–‡ä»¶è¾“å‡º\n",
        "- **æ¸…æ´—åæ•°æ®**: `/data/processed/cleaned_comments.parquet`\n",
        "- **åŒ…å«å­—æ®µ**: id, subreddit.name, timestamp, body, cleaned_body, tokens_cleaned, sentiment, score\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
