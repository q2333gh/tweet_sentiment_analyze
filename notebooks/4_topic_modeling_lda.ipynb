{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 第四步：LDA主题建模 (Topic Modeling with LDA)\n",
        "\n",
        "## 本notebook的目标：\n",
        "1. 加载清洗后的数据\n",
        "2. 使用LDA进行主题提取\n",
        "3. 分析主题关键词\n",
        "4. 分析每个主题下的情感分布\n",
        "5. 主题可视化和分析\n",
        "\n",
        "## 分析重点\n",
        "- 识别climate change相关的主要讨论主题\n",
        "- 分析不同主题的情感倾向\n",
        "- 生成主题关键词列表\n",
        "- 主题分布可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "库导入完成！\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import CountVectorizer, IDF, VectorAssembler\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"库导入完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.0\n",
            "Available cores: 20\n"
          ]
        }
      ],
      "source": [
        "# 初始化Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_TopicModeling\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 情感分析数据加载完成，共 459,171 条记录\n",
            "\n",
            "数据结构:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- subreddit.name: string (nullable = true)\n",
            " |-- created_utc: long (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- cleaned_body: string (nullable = true)\n",
            " |-- tokens_cleaned: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- sentiment: double (nullable = true)\n",
            " |-- vader_sentiment: double (nullable = true)\n",
            " |-- sentiment_category: string (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            "\n",
            "\n",
            "✅ 找到tokens_cleaned列，可以直接进行主题建模\n"
          ]
        }
      ],
      "source": [
        "# 加载清洗后的数据和情感分析结果\n",
        "sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"✅ 情感分析数据加载完成，共 {record_count:,} 条记录\")\n",
        "    \n",
        "    print(\"\\n数据结构:\")\n",
        "    df_sentiment.printSchema()\n",
        "    \n",
        "    # 检查tokens_cleaned列是否存在\n",
        "    if 'tokens_cleaned' in df_sentiment.columns:\n",
        "        print(\"\\n✅ 找到tokens_cleaned列，可以直接进行主题建模\")\n",
        "    else:\n",
        "        print(\"\\n❌ 未找到tokens_cleaned列，需要重新分词\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ 数据加载失败: {e}\")\n",
        "    print(\"尝试加载清洗后的数据...\")\n",
        "    \n",
        "    # 备选方案：加载清洗后的数据\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_sentiment = spark.read.parquet(cleaned_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"✅ 清洗后数据加载完成，共 {record_count:,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 数据预处理 ===\n",
            "过滤后数据量: 447,889 条记录\n",
            "保留比例: 97.5%\n",
            "Climate相关评论: 439,212 条记录\n",
            "占过滤后数据: 98.1%\n",
            "\n",
            "最终用于主题建模的数据: 439,212 条记录\n"
          ]
        }
      ],
      "source": [
        "# 数据预处理：过滤和准备主题建模数据\n",
        "print(\"=== 数据预处理 ===\")\n",
        "\n",
        "# 1. 过滤掉token数量过少的文档（提高主题质量）\n",
        "df_filtered = df_sentiment.filter(\n",
        "    F.size(F.col(\"tokens_cleaned\")) >= 5  # 至少5个词\n",
        ")\n",
        "\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"过滤后数据量: {filtered_count:,} 条记录\")\n",
        "print(f\"保留比例: {filtered_count/record_count*100:.1f}%\")\n",
        "\n",
        "# 2. 进一步过滤Climate Change相关关键词，确保主题相关性\n",
        "climate_keywords = [\n",
        "    'climate', 'warming', 'carbon', 'emission', 'greenhouse', 'temperature',\n",
        "    'fossil', 'renewable', 'energy', 'pollution', 'environment', 'sustainability',\n",
        "    'weather', 'ice', 'sea', 'level', 'drought', 'flood'\n",
        "]\n",
        "\n",
        "# 创建过滤条件：至少包含一个climate相关词汇\n",
        "def contains_climate_keywords(tokens):\n",
        "    if tokens is None:\n",
        "        return False\n",
        "    tokens_lower = [token.lower() for token in tokens]\n",
        "    return any(keyword in tokens_lower for keyword in climate_keywords)\n",
        "\n",
        "contains_climate_udf = F.udf(contains_climate_keywords, BooleanType())\n",
        "\n",
        "df_climate = df_filtered.filter(\n",
        "    contains_climate_udf(F.col(\"tokens_cleaned\"))\n",
        ")\n",
        "\n",
        "climate_count = df_climate.count()\n",
        "print(f\"Climate相关评论: {climate_count:,} 条记录\")\n",
        "print(f\"占过滤后数据: {climate_count/filtered_count*100:.1f}%\")\n",
        "\n",
        "# 缓存最终用于建模的数据\n",
        "df_climate.cache()\n",
        "print(f\"\\n最终用于主题建模的数据: {climate_count:,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 构建词汇特征向量 ===\n",
            "1. 采样数据以减少计算负担...\n",
            "采样后数据量: 132,035 条记录 (原数据的 30.0%)\n",
            "2. 训练CountVectorizer...\n",
            "3. 训练IDF...\n",
            "词汇表大小: 2000\n",
            "特征向量维度: 2000\n",
            "\n",
            "词汇表示例（前20个词）:\n",
            "0: climate\n",
            "1: change\n",
            "2: people\n",
            "3: like\n",
            "4: re\n",
            "5: think\n",
            "6: one\n",
            "7: even\n",
            "8: .\n",
            "9: m\n",
            "10: get\n",
            "11: change.\n",
            "12: also\n",
            "13: going\n",
            "14: us\n",
            "15: much\n",
            "16: make\n",
            "17: world\n",
            "18: need\n",
            "19: global\n"
          ]
        }
      ],
      "source": [
        "# 构建词汇特征向量\n",
        "print(\"=== 构建词汇特征向量 ===\")\n",
        "\n",
        "# 1. 采样数据以减少内存压力\n",
        "print(\"1. 采样数据以减少计算负担...\")\n",
        "sample_fraction = 0.3  # 使用30%的数据进行主题建模\n",
        "df_sample = df_climate.sample(fraction=sample_fraction, seed=42)\n",
        "sample_count = df_sample.count()\n",
        "print(f\"采样后数据量: {sample_count:,} 条记录 (原数据的 {sample_fraction*100}%)\")\n",
        "\n",
        "# 2. CountVectorizer：将tokens转换为词频向量\n",
        "# 设置更小的词汇表大小和更高的最小词频，避免内存问题\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=2000,  # 减少词汇表大小\n",
        "    minDF=10.0       # 提高最小文档频率\n",
        ")\n",
        "\n",
        "print(\"2. 训练CountVectorizer...\")\n",
        "count_model = count_vectorizer.fit(df_sample)\n",
        "df_vectorized = count_model.transform(df_sample)\n",
        "\n",
        "# 3. TF-IDF：计算词汇重要性权重\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "print(\"3. 训练IDF...\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"词汇表大小: {len(count_model.vocabulary)}\")\n",
        "print(f\"特征向量维度: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# 显示词汇表中的一些示例词汇\n",
        "print(\"\\n词汇表示例（前20个词）:\")\n",
        "for i, word in enumerate(count_model.vocabulary[:20]):\n",
        "    print(f\"{i}: {word}\")\n",
        "\n",
        "# 更新climate_count为采样后的数量\n",
        "climate_count = sample_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LDA主题建模 ===\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "LDA.__init__() got an unexpected keyword argument 'topicsCol'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m NUM_TOPICS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# 减少主题数量\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 创建LDA模型\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m lda \u001b[38;5;241m=\u001b[39m \u001b[43mLDA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopicsCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic_distribution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TOPICS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 减少迭代次数以加快训练\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练LDA模型（\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_TOPICS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m个主题）...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mfit(df_tfidf)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: LDA.__init__() got an unexpected keyword argument 'topicsCol'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 49896)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# LDA主题建模\n",
        "print(\"=== LDA主题建模 ===\")\n",
        "\n",
        "# 设置主题数量（减少主题数量以适应较小的数据集）\n",
        "NUM_TOPICS = 5  # 减少主题数量\n",
        "\n",
        "# 创建LDA模型\n",
        "lda = LDA(\n",
        "    featuresCol=\"features\", \n",
        "    topicsCol=\"topic_distribution\",\n",
        "    k=NUM_TOPICS,\n",
        "    maxIter=10,  # 减少迭代次数以加快训练\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"开始训练LDA模型（{NUM_TOPICS}个主题）...\")\n",
        "lda_model = lda.fit(df_tfidf)\n",
        "\n",
        "print(\"\\n✅ LDA模型训练完成！\")\n",
        "print(f\"模型困惑度: {lda_model.logPerplexity(df_tfidf):.2f}\")\n",
        "print(f\"模型对数似然: {lda_model.logLikelihood(df_tfidf):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 提取和分析主题关键词\n",
        "print(\"=== 主题关键词分析 ===\")\n",
        "\n",
        "# 获取每个主题的关键词\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
        "vocabulary = count_model.vocabulary\n",
        "\n",
        "def get_topic_words(topic_data):\n",
        "    \"\"\"将主题的词汇索引转换为实际词汇\"\"\"\n",
        "    topics_list = []\n",
        "    \n",
        "    for row in topic_data.collect():\n",
        "        topic_id = row['topic']\n",
        "        term_indices = row['termIndices']\n",
        "        term_weights = row['termWeights']\n",
        "        \n",
        "        # 转换索引为词汇\n",
        "        words = [vocabulary[idx] for idx in term_indices]\n",
        "        \n",
        "        topics_list.append({\n",
        "            'topic_id': topic_id,\n",
        "            'words': words,\n",
        "            'weights': term_weights\n",
        "        })\n",
        "    \n",
        "    return topics_list\n",
        "\n",
        "topic_words = get_topic_words(topics)\n",
        "\n",
        "# 显示每个主题的关键词\n",
        "print(\"\\n=== 主题关键词列表 ===\")\n",
        "for topic in topic_words:\n",
        "    topic_id = topic['topic_id']\n",
        "    words = topic['words'][:10]  # 显示前10个关键词\n",
        "    weights = topic['weights'][:10]\n",
        "    \n",
        "    print(f\"\\n🔍 主题 {topic_id}:\")\n",
        "    for word, weight in zip(words, weights):\n",
        "        print(f\"  {word}: {weight:.4f}\")\n",
        "    \n",
        "    # 生成主题描述（基于关键词）\n",
        "    key_terms = \", \".join(words[:5])\n",
        "    print(f\"  💡 关键术语: {key_terms}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 为文档分配主题\n",
        "print(\"=== 文档主题分配 ===\")\n",
        "\n",
        "# 转换文档，获得主题分布\n",
        "df_topics = lda_model.transform(df_tfidf)\n",
        "\n",
        "# 为每个文档分配主导主题（概率最高的主题）\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"获取主导主题ID\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return -1\n",
        "    return int(np.argmax(topic_distribution.toArray()))\n",
        "\n",
        "def get_topic_probability(topic_distribution):\n",
        "    \"\"\"获取主导主题的概率\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return 0.0\n",
        "    return float(np.max(topic_distribution.toArray()))\n",
        "\n",
        "get_dominant_topic_udf = F.udf(get_dominant_topic, IntegerType())\n",
        "get_topic_prob_udf = F.udf(get_topic_probability, DoubleType())\n",
        "\n",
        "# 添加主导主题列\n",
        "df_topics = df_topics.withColumn(\n",
        "    \"dominant_topic\", \n",
        "    get_dominant_topic_udf(F.col(\"topic_distribution\"))\n",
        ").withColumn(\n",
        "    \"topic_probability\",\n",
        "    get_topic_prob_udf(F.col(\"topic_distribution\"))\n",
        ")\n",
        "\n",
        "print(\"✅ 主题分配完成\")\n",
        "\n",
        "# 显示主题分布统计\n",
        "print(\"\\n主题分布统计:\")\n",
        "topic_dist = df_topics.groupBy(\"dominant_topic\").count().orderBy(\"dominant_topic\")\n",
        "topic_dist.show()\n",
        "\n",
        "# 转换为Pandas进行更详细的分析\n",
        "topic_dist_pd = topic_dist.toPandas()\n",
        "total_docs = topic_dist_pd['count'].sum()\n",
        "topic_dist_pd['percentage'] = (topic_dist_pd['count'] / total_docs * 100).round(2)\n",
        "\n",
        "print(\"\\n详细主题分布:\")\n",
        "for _, row in topic_dist_pd.iterrows():\n",
        "    topic_id = int(row['dominant_topic'])\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    \n",
        "    # 获取主题关键词\n",
        "    if topic_id >= 0 and topic_id < len(topic_words):\n",
        "        topic_keywords = \", \".join(topic_words[topic_id]['words'][:3])\n",
        "    else:\n",
        "        topic_keywords = \"未定义\"\n",
        "    \n",
        "    print(f\"主题 {topic_id} ({topic_keywords}): {count:,} 文档 ({pct}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分析每个主题下的情感分布\n",
        "print(\"=== 主题情感分析 ===\")\n",
        "\n",
        "# 检查是否有VADER情感分数\n",
        "sentiment_cols = [col for col in df_topics.columns if 'vader' in col.lower() or \n",
        "                 col in ['sentiment', 'compound_score', 'pos_score', 'neu_score', 'neg_score']]\n",
        "\n",
        "print(f\"找到的情感相关列: {sentiment_cols}\")\n",
        "\n",
        "# 如果有VADER分数，使用VADER；否则使用原始sentiment\n",
        "if 'compound_score' in df_topics.columns:\n",
        "    sentiment_col = 'compound_score'\n",
        "    print(\"使用VADER compound score进行情感分析\")\n",
        "elif 'sentiment' in df_topics.columns:\n",
        "    sentiment_col = 'sentiment'\n",
        "    print(\"使用原始sentiment score进行情感分析\")\n",
        "else:\n",
        "    print(\"❌ 未找到情感分数列，跳过情感分析\")\n",
        "    sentiment_col = None\n",
        "\n",
        "if sentiment_col:\n",
        "    # 定义情感分类函数\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"未知\"\n",
        "        elif score > 0.1:\n",
        "            return \"积极\"\n",
        "        elif score < -0.1:\n",
        "            return \"消极\"\n",
        "        else:\n",
        "            return \"中性\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    # 添加情感分类列\n",
        "    df_topic_sentiment = df_topics.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(sentiment_col))\n",
        "    )\n",
        "    \n",
        "    # 计算每个主题的情感分布\n",
        "    topic_sentiment_dist = df_topic_sentiment.groupBy(\"dominant_topic\", \"sentiment_label\").count().orderBy(\"dominant_topic\", \"sentiment_label\")\n",
        "    \n",
        "    print(\"\\n各主题情感分布:\")\n",
        "    topic_sentiment_dist.show()\n",
        "    \n",
        "    # 转换为透视表格式便于分析\n",
        "    topic_sentiment_pd = topic_sentiment_dist.toPandas()\n",
        "    pivot_sentiment = topic_sentiment_pd.pivot(index='dominant_topic', columns='sentiment_label', values='count').fillna(0)\n",
        "    \n",
        "    # 计算比例\n",
        "    pivot_sentiment_pct = pivot_sentiment.div(pivot_sentiment.sum(axis=1), axis=0) * 100\n",
        "    \n",
        "    print(\"\\n各主题情感分布比例(%)：\")\n",
        "    print(pivot_sentiment_pct.round(2))\n",
        "    \n",
        "    # 分析每个主题的情感特征\n",
        "    print(\"\\n=== 主题情感特征分析 ===\")\n",
        "    for topic_id in range(NUM_TOPICS):\n",
        "        if topic_id in pivot_sentiment_pct.index:\n",
        "            row = pivot_sentiment_pct.loc[topic_id]\n",
        "            pos_pct = row.get('积极', 0)\n",
        "            neg_pct = row.get('消极', 0)\n",
        "            neu_pct = row.get('中性', 0)\n",
        "            \n",
        "            # 获取主题关键词\n",
        "            keywords = \", \".join(topic_words[topic_id]['words'][:5])\n",
        "            \n",
        "            # 判断主题情感倾向\n",
        "            if pos_pct > neg_pct + 10:\n",
        "                tendency = \"偏积极\"\n",
        "            elif neg_pct > pos_pct + 10:\n",
        "                tendency = \"偏消极\"\n",
        "            else:\n",
        "                tendency = \"中性\"\n",
        "            \n",
        "            print(f\"\\n主题 {topic_id} ({keywords}):\")\n",
        "            print(f\"  情感倾向: {tendency}\")\n",
        "            print(f\"  积极: {pos_pct:.1f}% | 中性: {neu_pct:.1f}% | 消极: {neg_pct:.1f}%\")\n",
        "            \n",
        "            # 计算情感极化程度\n",
        "            polarization = abs(pos_pct - neg_pct)\n",
        "            print(f\"  情感极化程度: {polarization:.1f}% ({'高' if polarization > 20 else '中' if polarization > 10 else '低'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存主题建模结果\n",
        "print(\"=== 保存结果 ===\")\n",
        "\n",
        "# 1. 保存带主题信息的数据\n",
        "output_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "# 选择需要保存的列\n",
        "columns_to_save = [\n",
        "    \"id\", \"`subreddit.name`\", \"timestamp\", \"cleaned_body\", \n",
        "    \"dominant_topic\", \"topic_probability\"\n",
        "]\n",
        "\n",
        "# 如果有情感分析结果，也包含进去\n",
        "if sentiment_col:\n",
        "    columns_to_save.extend([sentiment_col, \"sentiment_label\"])\n",
        "\n",
        "# 确保列存在再保存\n",
        "available_cols = [col for col in columns_to_save if col in df_topic_sentiment.columns]\n",
        "\n",
        "df_result = df_topic_sentiment.select(available_cols)\n",
        "df_result.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"✅ 主题分析结果已保存到: {output_path}\")\n",
        "print(f\"保存列数: {len(available_cols)}\")\n",
        "print(f\"保存记录数: {df_result.count():,}\")\n",
        "\n",
        "# 2. 保存主题关键词总结\n",
        "import json\n",
        "\n",
        "topic_summary = {\n",
        "    \"model_info\": {\n",
        "        \"num_topics\": NUM_TOPICS,\n",
        "        \"vocab_size\": len(count_model.vocabulary),\n",
        "        \"total_documents\": climate_count,\n",
        "        \"log_perplexity\": lda_model.logPerplexity(df_tfidf),\n",
        "        \"log_likelihood\": lda_model.logLikelihood(df_tfidf)\n",
        "    },\n",
        "    \"topics\": []\n",
        "}\n",
        "\n",
        "for topic_id, topic_data in enumerate(topic_words):\n",
        "    topic_info = {\n",
        "        \"topic_id\": topic_id,\n",
        "        \"keywords\": topic_data['words'][:10],\n",
        "        \"weights\": [float(w) for w in topic_data['weights'][:10]],\n",
        "        \"document_count\": int(topic_dist_pd[topic_dist_pd['dominant_topic'] == topic_id]['count'].iloc[0]) if topic_id in topic_dist_pd['dominant_topic'].values else 0\n",
        "    }\n",
        "    \n",
        "    # 如果有情感数据，添加情感分布\n",
        "    if sentiment_col and topic_id in pivot_sentiment_pct.index:\n",
        "        topic_info[\"sentiment_distribution\"] = {\n",
        "            \"positive\": float(pivot_sentiment_pct.loc[topic_id].get('积极', 0)),\n",
        "            \"neutral\": float(pivot_sentiment_pct.loc[topic_id].get('中性', 0)),\n",
        "            \"negative\": float(pivot_sentiment_pct.loc[topic_id].get('消极', 0))\n",
        "        }\n",
        "    \n",
        "    topic_summary[\"topics\"].append(topic_info)\n",
        "\n",
        "# 保存主题总结到JSON文件\n",
        "summary_path = \"/home/jovyan/work/data/processed/topic_summary.json\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(topic_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ 主题总结已保存到: {summary_path}\")\n",
        "\n",
        "print(\"\\n=== LDA主题建模完成！ ===\")\n",
        "print(f\"共识别出 {NUM_TOPICS} 个主题\")\n",
        "print(f\"处理了 {climate_count:,} 条climate相关评论\")\n",
        "print(\"主题分析结果可用于后续的分类建模\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
