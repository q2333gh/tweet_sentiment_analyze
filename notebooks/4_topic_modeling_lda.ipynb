{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ç¬¬å››æ­¥ï¼šLDAä¸»é¢˜å»ºæ¨¡ (Topic Modeling with LDA)\n",
        "\n",
        "## æœ¬notebookçš„ç›®æ ‡ï¼š\n",
        "1. åŠ è½½æ¸…æ´—åçš„æ•°æ®\n",
        "2. ä½¿ç”¨LDAè¿›è¡Œä¸»é¢˜æå–\n",
        "3. åˆ†æä¸»é¢˜å…³é”®è¯\n",
        "4. åˆ†ææ¯ä¸ªä¸»é¢˜ä¸‹çš„æƒ…æ„Ÿåˆ†å¸ƒ\n",
        "5. ä¸»é¢˜å¯è§†åŒ–å’Œåˆ†æ\n",
        "\n",
        "## åˆ†æé‡ç‚¹\n",
        "- è¯†åˆ«climate changeç›¸å…³çš„ä¸»è¦è®¨è®ºä¸»é¢˜\n",
        "- åˆ†æä¸åŒä¸»é¢˜çš„æƒ…æ„Ÿå€¾å‘\n",
        "- ç”Ÿæˆä¸»é¢˜å…³é”®è¯åˆ—è¡¨\n",
        "- ä¸»é¢˜åˆ†å¸ƒå¯è§†åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "åº“å¯¼å…¥å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import CountVectorizer, IDF, VectorAssembler\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"åº“å¯¼å…¥å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.0\n",
            "Available cores: 20\n"
          ]
        }
      ],
      "source": [
        "# åˆå§‹åŒ–Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_TopicModeling\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æƒ…æ„Ÿåˆ†ææ•°æ®åŠ è½½å®Œæˆï¼Œå…± 459,171 æ¡è®°å½•\n",
            "\n",
            "æ•°æ®ç»“æ„:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- subreddit.name: string (nullable = true)\n",
            " |-- created_utc: long (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- cleaned_body: string (nullable = true)\n",
            " |-- tokens_cleaned: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- sentiment: double (nullable = true)\n",
            " |-- vader_sentiment: double (nullable = true)\n",
            " |-- sentiment_category: string (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            "\n",
            "\n",
            "âœ… æ‰¾åˆ°tokens_cleanedåˆ—ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œä¸»é¢˜å»ºæ¨¡\n"
          ]
        }
      ],
      "source": [
        "# åŠ è½½æ¸…æ´—åçš„æ•°æ®å’Œæƒ…æ„Ÿåˆ†æç»“æœ\n",
        "sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"âœ… æƒ…æ„Ÿåˆ†ææ•°æ®åŠ è½½å®Œæˆï¼Œå…± {record_count:,} æ¡è®°å½•\")\n",
        "    \n",
        "    print(\"\\næ•°æ®ç»“æ„:\")\n",
        "    df_sentiment.printSchema()\n",
        "    \n",
        "    # æ£€æŸ¥tokens_cleanedåˆ—æ˜¯å¦å­˜åœ¨\n",
        "    if 'tokens_cleaned' in df_sentiment.columns:\n",
        "        print(\"\\nâœ… æ‰¾åˆ°tokens_cleanedåˆ—ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œä¸»é¢˜å»ºæ¨¡\")\n",
        "    else:\n",
        "        print(\"\\nâŒ æœªæ‰¾åˆ°tokens_cleanedåˆ—ï¼Œéœ€è¦é‡æ–°åˆ†è¯\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}\")\n",
        "    print(\"å°è¯•åŠ è½½æ¸…æ´—åçš„æ•°æ®...\")\n",
        "    \n",
        "    # å¤‡é€‰æ–¹æ¡ˆï¼šåŠ è½½æ¸…æ´—åçš„æ•°æ®\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_sentiment = spark.read.parquet(cleaned_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"âœ… æ¸…æ´—åæ•°æ®åŠ è½½å®Œæˆï¼Œå…± {record_count:,} æ¡è®°å½•\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== æ•°æ®é¢„å¤„ç† ===\n",
            "è¿‡æ»¤åæ•°æ®é‡: 447,889 æ¡è®°å½•\n",
            "ä¿ç•™æ¯”ä¾‹: 97.5%\n",
            "Climateç›¸å…³è¯„è®º: 439,212 æ¡è®°å½•\n",
            "å è¿‡æ»¤åæ•°æ®: 98.1%\n",
            "\n",
            "æœ€ç»ˆç”¨äºä¸»é¢˜å»ºæ¨¡çš„æ•°æ®: 439,212 æ¡è®°å½•\n"
          ]
        }
      ],
      "source": [
        "# æ•°æ®é¢„å¤„ç†ï¼šè¿‡æ»¤å’Œå‡†å¤‡ä¸»é¢˜å»ºæ¨¡æ•°æ®\n",
        "print(\"=== æ•°æ®é¢„å¤„ç† ===\")\n",
        "\n",
        "# 1. è¿‡æ»¤æ‰tokenæ•°é‡è¿‡å°‘çš„æ–‡æ¡£ï¼ˆæé«˜ä¸»é¢˜è´¨é‡ï¼‰\n",
        "df_filtered = df_sentiment.filter(\n",
        "    F.size(F.col(\"tokens_cleaned\")) >= 5  # è‡³å°‘5ä¸ªè¯\n",
        ")\n",
        "\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"è¿‡æ»¤åæ•°æ®é‡: {filtered_count:,} æ¡è®°å½•\")\n",
        "print(f\"ä¿ç•™æ¯”ä¾‹: {filtered_count/record_count*100:.1f}%\")\n",
        "\n",
        "# 2. è¿›ä¸€æ­¥è¿‡æ»¤Climate Changeç›¸å…³å…³é”®è¯ï¼Œç¡®ä¿ä¸»é¢˜ç›¸å…³æ€§\n",
        "climate_keywords = [\n",
        "    'climate', 'warming', 'carbon', 'emission', 'greenhouse', 'temperature',\n",
        "    'fossil', 'renewable', 'energy', 'pollution', 'environment', 'sustainability',\n",
        "    'weather', 'ice', 'sea', 'level', 'drought', 'flood'\n",
        "]\n",
        "\n",
        "# åˆ›å»ºè¿‡æ»¤æ¡ä»¶ï¼šè‡³å°‘åŒ…å«ä¸€ä¸ªclimateç›¸å…³è¯æ±‡\n",
        "def contains_climate_keywords(tokens):\n",
        "    if tokens is None:\n",
        "        return False\n",
        "    tokens_lower = [token.lower() for token in tokens]\n",
        "    return any(keyword in tokens_lower for keyword in climate_keywords)\n",
        "\n",
        "contains_climate_udf = F.udf(contains_climate_keywords, BooleanType())\n",
        "\n",
        "df_climate = df_filtered.filter(\n",
        "    contains_climate_udf(F.col(\"tokens_cleaned\"))\n",
        ")\n",
        "\n",
        "climate_count = df_climate.count()\n",
        "print(f\"Climateç›¸å…³è¯„è®º: {climate_count:,} æ¡è®°å½•\")\n",
        "print(f\"å è¿‡æ»¤åæ•°æ®: {climate_count/filtered_count*100:.1f}%\")\n",
        "\n",
        "# ç¼“å­˜æœ€ç»ˆç”¨äºå»ºæ¨¡çš„æ•°æ®\n",
        "df_climate.cache()\n",
        "print(f\"\\næœ€ç»ˆç”¨äºä¸»é¢˜å»ºæ¨¡çš„æ•°æ®: {climate_count:,} æ¡è®°å½•\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== æ„å»ºè¯æ±‡ç‰¹å¾å‘é‡ ===\n",
            "è®­ç»ƒCountVectorizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 35026)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "ename": "Py4JError",
          "evalue": "An error occurred while calling o60.fit",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(\n\u001b[1;32m      7\u001b[0m     inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      8\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_features\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     vocabSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,  \u001b[38;5;66;03m# è¯æ±‡è¡¨å¤§å°\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     minDF\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m        \u001b[38;5;66;03m# æœ€å°æ–‡æ¡£é¢‘ç‡ï¼šè¯æ±‡è‡³å°‘åœ¨5ä¸ªæ–‡æ¡£ä¸­å‡ºç°\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè®­ç»ƒCountVectorizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m count_model \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_climate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m df_vectorized \u001b[38;5;241m=\u001b[39m count_model\u001b[38;5;241m.\u001b[39mtransform(df_climate)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 2. TF-IDFï¼šè®¡ç®—è¯æ±‡é‡è¦æ€§æƒé‡\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o60.fit"
          ]
        }
      ],
      "source": [
        "# æ„å»ºè¯æ±‡ç‰¹å¾å‘é‡\n",
        "print(\"=== æ„å»ºè¯æ±‡ç‰¹å¾å‘é‡ ===\")\n",
        "\n",
        "# 1. CountVectorizerï¼šå°†tokensè½¬æ¢ä¸ºè¯é¢‘å‘é‡\n",
        "# è®¾ç½®è¾ƒå°çš„è¯æ±‡è¡¨å¤§å°å’Œæœ€å°è¯é¢‘ï¼Œé¿å…å†…å­˜é—®é¢˜\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=5000,  # è¯æ±‡è¡¨å¤§å°\n",
        "    minDF=5.0        # æœ€å°æ–‡æ¡£é¢‘ç‡ï¼šè¯æ±‡è‡³å°‘åœ¨5ä¸ªæ–‡æ¡£ä¸­å‡ºç°\n",
        ")\n",
        "\n",
        "print(\"è®­ç»ƒCountVectorizer...\")\n",
        "count_model = count_vectorizer.fit(df_climate)\n",
        "df_vectorized = count_model.transform(df_climate)\n",
        "\n",
        "# 2. TF-IDFï¼šè®¡ç®—è¯æ±‡é‡è¦æ€§æƒé‡\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "print(\"è®­ç»ƒIDF...\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"è¯æ±‡è¡¨å¤§å°: {len(count_model.vocabulary)}\")\n",
        "print(f\"ç‰¹å¾å‘é‡ç»´åº¦: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# æ˜¾ç¤ºè¯æ±‡è¡¨ä¸­çš„ä¸€äº›ç¤ºä¾‹è¯æ±‡\n",
        "print(\"\\nè¯æ±‡è¡¨ç¤ºä¾‹ï¼ˆå‰20ä¸ªè¯ï¼‰:\")\n",
        "for i, word in enumerate(count_model.vocabulary[:20]):\n",
        "    print(f\"{i}: {word}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LDAä¸»é¢˜å»ºæ¨¡\n",
        "print(\"=== LDAä¸»é¢˜å»ºæ¨¡ ===\")\n",
        "\n",
        "# è®¾ç½®ä¸»é¢˜æ•°é‡\n",
        "NUM_TOPICS = 8  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
        "\n",
        "# åˆ›å»ºLDAæ¨¡å‹\n",
        "lda = LDA(\n",
        "    featuresCol=\"features\", \n",
        "    topicsCol=\"topic_distribution\",\n",
        "    k=NUM_TOPICS,\n",
        "    maxIter=20,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"å¼€å§‹è®­ç»ƒLDAæ¨¡å‹ï¼ˆ{NUM_TOPICS}ä¸ªä¸»é¢˜ï¼‰...\")\n",
        "lda_model = lda.fit(df_tfidf)\n",
        "\n",
        "print(\"\\nâœ… LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n",
        "print(f\"æ¨¡å‹å›°æƒ‘åº¦: {lda_model.logPerplexity(df_tfidf):.2f}\")\n",
        "print(f\"æ¨¡å‹å¯¹æ•°ä¼¼ç„¶: {lda_model.logLikelihood(df_tfidf):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æå–å’Œåˆ†æä¸»é¢˜å…³é”®è¯\n",
        "print(\"=== ä¸»é¢˜å…³é”®è¯åˆ†æ ===\")\n",
        "\n",
        "# è·å–æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
        "vocabulary = count_model.vocabulary\n",
        "\n",
        "def get_topic_words(topic_data):\n",
        "    \"\"\"å°†ä¸»é¢˜çš„è¯æ±‡ç´¢å¼•è½¬æ¢ä¸ºå®é™…è¯æ±‡\"\"\"\n",
        "    topics_list = []\n",
        "    \n",
        "    for row in topic_data.collect():\n",
        "        topic_id = row['topic']\n",
        "        term_indices = row['termIndices']\n",
        "        term_weights = row['termWeights']\n",
        "        \n",
        "        # è½¬æ¢ç´¢å¼•ä¸ºè¯æ±‡\n",
        "        words = [vocabulary[idx] for idx in term_indices]\n",
        "        \n",
        "        topics_list.append({\n",
        "            'topic_id': topic_id,\n",
        "            'words': words,\n",
        "            'weights': term_weights\n",
        "        })\n",
        "    \n",
        "    return topics_list\n",
        "\n",
        "topic_words = get_topic_words(topics)\n",
        "\n",
        "# æ˜¾ç¤ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯\n",
        "print(\"\\n=== ä¸»é¢˜å…³é”®è¯åˆ—è¡¨ ===\")\n",
        "for topic in topic_words:\n",
        "    topic_id = topic['topic_id']\n",
        "    words = topic['words'][:10]  # æ˜¾ç¤ºå‰10ä¸ªå…³é”®è¯\n",
        "    weights = topic['weights'][:10]\n",
        "    \n",
        "    print(f\"\\nğŸ” ä¸»é¢˜ {topic_id}:\")\n",
        "    for word, weight in zip(words, weights):\n",
        "        print(f\"  {word}: {weight:.4f}\")\n",
        "    \n",
        "    # ç”Ÿæˆä¸»é¢˜æè¿°ï¼ˆåŸºäºå…³é”®è¯ï¼‰\n",
        "    key_terms = \", \".join(words[:5])\n",
        "    print(f\"  ğŸ’¡ å…³é”®æœ¯è¯­: {key_terms}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¸ºæ–‡æ¡£åˆ†é…ä¸»é¢˜\n",
        "print(\"=== æ–‡æ¡£ä¸»é¢˜åˆ†é… ===\")\n",
        "\n",
        "# è½¬æ¢æ–‡æ¡£ï¼Œè·å¾—ä¸»é¢˜åˆ†å¸ƒ\n",
        "df_topics = lda_model.transform(df_tfidf)\n",
        "\n",
        "# ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸»å¯¼ä¸»é¢˜ï¼ˆæ¦‚ç‡æœ€é«˜çš„ä¸»é¢˜ï¼‰\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"è·å–ä¸»å¯¼ä¸»é¢˜ID\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return -1\n",
        "    return int(np.argmax(topic_distribution.toArray()))\n",
        "\n",
        "def get_topic_probability(topic_distribution):\n",
        "    \"\"\"è·å–ä¸»å¯¼ä¸»é¢˜çš„æ¦‚ç‡\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return 0.0\n",
        "    return float(np.max(topic_distribution.toArray()))\n",
        "\n",
        "get_dominant_topic_udf = F.udf(get_dominant_topic, IntegerType())\n",
        "get_topic_prob_udf = F.udf(get_topic_probability, DoubleType())\n",
        "\n",
        "# æ·»åŠ ä¸»å¯¼ä¸»é¢˜åˆ—\n",
        "df_topics = df_topics.withColumn(\n",
        "    \"dominant_topic\", \n",
        "    get_dominant_topic_udf(F.col(\"topic_distribution\"))\n",
        ").withColumn(\n",
        "    \"topic_probability\",\n",
        "    get_topic_prob_udf(F.col(\"topic_distribution\"))\n",
        ")\n",
        "\n",
        "print(\"âœ… ä¸»é¢˜åˆ†é…å®Œæˆ\")\n",
        "\n",
        "# æ˜¾ç¤ºä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡\n",
        "print(\"\\nä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡:\")\n",
        "topic_dist = df_topics.groupBy(\"dominant_topic\").count().orderBy(\"dominant_topic\")\n",
        "topic_dist.show()\n",
        "\n",
        "# è½¬æ¢ä¸ºPandasè¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æ\n",
        "topic_dist_pd = topic_dist.toPandas()\n",
        "total_docs = topic_dist_pd['count'].sum()\n",
        "topic_dist_pd['percentage'] = (topic_dist_pd['count'] / total_docs * 100).round(2)\n",
        "\n",
        "print(\"\\nè¯¦ç»†ä¸»é¢˜åˆ†å¸ƒ:\")\n",
        "for _, row in topic_dist_pd.iterrows():\n",
        "    topic_id = int(row['dominant_topic'])\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    \n",
        "    # è·å–ä¸»é¢˜å…³é”®è¯\n",
        "    if topic_id >= 0 and topic_id < len(topic_words):\n",
        "        topic_keywords = \", \".join(topic_words[topic_id]['words'][:3])\n",
        "    else:\n",
        "        topic_keywords = \"æœªå®šä¹‰\"\n",
        "    \n",
        "    print(f\"ä¸»é¢˜ {topic_id} ({topic_keywords}): {count:,} æ–‡æ¡£ ({pct}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ†ææ¯ä¸ªä¸»é¢˜ä¸‹çš„æƒ…æ„Ÿåˆ†å¸ƒ\n",
        "print(\"=== ä¸»é¢˜æƒ…æ„Ÿåˆ†æ ===\")\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦æœ‰VADERæƒ…æ„Ÿåˆ†æ•°\n",
        "sentiment_cols = [col for col in df_topics.columns if 'vader' in col.lower() or \n",
        "                 col in ['sentiment', 'compound_score', 'pos_score', 'neu_score', 'neg_score']]\n",
        "\n",
        "print(f\"æ‰¾åˆ°çš„æƒ…æ„Ÿç›¸å…³åˆ—: {sentiment_cols}\")\n",
        "\n",
        "# å¦‚æœæœ‰VADERåˆ†æ•°ï¼Œä½¿ç”¨VADERï¼›å¦åˆ™ä½¿ç”¨åŸå§‹sentiment\n",
        "if 'compound_score' in df_topics.columns:\n",
        "    sentiment_col = 'compound_score'\n",
        "    print(\"ä½¿ç”¨VADER compound scoreè¿›è¡Œæƒ…æ„Ÿåˆ†æ\")\n",
        "elif 'sentiment' in df_topics.columns:\n",
        "    sentiment_col = 'sentiment'\n",
        "    print(\"ä½¿ç”¨åŸå§‹sentiment scoreè¿›è¡Œæƒ…æ„Ÿåˆ†æ\")\n",
        "else:\n",
        "    print(\"âŒ æœªæ‰¾åˆ°æƒ…æ„Ÿåˆ†æ•°åˆ—ï¼Œè·³è¿‡æƒ…æ„Ÿåˆ†æ\")\n",
        "    sentiment_col = None\n",
        "\n",
        "if sentiment_col:\n",
        "    # å®šä¹‰æƒ…æ„Ÿåˆ†ç±»å‡½æ•°\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"æœªçŸ¥\"\n",
        "        elif score > 0.1:\n",
        "            return \"ç§¯æ\"\n",
        "        elif score < -0.1:\n",
        "            return \"æ¶ˆæ\"\n",
        "        else:\n",
        "            return \"ä¸­æ€§\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    # æ·»åŠ æƒ…æ„Ÿåˆ†ç±»åˆ—\n",
        "    df_topic_sentiment = df_topics.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(sentiment_col))\n",
        "    )\n",
        "    \n",
        "    # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„æƒ…æ„Ÿåˆ†å¸ƒ\n",
        "    topic_sentiment_dist = df_topic_sentiment.groupBy(\"dominant_topic\", \"sentiment_label\").count().orderBy(\"dominant_topic\", \"sentiment_label\")\n",
        "    \n",
        "    print(\"\\nå„ä¸»é¢˜æƒ…æ„Ÿåˆ†å¸ƒ:\")\n",
        "    topic_sentiment_dist.show()\n",
        "    \n",
        "    # è½¬æ¢ä¸ºé€è§†è¡¨æ ¼å¼ä¾¿äºåˆ†æ\n",
        "    topic_sentiment_pd = topic_sentiment_dist.toPandas()\n",
        "    pivot_sentiment = topic_sentiment_pd.pivot(index='dominant_topic', columns='sentiment_label', values='count').fillna(0)\n",
        "    \n",
        "    # è®¡ç®—æ¯”ä¾‹\n",
        "    pivot_sentiment_pct = pivot_sentiment.div(pivot_sentiment.sum(axis=1), axis=0) * 100\n",
        "    \n",
        "    print(\"\\nå„ä¸»é¢˜æƒ…æ„Ÿåˆ†å¸ƒæ¯”ä¾‹(%)ï¼š\")\n",
        "    print(pivot_sentiment_pct.round(2))\n",
        "    \n",
        "    # åˆ†ææ¯ä¸ªä¸»é¢˜çš„æƒ…æ„Ÿç‰¹å¾\n",
        "    print(\"\\n=== ä¸»é¢˜æƒ…æ„Ÿç‰¹å¾åˆ†æ ===\")\n",
        "    for topic_id in range(NUM_TOPICS):\n",
        "        if topic_id in pivot_sentiment_pct.index:\n",
        "            row = pivot_sentiment_pct.loc[topic_id]\n",
        "            pos_pct = row.get('ç§¯æ', 0)\n",
        "            neg_pct = row.get('æ¶ˆæ', 0)\n",
        "            neu_pct = row.get('ä¸­æ€§', 0)\n",
        "            \n",
        "            # è·å–ä¸»é¢˜å…³é”®è¯\n",
        "            keywords = \", \".join(topic_words[topic_id]['words'][:5])\n",
        "            \n",
        "            # åˆ¤æ–­ä¸»é¢˜æƒ…æ„Ÿå€¾å‘\n",
        "            if pos_pct > neg_pct + 10:\n",
        "                tendency = \"åç§¯æ\"\n",
        "            elif neg_pct > pos_pct + 10:\n",
        "                tendency = \"åæ¶ˆæ\"\n",
        "            else:\n",
        "                tendency = \"ä¸­æ€§\"\n",
        "            \n",
        "            print(f\"\\nä¸»é¢˜ {topic_id} ({keywords}):\")\n",
        "            print(f\"  æƒ…æ„Ÿå€¾å‘: {tendency}\")\n",
        "            print(f\"  ç§¯æ: {pos_pct:.1f}% | ä¸­æ€§: {neu_pct:.1f}% | æ¶ˆæ: {neg_pct:.1f}%\")\n",
        "            \n",
        "            # è®¡ç®—æƒ…æ„ŸæåŒ–ç¨‹åº¦\n",
        "            polarization = abs(pos_pct - neg_pct)\n",
        "            print(f\"  æƒ…æ„ŸæåŒ–ç¨‹åº¦: {polarization:.1f}% ({'é«˜' if polarization > 20 else 'ä¸­' if polarization > 10 else 'ä½'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜ä¸»é¢˜å»ºæ¨¡ç»“æœ\n",
        "print(\"=== ä¿å­˜ç»“æœ ===\")\n",
        "\n",
        "# 1. ä¿å­˜å¸¦ä¸»é¢˜ä¿¡æ¯çš„æ•°æ®\n",
        "output_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "# é€‰æ‹©éœ€è¦ä¿å­˜çš„åˆ—\n",
        "columns_to_save = [\n",
        "    \"id\", \"`subreddit.name`\", \"timestamp\", \"cleaned_body\", \n",
        "    \"dominant_topic\", \"topic_probability\"\n",
        "]\n",
        "\n",
        "# å¦‚æœæœ‰æƒ…æ„Ÿåˆ†æç»“æœï¼Œä¹ŸåŒ…å«è¿›å»\n",
        "if sentiment_col:\n",
        "    columns_to_save.extend([sentiment_col, \"sentiment_label\"])\n",
        "\n",
        "# ç¡®ä¿åˆ—å­˜åœ¨å†ä¿å­˜\n",
        "available_cols = [col for col in columns_to_save if col in df_topic_sentiment.columns]\n",
        "\n",
        "df_result = df_topic_sentiment.select(available_cols)\n",
        "df_result.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"âœ… ä¸»é¢˜åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}\")\n",
        "print(f\"ä¿å­˜åˆ—æ•°: {len(available_cols)}\")\n",
        "print(f\"ä¿å­˜è®°å½•æ•°: {df_result.count():,}\")\n",
        "\n",
        "# 2. ä¿å­˜ä¸»é¢˜å…³é”®è¯æ€»ç»“\n",
        "import json\n",
        "\n",
        "topic_summary = {\n",
        "    \"model_info\": {\n",
        "        \"num_topics\": NUM_TOPICS,\n",
        "        \"vocab_size\": len(count_model.vocabulary),\n",
        "        \"total_documents\": climate_count,\n",
        "        \"log_perplexity\": lda_model.logPerplexity(df_tfidf),\n",
        "        \"log_likelihood\": lda_model.logLikelihood(df_tfidf)\n",
        "    },\n",
        "    \"topics\": []\n",
        "}\n",
        "\n",
        "for topic_id, topic_data in enumerate(topic_words):\n",
        "    topic_info = {\n",
        "        \"topic_id\": topic_id,\n",
        "        \"keywords\": topic_data['words'][:10],\n",
        "        \"weights\": [float(w) for w in topic_data['weights'][:10]],\n",
        "        \"document_count\": int(topic_dist_pd[topic_dist_pd['dominant_topic'] == topic_id]['count'].iloc[0]) if topic_id in topic_dist_pd['dominant_topic'].values else 0\n",
        "    }\n",
        "    \n",
        "    # å¦‚æœæœ‰æƒ…æ„Ÿæ•°æ®ï¼Œæ·»åŠ æƒ…æ„Ÿåˆ†å¸ƒ\n",
        "    if sentiment_col and topic_id in pivot_sentiment_pct.index:\n",
        "        topic_info[\"sentiment_distribution\"] = {\n",
        "            \"positive\": float(pivot_sentiment_pct.loc[topic_id].get('ç§¯æ', 0)),\n",
        "            \"neutral\": float(pivot_sentiment_pct.loc[topic_id].get('ä¸­æ€§', 0)),\n",
        "            \"negative\": float(pivot_sentiment_pct.loc[topic_id].get('æ¶ˆæ', 0))\n",
        "        }\n",
        "    \n",
        "    topic_summary[\"topics\"].append(topic_info)\n",
        "\n",
        "# ä¿å­˜ä¸»é¢˜æ€»ç»“åˆ°JSONæ–‡ä»¶\n",
        "summary_path = \"/home/jovyan/work/data/processed/topic_summary.json\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(topic_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… ä¸»é¢˜æ€»ç»“å·²ä¿å­˜åˆ°: {summary_path}\")\n",
        "\n",
        "print(\"\\n=== LDAä¸»é¢˜å»ºæ¨¡å®Œæˆï¼ ===\")\n",
        "print(f\"å…±è¯†åˆ«å‡º {NUM_TOPICS} ä¸ªä¸»é¢˜\")\n",
        "print(f\"å¤„ç†äº† {climate_count:,} æ¡climateç›¸å…³è¯„è®º\")\n",
        "print(\"ä¸»é¢˜åˆ†æç»“æœå¯ç”¨äºåç»­çš„åˆ†ç±»å»ºæ¨¡\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
