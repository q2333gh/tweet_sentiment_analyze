{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Step 4: LDA Topic Modeling (Topic Modeling with LDA)\n",
        "\n",
        "## Objectives of this notebook:\n",
        "1. Load cleaned data\n",
        "2. Perform topic extraction using LDA\n",
        "3. Analyze topic keywords\n",
        "4. Analyze sentiment distribution for each topic\n",
        "5. Topic visualization and analysis\n",
        "\n",
        "## Analysis Focus\n",
        "- Identify mainlyly discussion topics related to climate change\n",
        "- Analyze sentiment tendencies for different topics\n",
        "- Generate topic keyword lists\n",
        "- Topic distribution visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import CountVectorizer, IDF, VectorAssembler\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set chart style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_TopicModeling\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loadingcleaned data and sentiment analysisresults\n",
        "sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"✅ sentiment analysis data loadingcompleted，total {record_count:,} records\")\n",
        "    \n",
        "    print(\"\\nData structure:\")\n",
        "    df_sentiment.printSchema()\n",
        "    \n",
        "    # Checkingwhether tokens_cleaned column exists\n",
        "    if 'tokens_cleaned' in df_sentiment.columns:\n",
        "        print(\"\\n✅ Found tokens_cleaned column，can proceed directly with topic modeling\")\n",
        "    else:\n",
        "        print(\"\\n❌ tokens_cleaned column not found，need to re-tokenize\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Data loading failed: {e}\")\n",
        "    print(\"try loading cleaned data...\")\n",
        "    \n",
        "    # alternativesolution：loadingcleaned data\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_sentiment = spark.read.parquet(cleaned_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"✅ cleaned data loadingcompleted，total {record_count:,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data preprocessing：filter and prepare topic modelingdata\n",
        "print(\"=== data preprocessing ===\")\n",
        "\n",
        "# 1. Filter out documents with too few tokens（improve topic quality）\n",
        "df_filtered = df_sentiment.filter(\n",
        "    F.size(F.col(\"tokens_cleaned\")) >= 5  # at least 5 words\n",
        ")\n",
        "\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"Data size after filtering: {filtered_count:,} records\")\n",
        "print(f\"Retention ratio: {filtered_count/record_count*100:.1f}%\")\n",
        "\n",
        "# 2. Further filter Climate Change related keywords，ensure topic relevance\n",
        "climate_keywords = [\n",
        "    'climate', 'warming', 'carbon', 'emission', 'greenhouse', 'temperature',\n",
        "    'fossil', 'renewable', 'energy', 'pollution', 'environment', 'sustainability',\n",
        "    'weather', 'ice', 'sea', 'level', 'drought', 'flood'\n",
        "]\n",
        "\n",
        "# create filter condition：contains at least one climate-related word\n",
        "def contains_climate_keywords(tokens):\n",
        "    if tokens is None:\n",
        "        return False\n",
        "    tokens_lower = [token.lower() for token in tokens]\n",
        "    return any(keyword in tokens_lower for keyword in climate_keywords)\n",
        "\n",
        "contains_climate_udf = F.udf(contains_climate_keywords, BooleanType())\n",
        "\n",
        "df_climate = df_filtered.filter(\n",
        "    contains_climate_udf(F.col(\"tokens_cleaned\"))\n",
        ")\n",
        "\n",
        "climate_count = df_climate.count()\n",
        "print(f\"Climate-related comments: {climate_count:,} records\")\n",
        "print(f\"percentage of filtered data: {climate_count/filtered_count*100:.1f}%\")\n",
        "\n",
        "# cache final data for modeling\n",
        "df_climate.cache()\n",
        "print(f\"\\nfinal data for topic modeling: {climate_count:,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build vocabulary feature vectors\n",
        "print(\"=== Build vocabulary feature vectors ===\")\n",
        "\n",
        "# 1. sample data to reduce memory pressure\n",
        "print(\"1. 采样data以reduce computational burden...\")\n",
        "sample_fraction = 0.3  # use 30% of data for topic modeling\n",
        "df_sample = df_climate.sample(fraction=sample_fraction, seed=42)\n",
        "sample_count = df_sample.count()\n",
        "print(f\"data size after sampling: {sample_count:,} records (of original data {sample_fraction*100}%)\")\n",
        "\n",
        "# 2. CountVectorizer：Convert tokens to term frequency vectors\n",
        "# set smaller vocabulary size and higher minimum word frequency，avoid memory issues\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=2000,  # Reduce vocabulary size\n",
        "    minDF=10.0       # improve minimum document frequency\n",
        ")\n",
        "\n",
        "print(\"2. trainingCountVectorizer...\")\n",
        "count_model = count_vectorizer.fit(df_sample)\n",
        "df_vectorized = count_model.transform(df_sample)\n",
        "\n",
        "# 3. TF-IDF：Calculate word importance weights\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "print(\"3. trainingIDF...\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"Vocabulary size: {len(count_model.vocabulary)}\")\n",
        "print(f\"Feature vector dimensions: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# display some example words in vocabulary\n",
        "print(\"\\nVocabulary examples（first 20 words）:\")\n",
        "for i, word in enumerate(count_model.vocabulary[:20]):\n",
        "    print(f\"{i}: {word}\")\n",
        "\n",
        "# update climate_count to post-sampling count\n",
        "climate_count = sample_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LDA Topic Modeling\n",
        "print(\"=== LDA Topic Modeling ===\")\n",
        "\n",
        "# Set number of topics（reduce number of topics to fit smaller dataset）\n",
        "NUM_TOPICS = 5  # reduce number of topics\n",
        "\n",
        "# create LDA model\n",
        "lda = LDA(\n",
        "    featuresCol=\"features\", \n",
        "    topicsCol=\"topic_distribution\",\n",
        "    k=NUM_TOPICS,\n",
        "    maxIter=10,  # reduce iteration count to speed up training\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"start training LDA model（{NUM_TOPICS}topics）...\")\n",
        "lda_model = lda.fit(df_tfidf)\n",
        "\n",
        "print(\"\\n✅ LDA model training completed!\")\n",
        "print(f\"model perplexity: {lda_model.logPerplexity(df_tfidf):.2f}\")\n",
        "print(f\"model log likelihood: {lda_model.logLikelihood(df_tfidf):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and analyze topic keywords\n",
        "print(\"=== Topic Keywords Analysis ===\")\n",
        "\n",
        "# get keywords for each topic\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
        "vocabulary = count_model.vocabulary\n",
        "\n",
        "def get_topic_words(topic_data):\n",
        "    \"\"\"Convert topic word indices to actual words\"\"\"\n",
        "    topics_list = []\n",
        "    \n",
        "    for row in topic_data.collect():\n",
        "        topic_id = row['topic']\n",
        "        term_indices = row['termIndices']\n",
        "        term_weights = row['termWeights']\n",
        "        \n",
        "        # Convert indices to words\n",
        "        words = [vocabulary[idx] for idx in term_indices]\n",
        "        \n",
        "        topics_list.append({\n",
        "            'topic_id': topic_id,\n",
        "            'words': words,\n",
        "            'weights': term_weights\n",
        "        })\n",
        "    \n",
        "    return topics_list\n",
        "\n",
        "topic_words = get_topic_words(topics)\n",
        "\n",
        "# display keywords for each topic\n",
        "print(\"\\n=== Topic Keywords List ===\")\n",
        "for topic in topic_words:\n",
        "    topic_id = topic['topic_id']\n",
        "    words = topic['words'][:10]  # Display top 10 keywords\n",
        "    weights = topic['weights'][:10]\n",
        "    \n",
        "    print(f\"\\n🔍 Topic {topic_id}:\")\n",
        "    for word, weight in zip(words, weights):\n",
        "        print(f\"  {word}: {weight:.4f}\")\n",
        "    \n",
        "    # generate topic description（based on keywords）\n",
        "    key_terms = \", \".join(words[:5])\n",
        "    print(f\"  💡 Key terms: {key_terms}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fordocument分配Topic\n",
        "print(\"=== documentTopic分配 ===\")\n",
        "\n",
        "# convertdocument，获得Topicdistribution\n",
        "df_topics = lda_model.transform(df_tfidf)\n",
        "\n",
        "# for每document分配dominantTopic（概率mostHigh的Topic）\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"getdominantTopicID\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return -1\n",
        "    return int(np.argmax(topic_distribution.toArray()))\n",
        "\n",
        "def get_topic_probability(topic_distribution):\n",
        "    \"\"\"getdominantTopic的概率\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return 0.0\n",
        "    return float(np.max(topic_distribution.toArray()))\n",
        "\n",
        "get_dominant_topic_udf = F.udf(get_dominant_topic, IntegerType())\n",
        "get_topic_prob_udf = F.udf(get_topic_probability, DoubleType())\n",
        "\n",
        "# addingdominantTopiccolumns\n",
        "df_topics = df_topics.withColumn(\n",
        "    \"dominant_topic\", \n",
        "    get_dominant_topic_udf(F.col(\"topic_distribution\"))\n",
        ").withColumn(\n",
        "    \"topic_probability\",\n",
        "    get_topic_prob_udf(F.col(\"topic_distribution\"))\n",
        ")\n",
        "\n",
        "print(\"✅ Topic分配completed\")\n",
        "\n",
        "# displayTopicdistribution统计\n",
        "print(\"\\nTopicdistribution统计:\")\n",
        "topic_dist = df_topics.groupBy(\"dominant_topic\").count().orderBy(\"dominant_topic\")\n",
        "topic_dist.show()\n",
        "\n",
        "# Convert to Pandas for detailed analysis\n",
        "topic_dist_pd = topic_dist.toPandas()\n",
        "total_docs = topic_dist_pd['count'].sum()\n",
        "topic_dist_pd['percentage'] = (topic_dist_pd['count'] / total_docs * 100).round(2)\n",
        "\n",
        "print(\"\\ndetailedTopicdistribution:\")\n",
        "for _, row in topic_dist_pd.iterrows():\n",
        "    topic_id = int(row['dominant_topic'])\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    \n",
        "    # getTopickey词\n",
        "    if topic_id >= 0 and topic_id < len(topic_words):\n",
        "        topic_keywords = \", \".join(topic_words[topic_id]['words'][:3])\n",
        "    else:\n",
        "        topic_keywords = \"undefined\"\n",
        "    \n",
        "    print(f\"Topic {topic_id} ({topic_keywords}): {count:,} document ({pct}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyze每topics下的情感distribution\n",
        "print(\"=== Topicsentiment analysis ===\")\n",
        "\n",
        "# Check if VADER sentiment scores exist\n",
        "sentiment_cols = [col for col in df_topics.columns if 'vader' in col.lower() or \n",
        "                 col in ['sentiment', 'compound_score', 'pos_score', 'neu_score', 'neg_score']]\n",
        "\n",
        "print(f\"Found sentiment-related columns: {sentiment_cols}\")\n",
        "\n",
        "# ifhaveVADERscore，usingVADER；otherwiseusing原始sentiment\n",
        "if 'compound_score' in df_topics.columns:\n",
        "    sentiment_col = 'compound_score'\n",
        "    print(\"usingVADER compound scoreperformsentiment analysis\")\n",
        "elif 'sentiment' in df_topics.columns:\n",
        "    sentiment_col = 'sentiment'\n",
        "    print(\"using原始sentiment scoreperformsentiment analysis\")\n",
        "else:\n",
        "    print(\"❌ Sentiment score column not found，skipsentiment analysis\")\n",
        "    sentiment_col = None\n",
        "\n",
        "if sentiment_col:\n",
        "    # definition情感classificationfunction\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"Unknown\"\n",
        "        elif score > 0.1:\n",
        "            return \"Positive\"\n",
        "        elif score < -0.1:\n",
        "            return \"Negative\"\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    # adding情感classificationcolumns\n",
        "    df_topic_sentiment = df_topics.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(sentiment_col))\n",
        "    )\n",
        "    \n",
        "    # Calculating每topics的情感distribution\n",
        "    topic_sentiment_dist = df_topic_sentiment.groupBy(\"dominant_topic\", \"sentiment_label\").count().orderBy(\"dominant_topic\", \"sentiment_label\")\n",
        "    \n",
        "    print(\"\\neachTopic情感distribution:\")\n",
        "    topic_sentiment_dist.show()\n",
        "    \n",
        "    # Convert to pivot table format for analysis\n",
        "    topic_sentiment_pd = topic_sentiment_dist.toPandas()\n",
        "    pivot_sentiment = topic_sentiment_pd.pivot(index='dominant_topic', columns='sentiment_label', values='count').fillna(0)\n",
        "    \n",
        "    # Calculate proportions\n",
        "    pivot_sentiment_pct = pivot_sentiment.div(pivot_sentiment.sum(axis=1), axis=0) * 100\n",
        "    \n",
        "    print(\"\\neachTopic情感distribution比例(%)：\")\n",
        "    print(pivot_sentiment_pct.round(2))\n",
        "    \n",
        "    # analyze每topics的情感feature\n",
        "    print(\"\\n=== Topic情感featureanalyze ===\")\n",
        "    for topic_id in range(NUM_TOPICS):\n",
        "        if topic_id in pivot_sentiment_pct.index:\n",
        "            row = pivot_sentiment_pct.loc[topic_id]\n",
        "            pos_pct = row.get('Positive', 0)\n",
        "            neg_pct = row.get('Negative', 0)\n",
        "            neu_pct = row.get('Neutral', 0)\n",
        "            \n",
        "            # getTopickey词\n",
        "            keywords = \", \".join(topic_words[topic_id]['words'][:5])\n",
        "            \n",
        "            # 判断TopicSentiment tendency\n",
        "            if pos_pct > neg_pct + 10:\n",
        "                tendency = \"偏Positive\"\n",
        "            elif neg_pct > pos_pct + 10:\n",
        "                tendency = \"偏Negative\"\n",
        "            else:\n",
        "                tendency = \"Neutral\"\n",
        "            \n",
        "            print(f\"\\nTopic {topic_id} ({keywords}):\")\n",
        "            print(f\"  Sentiment tendency: {tendency}\")\n",
        "            print(f\"  Positive: {pos_pct:.1f}% | Neutral: {neu_pct:.1f}% | Negative: {neg_pct:.1f}%\")\n",
        "            \n",
        "            # Calculate sentiment polarization degree\n",
        "            polarization = abs(pos_pct - neg_pct)\n",
        "            print(f\"  Sentiment polarization degree: {polarization:.1f}% ({'High' if polarization > 20 else 'in' if polarization > 10 else 'Low'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# savingTopicmodelingresults\n",
        "print(\"=== Save Results ===\")\n",
        "\n",
        "# 1. saving带Topicinformation的data\n",
        "output_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "# Select columns to save\n",
        "columns_to_save = [\n",
        "    \"id\", \"`subreddit.name`\", \"timestamp\", \"cleaned_body\", \n",
        "    \"dominant_topic\", \"topic_probability\"\n",
        "]\n",
        "\n",
        "# ifhavesentiment analysisresults，也contains进去\n",
        "if sentiment_col:\n",
        "    columns_to_save.extend([sentiment_col, \"sentiment_label\"])\n",
        "\n",
        "# Ensure columns exist before saving\n",
        "available_cols = [col for col in columns_to_save if col in df_topic_sentiment.columns]\n",
        "\n",
        "df_result = df_topic_sentiment.select(available_cols)\n",
        "df_result.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"✅ Topicanalyzeresultsalreadysavingto: {output_path}\")\n",
        "print(f\"Saved columns: {len(available_cols)}\")\n",
        "print(f\"Saved records: {df_result.count():,}\")\n",
        "\n",
        "# 2. savingTopickey词summary\n",
        "import json\n",
        "\n",
        "topic_summary = {\n",
        "    \"model_info\": {\n",
        "        \"num_topics\": NUM_TOPICS,\n",
        "        \"vocab_size\": len(count_model.vocabulary),\n",
        "        \"total_documents\": climate_count,\n",
        "        \"log_perplexity\": lda_model.logPerplexity(df_tfidf),\n",
        "        \"log_likelihood\": lda_model.logLikelihood(df_tfidf)\n",
        "    },\n",
        "    \"topics\": []\n",
        "}\n",
        "\n",
        "for topic_id, topic_data in enumerate(topic_words):\n",
        "    topic_info = {\n",
        "        \"topic_id\": topic_id,\n",
        "        \"keywords\": topic_data['words'][:10],\n",
        "        \"weights\": [float(w) for w in topic_data['weights'][:10]],\n",
        "        \"document_count\": int(topic_dist_pd[topic_dist_pd['dominant_topic'] == topic_id]['count'].iloc[0]) if topic_id in topic_dist_pd['dominant_topic'].values else 0\n",
        "    }\n",
        "    \n",
        "    # ifhave情感data，adding情感distribution\n",
        "    if sentiment_col and topic_id in pivot_sentiment_pct.index:\n",
        "        topic_info[\"sentiment_distribution\"] = {\n",
        "            \"positive\": float(pivot_sentiment_pct.loc[topic_id].get('Positive', 0)),\n",
        "            \"neutral\": float(pivot_sentiment_pct.loc[topic_id].get('Neutral', 0)),\n",
        "            \"negative\": float(pivot_sentiment_pct.loc[topic_id].get('Negative', 0))\n",
        "        }\n",
        "    \n",
        "    topic_summary[\"topics\"].append(topic_info)\n",
        "\n",
        "# savingTopicsummarytoJSONfile\n",
        "summary_path = \"/home/jovyan/work/data/processed/topic_summary.json\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(topic_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Topicsummaryalreadysavingto: {summary_path}\")\n",
        "\n",
        "print(\"\\n=== LDA Topic Modelingcompleted! ===\")\n",
        "print(f\"Identified {NUM_TOPICS} topics\")\n",
        "print(f\"Processed {climate_count:,} climate-related comments\")\n",
        "print(\"Topicanalyzeresults可forsubsequent的classificationmodeling\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
