{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Step 4: LDA Topic Modeling (Topic Modeling with LDA)\n",
        "\n",
        "## Objectives of this notebook:\n",
        "1. Load cleaned data\n",
        "2. Perform topic extraction using LDA\n",
        "3. Analyze topic keywords\n",
        "4. Analyze sentiment distribution for each topic\n",
        "5. Topic visualization and analysis\n",
        "\n",
        "## Analysis Focus\n",
        "- Identify main discussion topics related to climate change\n",
        "- Analyze sentiment tendencies for different topics\n",
        "- Generate topic keyword lists\n",
        "- Topic distribution visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import CountVectorizer, IDF, VectorAssembler\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set chart style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.0\n",
            "Available cores: 20\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_TopicModeling\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Sentiment analysis data loaded successfully, total 459,171 records\n",
            "\n",
            "Data structure:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- subreddit.name: string (nullable = true)\n",
            " |-- created_utc: long (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- cleaned_body: string (nullable = true)\n",
            " |-- tokens_cleaned: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- sentiment: double (nullable = true)\n",
            " |-- vader_sentiment: double (nullable = true)\n",
            " |-- sentiment_category: string (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            "\n",
            "\n",
            "âœ… Found tokens_cleaned column, can proceed directly with topic modeling\n"
          ]
        }
      ],
      "source": [
        "# Load cleaned data and sentiment analysis results\n",
        "sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"âœ… Sentiment analysis data loaded successfully, total {record_count:,} records\")\n",
        "    \n",
        "    print(\"\\nData structure:\")\n",
        "    df_sentiment.printSchema()\n",
        "    \n",
        "    # Check if tokens_cleaned column exists\n",
        "    if 'tokens_cleaned' in df_sentiment.columns:\n",
        "        print(\"\\nâœ… Found tokens_cleaned column, can proceed directly with topic modeling\")\n",
        "    else:\n",
        "        print(\"\\nâŒ tokens_cleaned column not found, need to re-tokenize\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Data loading failed: {e}\")\n",
        "    print(\"Trying to load cleaned data...\")\n",
        "    \n",
        "    # Alternative solution: load cleaned data\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_sentiment = spark.read.parquet(cleaned_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"âœ… Cleaned data loaded successfully, total {record_count:,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Preprocessing ===\n",
            "Data size after filtering: 447,889 records\n",
            "Retention ratio: 97.5%\n",
            "Climate-related comments: 439,212 records\n",
            "Percentage of filtered data: 98.1%\n",
            "\n",
            "Final data for topic modeling: 439,212 records\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing: filter and prepare topic modeling data\n",
        "print(\"=== Data Preprocessing ===\")\n",
        "\n",
        "# 1. Filter out documents with too few tokens (improve topic quality)\n",
        "df_filtered = df_sentiment.filter(\n",
        "    F.size(F.col(\"tokens_cleaned\")) >= 5  # at least 5 words\n",
        ")\n",
        "\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"Data size after filtering: {filtered_count:,} records\")\n",
        "print(f\"Retention ratio: {filtered_count/record_count*100:.1f}%\")\n",
        "\n",
        "# 2. Further filter Climate Change related keywords to ensure topic relevance\n",
        "climate_keywords = [\n",
        "    'climate', 'warming', 'carbon', 'emission', 'greenhouse', 'temperature',\n",
        "    'fossil', 'renewable', 'energy', 'pollution', 'environment', 'sustainability',\n",
        "    'weather', 'ice', 'sea', 'level', 'drought', 'flood'\n",
        "]\n",
        "\n",
        "# Create filter condition: contains at least one climate-related word\n",
        "def contains_climate_keywords(tokens):\n",
        "    if tokens is None:\n",
        "        return False\n",
        "    tokens_lower = [token.lower() for token in tokens]\n",
        "    return any(keyword in tokens_lower for keyword in climate_keywords)\n",
        "\n",
        "contains_climate_udf = F.udf(contains_climate_keywords, BooleanType())\n",
        "\n",
        "df_climate = df_filtered.filter(\n",
        "    contains_climate_udf(F.col(\"tokens_cleaned\"))\n",
        ")\n",
        "\n",
        "climate_count = df_climate.count()\n",
        "print(f\"Climate-related comments: {climate_count:,} records\")\n",
        "print(f\"Percentage of filtered data: {climate_count/filtered_count*100:.1f}%\")\n",
        "\n",
        "# Cache final data for modeling\n",
        "df_climate.cache()\n",
        "print(f\"\\nFinal data for topic modeling: {climate_count:,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Building Vocabulary Feature Vectors ===\n",
            "1. Sampling data to reduce computational burden...\n",
            "Data size after sampling: 132,035 records (30.0% of original data)\n",
            "2. Training CountVectorizer...\n",
            "3. Training IDF...\n",
            "Vocabulary size: 2000\n",
            "Feature vector dimensions: 2000\n",
            "\n",
            "Vocabulary examples (first 20 words):\n",
            "0: climate\n",
            "1: change\n",
            "2: people\n",
            "3: like\n",
            "4: re\n",
            "5: think\n",
            "6: one\n",
            "7: even\n",
            "8: .\n",
            "9: m\n",
            "10: get\n",
            "11: change.\n",
            "12: also\n",
            "13: going\n",
            "14: us\n",
            "15: much\n",
            "16: make\n",
            "17: world\n",
            "18: need\n",
            "19: global\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary feature vectors\n",
        "print(\"=== Building Vocabulary Feature Vectors ===\")\n",
        "\n",
        "# 1. Sample data to reduce memory pressure\n",
        "print(\"1. Sampling data to reduce computational burden...\")\n",
        "sample_fraction = 0.3  # Use 30% of data for topic modeling\n",
        "df_sample = df_climate.sample(fraction=sample_fraction, seed=42)\n",
        "sample_count = df_sample.count()\n",
        "print(f\"Data size after sampling: {sample_count:,} records ({sample_fraction*100}% of original data)\")\n",
        "\n",
        "# 2. CountVectorizer: Convert tokens to term frequency vectors\n",
        "# Set smaller vocabulary size and higher minimum word frequency to avoid memory issues\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=2000,  # Reduce vocabulary size\n",
        "    minDF=10.0       # Increase minimum document frequency\n",
        ")\n",
        "\n",
        "print(\"2. Training CountVectorizer...\")\n",
        "count_model = count_vectorizer.fit(df_sample)\n",
        "df_vectorized = count_model.transform(df_sample)\n",
        "\n",
        "# 3. TF-IDF: Calculate word importance weights\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "print(\"3. Training IDF...\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"Vocabulary size: {len(count_model.vocabulary)}\")\n",
        "print(f\"Feature vector dimensions: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# Display some example words in vocabulary\n",
        "print(\"\\nVocabulary examples (first 20 words):\")\n",
        "for i, word in enumerate(count_model.vocabulary[:20]):\n",
        "    print(f\"{i}: {word}\")\n",
        "\n",
        "# Update climate_count to post-sampling count\n",
        "climate_count = sample_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LDA Topic Modeling ===\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "LDA.__init__() got an unexpected keyword argument 'topicsCol'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m NUM_TOPICS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Reduce number of topics\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create LDA model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m lda \u001b[38;5;241m=\u001b[39m \u001b[43mLDA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopicsCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic_distribution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TOPICS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduce iteration count to speed up training\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting LDA model training (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_TOPICS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m topics)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mfit(df_tfidf)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: LDA.__init__() got an unexpected keyword argument 'topicsCol'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 54022)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# LDA Topic Modeling\n",
        "print(\"=== LDA Topic Modeling ===\")\n",
        "\n",
        "# Set number of topics (reduce number of topics to fit smaller dataset)\n",
        "NUM_TOPICS = 5  # Reduce number of topics\n",
        "\n",
        "# Create LDA model\n",
        "lda = LDA(\n",
        "    featuresCol=\"features\", \n",
        "    topicsCol=\"topic_distribution\",\n",
        "    k=NUM_TOPICS,\n",
        "    maxIter=10,  # Reduce iteration count to speed up training\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"Starting LDA model training ({NUM_TOPICS} topics)...\")\n",
        "lda_model = lda.fit(df_tfidf)\n",
        "\n",
        "print(\"\\nâœ… LDA model training completed!\")\n",
        "print(f\"Model perplexity: {lda_model.logPerplexity(df_tfidf):.2f}\")\n",
        "print(f\"Model log likelihood: {lda_model.logLikelihood(df_tfidf):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and analyze topic keywords\n",
        "print(\"=== Topic Keywords Analysis ===\")\n",
        "\n",
        "# Get keywords for each topic\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
        "vocabulary = count_model.vocabulary\n",
        "\n",
        "def get_topic_words(topic_data):\n",
        "    \"\"\"Convert topic word indices to actual words\"\"\"\n",
        "    topics_list = []\n",
        "    \n",
        "    for row in topic_data.collect():\n",
        "        topic_id = row['topic']\n",
        "        term_indices = row['termIndices']\n",
        "        term_weights = row['termWeights']\n",
        "        \n",
        "        # Convert indices to words\n",
        "        words = [vocabulary[idx] for idx in term_indices]\n",
        "        \n",
        "        topics_list.append({\n",
        "            'topic_id': topic_id,\n",
        "            'words': words,\n",
        "            'weights': term_weights\n",
        "        })\n",
        "    \n",
        "    return topics_list\n",
        "\n",
        "topic_words = get_topic_words(topics)\n",
        "\n",
        "# Display keywords for each topic\n",
        "print(\"\\n=== Topic Keywords List ===\")\n",
        "for topic in topic_words:\n",
        "    topic_id = topic['topic_id']\n",
        "    words = topic['words'][:10]  # Display top 10 keywords\n",
        "    weights = topic['weights'][:10]\n",
        "    \n",
        "    print(f\"\\nðŸ” Topic {topic_id}:\")\n",
        "    for word, weight in zip(words, weights):\n",
        "        print(f\"  {word}: {weight:.4f}\")\n",
        "    \n",
        "    # Generate topic description (based on keywords)\n",
        "    key_terms = \", \".join(words[:5])\n",
        "    print(f\"  ðŸ’¡ Key terms: {key_terms}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign topics to documents\n",
        "print(\"=== Document Topic Assignment ===\")\n",
        "\n",
        "# Transform documents to get topic distribution\n",
        "df_topics = lda_model.transform(df_tfidf)\n",
        "\n",
        "# Assign dominant topic for each document (topic with highest probability)\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"Get dominant topic ID\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return -1\n",
        "    return int(np.argmax(topic_distribution.toArray()))\n",
        "\n",
        "def get_topic_probability(topic_distribution):\n",
        "    \"\"\"Get probability of dominant topic\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return 0.0\n",
        "    return float(np.max(topic_distribution.toArray()))\n",
        "\n",
        "get_dominant_topic_udf = F.udf(get_dominant_topic, IntegerType())\n",
        "get_topic_prob_udf = F.udf(get_topic_probability, DoubleType())\n",
        "\n",
        "# Add dominant topic columns\n",
        "df_topics = df_topics.withColumn(\n",
        "    \"dominant_topic\", \n",
        "    get_dominant_topic_udf(F.col(\"topic_distribution\"))\n",
        ").withColumn(\n",
        "    \"topic_probability\",\n",
        "    get_topic_prob_udf(F.col(\"topic_distribution\"))\n",
        ")\n",
        "\n",
        "print(\"âœ… Topic assignment completed\")\n",
        "\n",
        "# Display topic distribution statistics\n",
        "print(\"\\nTopic distribution statistics:\")\n",
        "topic_dist = df_topics.groupBy(\"dominant_topic\").count().orderBy(\"dominant_topic\")\n",
        "topic_dist.show()\n",
        "\n",
        "# Convert to Pandas for detailed analysis\n",
        "topic_dist_pd = topic_dist.toPandas()\n",
        "total_docs = topic_dist_pd['count'].sum()\n",
        "topic_dist_pd['percentage'] = (topic_dist_pd['count'] / total_docs * 100).round(2)\n",
        "\n",
        "print(\"\\nDetailed topic distribution:\")\n",
        "for _, row in topic_dist_pd.iterrows():\n",
        "    topic_id = int(row['dominant_topic'])\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    \n",
        "    # Get topic keywords\n",
        "    if topic_id >= 0 and topic_id < len(topic_words):\n",
        "        topic_keywords = \", \".join(topic_words[topic_id]['words'][:3])\n",
        "    else:\n",
        "        topic_keywords = \"undefined\"\n",
        "    \n",
        "    print(f\"Topic {topic_id} ({topic_keywords}): {count:,} documents ({pct}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sentiment distribution for each topic\n",
        "print(\"=== Topic Sentiment Analysis ===\")\n",
        "\n",
        "# Check if VADER sentiment scores exist\n",
        "sentiment_cols = [col for col in df_topics.columns if 'vader' in col.lower() or \n",
        "                 col in ['sentiment', 'compound_score', 'pos_score', 'neu_score', 'neg_score']]\n",
        "\n",
        "print(f\"Found sentiment-related columns: {sentiment_cols}\")\n",
        "\n",
        "# Use VADER score if available, otherwise use original sentiment\n",
        "if 'compound_score' in df_topics.columns:\n",
        "    sentiment_col = 'compound_score'\n",
        "    print(\"Using VADER compound score for sentiment analysis\")\n",
        "elif 'sentiment' in df_topics.columns:\n",
        "    sentiment_col = 'sentiment'\n",
        "    print(\"Using original sentiment score for sentiment analysis\")\n",
        "else:\n",
        "    print(\"âŒ Sentiment score column not found, skipping sentiment analysis\")\n",
        "    sentiment_col = None\n",
        "\n",
        "if sentiment_col:\n",
        "    # Define sentiment classification function\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"Unknown\"\n",
        "        elif score > 0.1:\n",
        "            return \"Positive\"\n",
        "        elif score < -0.1:\n",
        "            return \"Negative\"\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    # Add sentiment classification column\n",
        "    df_topic_sentiment = df_topics.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(sentiment_col))\n",
        "    )\n",
        "    \n",
        "    # Calculate sentiment distribution for each topic\n",
        "    topic_sentiment_dist = df_topic_sentiment.groupBy(\"dominant_topic\", \"sentiment_label\").count().orderBy(\"dominant_topic\", \"sentiment_label\")\n",
        "    \n",
        "    print(\"\\nSentiment distribution for each topic:\")\n",
        "    topic_sentiment_dist.show()\n",
        "    \n",
        "    # Convert to pivot table format for analysis\n",
        "    topic_sentiment_pd = topic_sentiment_dist.toPandas()\n",
        "    pivot_sentiment = topic_sentiment_pd.pivot(index='dominant_topic', columns='sentiment_label', values='count').fillna(0)\n",
        "    \n",
        "    # Calculate proportions\n",
        "    pivot_sentiment_pct = pivot_sentiment.div(pivot_sentiment.sum(axis=1), axis=0) * 100\n",
        "    \n",
        "    print(\"\\nSentiment distribution percentage for each topic (%):\")\n",
        "    print(pivot_sentiment_pct.round(2))\n",
        "    \n",
        "    # Analyze sentiment characteristics for each topic\n",
        "    print(\"\\n=== Topic Sentiment Characteristics Analysis ===\")\n",
        "    for topic_id in range(NUM_TOPICS):\n",
        "        if topic_id in pivot_sentiment_pct.index:\n",
        "            row = pivot_sentiment_pct.loc[topic_id]\n",
        "            pos_pct = row.get('Positive', 0)\n",
        "            neg_pct = row.get('Negative', 0)\n",
        "            neu_pct = row.get('Neutral', 0)\n",
        "            \n",
        "            # Get topic keywords\n",
        "            keywords = \", \".join(topic_words[topic_id]['words'][:5])\n",
        "            \n",
        "            # Determine topic sentiment tendency\n",
        "            if pos_pct > neg_pct + 10:\n",
        "                tendency = \"Positive-leaning\"\n",
        "            elif neg_pct > pos_pct + 10:\n",
        "                tendency = \"Negative-leaning\"\n",
        "            else:\n",
        "                tendency = \"Neutral\"\n",
        "            \n",
        "            print(f\"\\nTopic {topic_id} ({keywords}):\")\n",
        "            print(f\"  Sentiment tendency: {tendency}\")\n",
        "            print(f\"  Positive: {pos_pct:.1f}% | Neutral: {neu_pct:.1f}% | Negative: {neg_pct:.1f}%\")\n",
        "            \n",
        "            # Calculate sentiment polarization degree\n",
        "            polarization = abs(pos_pct - neg_pct)\n",
        "            print(f\"  Sentiment polarization degree: {polarization:.1f}% ({'High' if polarization > 20 else 'Medium' if polarization > 10 else 'Low'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save topic modeling results\n",
        "print(\"=== Save Results ===\")\n",
        "\n",
        "# 1. Save data with topic information\n",
        "output_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "# Select columns to save\n",
        "columns_to_save = [\n",
        "    \"id\", \"`subreddit.name`\", \"timestamp\", \"cleaned_body\", \n",
        "    \"dominant_topic\", \"topic_probability\"\n",
        "]\n",
        "\n",
        "# If sentiment analysis results exist, include them\n",
        "if sentiment_col:\n",
        "    columns_to_save.extend([sentiment_col, \"sentiment_label\"])\n",
        "\n",
        "# Ensure columns exist before saving\n",
        "available_cols = [col for col in columns_to_save if col in df_topic_sentiment.columns]\n",
        "\n",
        "df_result = df_topic_sentiment.select(available_cols)\n",
        "df_result.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"âœ… Topic analysis results saved to: {output_path}\")\n",
        "print(f\"Saved columns: {len(available_cols)}\")\n",
        "print(f\"Saved records: {df_result.count():,}\")\n",
        "\n",
        "# 2. Save topic keywords summary\n",
        "import json\n",
        "\n",
        "topic_summary = {\n",
        "    \"model_info\": {\n",
        "        \"num_topics\": NUM_TOPICS,\n",
        "        \"vocab_size\": len(count_model.vocabulary),\n",
        "        \"total_documents\": climate_count,\n",
        "        \"log_perplexity\": lda_model.logPerplexity(df_tfidf),\n",
        "        \"log_likelihood\": lda_model.logLikelihood(df_tfidf)\n",
        "    },\n",
        "    \"topics\": []\n",
        "}\n",
        "\n",
        "for topic_id, topic_data in enumerate(topic_words):\n",
        "    topic_info = {\n",
        "        \"topic_id\": topic_id,\n",
        "        \"keywords\": topic_data['words'][:10],\n",
        "        \"weights\": [float(w) for w in topic_data['weights'][:10]],\n",
        "        \"document_count\": int(topic_dist_pd[topic_dist_pd['dominant_topic'] == topic_id]['count'].iloc[0]) if topic_id in topic_dist_pd['dominant_topic'].values else 0\n",
        "    }\n",
        "    \n",
        "    # If sentiment data exists, add sentiment distribution\n",
        "    if sentiment_col and topic_id in pivot_sentiment_pct.index:\n",
        "        topic_info[\"sentiment_distribution\"] = {\n",
        "            \"positive\": float(pivot_sentiment_pct.loc[topic_id].get('Positive', 0)),\n",
        "            \"neutral\": float(pivot_sentiment_pct.loc[topic_id].get('Neutral', 0)),\n",
        "            \"negative\": float(pivot_sentiment_pct.loc[topic_id].get('Negative', 0))\n",
        "        }\n",
        "    \n",
        "    topic_summary[\"topics\"].append(topic_info)\n",
        "\n",
        "# Save topic summary to JSON file\n",
        "summary_path = \"/home/jovyan/work/data/processed/topic_summary.json\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(topic_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Topic summary saved to: {summary_path}\")\n",
        "\n",
        "print(\"\\n=== LDA Topic Modeling Completed! ===\")\n",
        "print(f\"Identified {NUM_TOPICS} topics\")\n",
        "print(f\"Processed {climate_count:,} climate-related comments\")\n",
        "print(\"Topic analysis results are ready for subsequent classification modeling\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
