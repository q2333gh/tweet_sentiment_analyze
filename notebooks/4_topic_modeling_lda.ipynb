{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 第四步：LDA主题建模 (Topic Modeling with LDA)\n",
        "\n",
        "## 本notebook的目标：\n",
        "1. 加载清洗后的数据\n",
        "2. 使用LDA进行主题提取\n",
        "3. 分析主题关键词\n",
        "4. 分析每个主题下的情感分布\n",
        "5. 主题可视化和分析\n",
        "\n",
        "## 分析重点\n",
        "- 识别climate change相关的主要讨论主题\n",
        "- 分析不同主题的情感倾向\n",
        "- 生成主题关键词列表\n",
        "- 主题分布可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "库导入完成！\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import CountVectorizer, IDF, VectorAssembler\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"库导入完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.0\n",
            "Available cores: 20\n"
          ]
        }
      ],
      "source": [
        "# 初始化Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_TopicModeling\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 情感分析数据加载完成，共 459,171 条记录\n",
            "\n",
            "数据结构:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- subreddit.name: string (nullable = true)\n",
            " |-- created_utc: long (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- cleaned_body: string (nullable = true)\n",
            " |-- tokens_cleaned: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- sentiment: double (nullable = true)\n",
            " |-- vader_sentiment: double (nullable = true)\n",
            " |-- sentiment_category: string (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            "\n",
            "\n",
            "✅ 找到tokens_cleaned列，可以直接进行主题建模\n"
          ]
        }
      ],
      "source": [
        "# 加载清洗后的数据和情感分析结果\n",
        "sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"✅ 情感分析数据加载完成，共 {record_count:,} 条记录\")\n",
        "    \n",
        "    print(\"\\n数据结构:\")\n",
        "    df_sentiment.printSchema()\n",
        "    \n",
        "    # 检查tokens_cleaned列是否存在\n",
        "    if 'tokens_cleaned' in df_sentiment.columns:\n",
        "        print(\"\\n✅ 找到tokens_cleaned列，可以直接进行主题建模\")\n",
        "    else:\n",
        "        print(\"\\n❌ 未找到tokens_cleaned列，需要重新分词\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ 数据加载失败: {e}\")\n",
        "    print(\"尝试加载清洗后的数据...\")\n",
        "    \n",
        "    # 备选方案：加载清洗后的数据\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_sentiment = spark.read.parquet(cleaned_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"✅ 清洗后数据加载完成，共 {record_count:,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 数据预处理 ===\n",
            "过滤后数据量: 447,889 条记录\n",
            "保留比例: 97.5%\n",
            "Climate相关评论: 439,212 条记录\n",
            "占过滤后数据: 98.1%\n",
            "\n",
            "最终用于主题建模的数据: 439,212 条记录\n"
          ]
        }
      ],
      "source": [
        "# 数据预处理：过滤和准备主题建模数据\n",
        "print(\"=== 数据预处理 ===\")\n",
        "\n",
        "# 1. 过滤掉token数量过少的文档（提高主题质量）\n",
        "df_filtered = df_sentiment.filter(\n",
        "    F.size(F.col(\"tokens_cleaned\")) >= 5  # 至少5个词\n",
        ")\n",
        "\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"过滤后数据量: {filtered_count:,} 条记录\")\n",
        "print(f\"保留比例: {filtered_count/record_count*100:.1f}%\")\n",
        "\n",
        "# 2. 进一步过滤Climate Change相关关键词，确保主题相关性\n",
        "climate_keywords = [\n",
        "    'climate', 'warming', 'carbon', 'emission', 'greenhouse', 'temperature',\n",
        "    'fossil', 'renewable', 'energy', 'pollution', 'environment', 'sustainability',\n",
        "    'weather', 'ice', 'sea', 'level', 'drought', 'flood'\n",
        "]\n",
        "\n",
        "# 创建过滤条件：至少包含一个climate相关词汇\n",
        "def contains_climate_keywords(tokens):\n",
        "    if tokens is None:\n",
        "        return False\n",
        "    tokens_lower = [token.lower() for token in tokens]\n",
        "    return any(keyword in tokens_lower for keyword in climate_keywords)\n",
        "\n",
        "contains_climate_udf = F.udf(contains_climate_keywords, BooleanType())\n",
        "\n",
        "df_climate = df_filtered.filter(\n",
        "    contains_climate_udf(F.col(\"tokens_cleaned\"))\n",
        ")\n",
        "\n",
        "climate_count = df_climate.count()\n",
        "print(f\"Climate相关评论: {climate_count:,} 条记录\")\n",
        "print(f\"占过滤后数据: {climate_count/filtered_count*100:.1f}%\")\n",
        "\n",
        "# 缓存最终用于建模的数据\n",
        "df_climate.cache()\n",
        "print(f\"\\n最终用于主题建模的数据: {climate_count:,} 条记录\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 构建词汇特征向量 ===\n",
            "训练CountVectorizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 35026)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "ename": "Py4JError",
          "evalue": "An error occurred while calling o60.fit",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(\n\u001b[1;32m      7\u001b[0m     inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      8\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_features\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     vocabSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,  \u001b[38;5;66;03m# 词汇表大小\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     minDF\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m        \u001b[38;5;66;03m# 最小文档频率：词汇至少在5个文档中出现\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m训练CountVectorizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m count_model \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_climate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m df_vectorized \u001b[38;5;241m=\u001b[39m count_model\u001b[38;5;241m.\u001b[39mtransform(df_climate)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 2. TF-IDF：计算词汇重要性权重\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o60.fit"
          ]
        }
      ],
      "source": [
        "# 构建词汇特征向量\n",
        "print(\"=== 构建词汇特征向量 ===\")\n",
        "\n",
        "# 1. CountVectorizer：将tokens转换为词频向量\n",
        "# 设置较小的词汇表大小和最小词频，避免内存问题\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=5000,  # 词汇表大小\n",
        "    minDF=5.0        # 最小文档频率：词汇至少在5个文档中出现\n",
        ")\n",
        "\n",
        "print(\"训练CountVectorizer...\")\n",
        "count_model = count_vectorizer.fit(df_climate)\n",
        "df_vectorized = count_model.transform(df_climate)\n",
        "\n",
        "# 2. TF-IDF：计算词汇重要性权重\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "print(\"训练IDF...\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"词汇表大小: {len(count_model.vocabulary)}\")\n",
        "print(f\"特征向量维度: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# 显示词汇表中的一些示例词汇\n",
        "print(\"\\n词汇表示例（前20个词）:\")\n",
        "for i, word in enumerate(count_model.vocabulary[:20]):\n",
        "    print(f\"{i}: {word}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LDA主题建模\n",
        "print(\"=== LDA主题建模 ===\")\n",
        "\n",
        "# 设置主题数量\n",
        "NUM_TOPICS = 8  # 可以根据需要调整\n",
        "\n",
        "# 创建LDA模型\n",
        "lda = LDA(\n",
        "    featuresCol=\"features\", \n",
        "    topicsCol=\"topic_distribution\",\n",
        "    k=NUM_TOPICS,\n",
        "    maxIter=20,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"开始训练LDA模型（{NUM_TOPICS}个主题）...\")\n",
        "lda_model = lda.fit(df_tfidf)\n",
        "\n",
        "print(\"\\n✅ LDA模型训练完成！\")\n",
        "print(f\"模型困惑度: {lda_model.logPerplexity(df_tfidf):.2f}\")\n",
        "print(f\"模型对数似然: {lda_model.logLikelihood(df_tfidf):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 提取和分析主题关键词\n",
        "print(\"=== 主题关键词分析 ===\")\n",
        "\n",
        "# 获取每个主题的关键词\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
        "vocabulary = count_model.vocabulary\n",
        "\n",
        "def get_topic_words(topic_data):\n",
        "    \"\"\"将主题的词汇索引转换为实际词汇\"\"\"\n",
        "    topics_list = []\n",
        "    \n",
        "    for row in topic_data.collect():\n",
        "        topic_id = row['topic']\n",
        "        term_indices = row['termIndices']\n",
        "        term_weights = row['termWeights']\n",
        "        \n",
        "        # 转换索引为词汇\n",
        "        words = [vocabulary[idx] for idx in term_indices]\n",
        "        \n",
        "        topics_list.append({\n",
        "            'topic_id': topic_id,\n",
        "            'words': words,\n",
        "            'weights': term_weights\n",
        "        })\n",
        "    \n",
        "    return topics_list\n",
        "\n",
        "topic_words = get_topic_words(topics)\n",
        "\n",
        "# 显示每个主题的关键词\n",
        "print(\"\\n=== 主题关键词列表 ===\")\n",
        "for topic in topic_words:\n",
        "    topic_id = topic['topic_id']\n",
        "    words = topic['words'][:10]  # 显示前10个关键词\n",
        "    weights = topic['weights'][:10]\n",
        "    \n",
        "    print(f\"\\n🔍 主题 {topic_id}:\")\n",
        "    for word, weight in zip(words, weights):\n",
        "        print(f\"  {word}: {weight:.4f}\")\n",
        "    \n",
        "    # 生成主题描述（基于关键词）\n",
        "    key_terms = \", \".join(words[:5])\n",
        "    print(f\"  💡 关键术语: {key_terms}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 为文档分配主题\n",
        "print(\"=== 文档主题分配 ===\")\n",
        "\n",
        "# 转换文档，获得主题分布\n",
        "df_topics = lda_model.transform(df_tfidf)\n",
        "\n",
        "# 为每个文档分配主导主题（概率最高的主题）\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"获取主导主题ID\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return -1\n",
        "    return int(np.argmax(topic_distribution.toArray()))\n",
        "\n",
        "def get_topic_probability(topic_distribution):\n",
        "    \"\"\"获取主导主题的概率\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return 0.0\n",
        "    return float(np.max(topic_distribution.toArray()))\n",
        "\n",
        "get_dominant_topic_udf = F.udf(get_dominant_topic, IntegerType())\n",
        "get_topic_prob_udf = F.udf(get_topic_probability, DoubleType())\n",
        "\n",
        "# 添加主导主题列\n",
        "df_topics = df_topics.withColumn(\n",
        "    \"dominant_topic\", \n",
        "    get_dominant_topic_udf(F.col(\"topic_distribution\"))\n",
        ").withColumn(\n",
        "    \"topic_probability\",\n",
        "    get_topic_prob_udf(F.col(\"topic_distribution\"))\n",
        ")\n",
        "\n",
        "print(\"✅ 主题分配完成\")\n",
        "\n",
        "# 显示主题分布统计\n",
        "print(\"\\n主题分布统计:\")\n",
        "topic_dist = df_topics.groupBy(\"dominant_topic\").count().orderBy(\"dominant_topic\")\n",
        "topic_dist.show()\n",
        "\n",
        "# 转换为Pandas进行更详细的分析\n",
        "topic_dist_pd = topic_dist.toPandas()\n",
        "total_docs = topic_dist_pd['count'].sum()\n",
        "topic_dist_pd['percentage'] = (topic_dist_pd['count'] / total_docs * 100).round(2)\n",
        "\n",
        "print(\"\\n详细主题分布:\")\n",
        "for _, row in topic_dist_pd.iterrows():\n",
        "    topic_id = int(row['dominant_topic'])\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    \n",
        "    # 获取主题关键词\n",
        "    if topic_id >= 0 and topic_id < len(topic_words):\n",
        "        topic_keywords = \", \".join(topic_words[topic_id]['words'][:3])\n",
        "    else:\n",
        "        topic_keywords = \"未定义\"\n",
        "    \n",
        "    print(f\"主题 {topic_id} ({topic_keywords}): {count:,} 文档 ({pct}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分析每个主题下的情感分布\n",
        "print(\"=== 主题情感分析 ===\")\n",
        "\n",
        "# 检查是否有VADER情感分数\n",
        "sentiment_cols = [col for col in df_topics.columns if 'vader' in col.lower() or \n",
        "                 col in ['sentiment', 'compound_score', 'pos_score', 'neu_score', 'neg_score']]\n",
        "\n",
        "print(f\"找到的情感相关列: {sentiment_cols}\")\n",
        "\n",
        "# 如果有VADER分数，使用VADER；否则使用原始sentiment\n",
        "if 'compound_score' in df_topics.columns:\n",
        "    sentiment_col = 'compound_score'\n",
        "    print(\"使用VADER compound score进行情感分析\")\n",
        "elif 'sentiment' in df_topics.columns:\n",
        "    sentiment_col = 'sentiment'\n",
        "    print(\"使用原始sentiment score进行情感分析\")\n",
        "else:\n",
        "    print(\"❌ 未找到情感分数列，跳过情感分析\")\n",
        "    sentiment_col = None\n",
        "\n",
        "if sentiment_col:\n",
        "    # 定义情感分类函数\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"未知\"\n",
        "        elif score > 0.1:\n",
        "            return \"积极\"\n",
        "        elif score < -0.1:\n",
        "            return \"消极\"\n",
        "        else:\n",
        "            return \"中性\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    # 添加情感分类列\n",
        "    df_topic_sentiment = df_topics.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(sentiment_col))\n",
        "    )\n",
        "    \n",
        "    # 计算每个主题的情感分布\n",
        "    topic_sentiment_dist = df_topic_sentiment.groupBy(\"dominant_topic\", \"sentiment_label\").count().orderBy(\"dominant_topic\", \"sentiment_label\")\n",
        "    \n",
        "    print(\"\\n各主题情感分布:\")\n",
        "    topic_sentiment_dist.show()\n",
        "    \n",
        "    # 转换为透视表格式便于分析\n",
        "    topic_sentiment_pd = topic_sentiment_dist.toPandas()\n",
        "    pivot_sentiment = topic_sentiment_pd.pivot(index='dominant_topic', columns='sentiment_label', values='count').fillna(0)\n",
        "    \n",
        "    # 计算比例\n",
        "    pivot_sentiment_pct = pivot_sentiment.div(pivot_sentiment.sum(axis=1), axis=0) * 100\n",
        "    \n",
        "    print(\"\\n各主题情感分布比例(%)：\")\n",
        "    print(pivot_sentiment_pct.round(2))\n",
        "    \n",
        "    # 分析每个主题的情感特征\n",
        "    print(\"\\n=== 主题情感特征分析 ===\")\n",
        "    for topic_id in range(NUM_TOPICS):\n",
        "        if topic_id in pivot_sentiment_pct.index:\n",
        "            row = pivot_sentiment_pct.loc[topic_id]\n",
        "            pos_pct = row.get('积极', 0)\n",
        "            neg_pct = row.get('消极', 0)\n",
        "            neu_pct = row.get('中性', 0)\n",
        "            \n",
        "            # 获取主题关键词\n",
        "            keywords = \", \".join(topic_words[topic_id]['words'][:5])\n",
        "            \n",
        "            # 判断主题情感倾向\n",
        "            if pos_pct > neg_pct + 10:\n",
        "                tendency = \"偏积极\"\n",
        "            elif neg_pct > pos_pct + 10:\n",
        "                tendency = \"偏消极\"\n",
        "            else:\n",
        "                tendency = \"中性\"\n",
        "            \n",
        "            print(f\"\\n主题 {topic_id} ({keywords}):\")\n",
        "            print(f\"  情感倾向: {tendency}\")\n",
        "            print(f\"  积极: {pos_pct:.1f}% | 中性: {neu_pct:.1f}% | 消极: {neg_pct:.1f}%\")\n",
        "            \n",
        "            # 计算情感极化程度\n",
        "            polarization = abs(pos_pct - neg_pct)\n",
        "            print(f\"  情感极化程度: {polarization:.1f}% ({'高' if polarization > 20 else '中' if polarization > 10 else '低'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存主题建模结果\n",
        "print(\"=== 保存结果 ===\")\n",
        "\n",
        "# 1. 保存带主题信息的数据\n",
        "output_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "# 选择需要保存的列\n",
        "columns_to_save = [\n",
        "    \"id\", \"`subreddit.name`\", \"timestamp\", \"cleaned_body\", \n",
        "    \"dominant_topic\", \"topic_probability\"\n",
        "]\n",
        "\n",
        "# 如果有情感分析结果，也包含进去\n",
        "if sentiment_col:\n",
        "    columns_to_save.extend([sentiment_col, \"sentiment_label\"])\n",
        "\n",
        "# 确保列存在再保存\n",
        "available_cols = [col for col in columns_to_save if col in df_topic_sentiment.columns]\n",
        "\n",
        "df_result = df_topic_sentiment.select(available_cols)\n",
        "df_result.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"✅ 主题分析结果已保存到: {output_path}\")\n",
        "print(f\"保存列数: {len(available_cols)}\")\n",
        "print(f\"保存记录数: {df_result.count():,}\")\n",
        "\n",
        "# 2. 保存主题关键词总结\n",
        "import json\n",
        "\n",
        "topic_summary = {\n",
        "    \"model_info\": {\n",
        "        \"num_topics\": NUM_TOPICS,\n",
        "        \"vocab_size\": len(count_model.vocabulary),\n",
        "        \"total_documents\": climate_count,\n",
        "        \"log_perplexity\": lda_model.logPerplexity(df_tfidf),\n",
        "        \"log_likelihood\": lda_model.logLikelihood(df_tfidf)\n",
        "    },\n",
        "    \"topics\": []\n",
        "}\n",
        "\n",
        "for topic_id, topic_data in enumerate(topic_words):\n",
        "    topic_info = {\n",
        "        \"topic_id\": topic_id,\n",
        "        \"keywords\": topic_data['words'][:10],\n",
        "        \"weights\": [float(w) for w in topic_data['weights'][:10]],\n",
        "        \"document_count\": int(topic_dist_pd[topic_dist_pd['dominant_topic'] == topic_id]['count'].iloc[0]) if topic_id in topic_dist_pd['dominant_topic'].values else 0\n",
        "    }\n",
        "    \n",
        "    # 如果有情感数据，添加情感分布\n",
        "    if sentiment_col and topic_id in pivot_sentiment_pct.index:\n",
        "        topic_info[\"sentiment_distribution\"] = {\n",
        "            \"positive\": float(pivot_sentiment_pct.loc[topic_id].get('积极', 0)),\n",
        "            \"neutral\": float(pivot_sentiment_pct.loc[topic_id].get('中性', 0)),\n",
        "            \"negative\": float(pivot_sentiment_pct.loc[topic_id].get('消极', 0))\n",
        "        }\n",
        "    \n",
        "    topic_summary[\"topics\"].append(topic_info)\n",
        "\n",
        "# 保存主题总结到JSON文件\n",
        "summary_path = \"/home/jovyan/work/data/processed/topic_summary.json\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(topic_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ 主题总结已保存到: {summary_path}\")\n",
        "\n",
        "print(\"\\n=== LDA主题建模完成！ ===\")\n",
        "print(f\"共识别出 {NUM_TOPICS} 个主题\")\n",
        "print(f\"处理了 {climate_count:,} 条climate相关评论\")\n",
        "print(\"主题分析结果可用于后续的分类建模\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
