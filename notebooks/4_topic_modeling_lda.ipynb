{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Step 4: LDA Topic Modeling (Topic Modeling with LDA)\n",
        "\n",
        "## Objectives of this notebook:\n",
        "1. Load cleaned data\n",
        "2. Perform topic extraction using LDA\n",
        "3. Analyze topic keywords\n",
        "4. Analyze sentiment distribution for each topic\n",
        "5. Topic visualization and analysis\n",
        "\n",
        "## Analysis Focus\n",
        "- Identify mainlyly discussion topics related to climate change\n",
        "- Analyze sentiment tendencies for different topics\n",
        "- Generate topic keyword lists\n",
        "- Topic distribution visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import CountVectorizer, IDF, VectorAssembler\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set chart style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TweetAnalysis_TopicModeling\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loadingcleaned data and sentiment analysisresults\n",
        "sentiment_data_path = \"/home/jovyan/work/data/processed/sentiment_analyzed_comments.parquet\"\n",
        "\n",
        "try:\n",
        "    df_sentiment = spark.read.parquet(sentiment_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"‚úÖ sentiment analysis data loadingcompletedÔºåtotal {record_count:,} records\")\n",
        "    \n",
        "    print(\"\\nData structure:\")\n",
        "    df_sentiment.printSchema()\n",
        "    \n",
        "    # Checkingwhether tokens_cleaned column exists\n",
        "    if 'tokens_cleaned' in df_sentiment.columns:\n",
        "        print(\"\\n‚úÖ Found tokens_cleaned columnÔºåcan proceed directly with topic modeling\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå tokens_cleaned column not foundÔºåneed to re-tokenize\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Data loading failed: {e}\")\n",
        "    print(\"try loading cleaned data...\")\n",
        "    \n",
        "    # alternativesolutionÔºöloadingcleaned data\n",
        "    cleaned_data_path = \"/home/jovyan/work/data/processed/cleaned_comments.parquet\"\n",
        "    df_sentiment = spark.read.parquet(cleaned_data_path)\n",
        "    df_sentiment.cache()\n",
        "    record_count = df_sentiment.count()\n",
        "    print(f\"‚úÖ cleaned data loadingcompletedÔºåtotal {record_count:,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data preprocessingÔºöfilter and prepare topic modelingdata\n",
        "print(\"=== data preprocessing ===\")\n",
        "\n",
        "# 1. Filter out documents with too few tokensÔºàimprove topic qualityÔºâ\n",
        "df_filtered = df_sentiment.filter(\n",
        "    F.size(F.col(\"tokens_cleaned\")) >= 5  # at least 5 words\n",
        ")\n",
        "\n",
        "filtered_count = df_filtered.count()\n",
        "print(f\"Data size after filtering: {filtered_count:,} records\")\n",
        "print(f\"Retention ratio: {filtered_count/record_count*100:.1f}%\")\n",
        "\n",
        "# 2. Further filter Climate Change related keywordsÔºåensure topic relevance\n",
        "climate_keywords = [\n",
        "    'climate', 'warming', 'carbon', 'emission', 'greenhouse', 'temperature',\n",
        "    'fossil', 'renewable', 'energy', 'pollution', 'environment', 'sustainability',\n",
        "    'weather', 'ice', 'sea', 'level', 'drought', 'flood'\n",
        "]\n",
        "\n",
        "# create filter conditionÔºöcontains at least one climate-related word\n",
        "def contains_climate_keywords(tokens):\n",
        "    if tokens is None:\n",
        "        return False\n",
        "    tokens_lower = [token.lower() for token in tokens]\n",
        "    return any(keyword in tokens_lower for keyword in climate_keywords)\n",
        "\n",
        "contains_climate_udf = F.udf(contains_climate_keywords, BooleanType())\n",
        "\n",
        "df_climate = df_filtered.filter(\n",
        "    contains_climate_udf(F.col(\"tokens_cleaned\"))\n",
        ")\n",
        "\n",
        "climate_count = df_climate.count()\n",
        "print(f\"Climate-related comments: {climate_count:,} records\")\n",
        "print(f\"percentage of filtered data: {climate_count/filtered_count*100:.1f}%\")\n",
        "\n",
        "# cache final data for modeling\n",
        "df_climate.cache()\n",
        "print(f\"\\nfinal data for topic modeling: {climate_count:,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build vocabulary feature vectors\n",
        "print(\"=== Build vocabulary feature vectors ===\")\n",
        "\n",
        "# 1. sample data to reduce memory pressure\n",
        "print(\"1. ÈááÊ†∑data‰ª•reduce computational burden...\")\n",
        "sample_fraction = 0.3  # use 30% of data for topic modeling\n",
        "df_sample = df_climate.sample(fraction=sample_fraction, seed=42)\n",
        "sample_count = df_sample.count()\n",
        "print(f\"data size after sampling: {sample_count:,} records (of original data {sample_fraction*100}%)\")\n",
        "\n",
        "# 2. CountVectorizerÔºöConvert tokens to term frequency vectors\n",
        "# set smaller vocabulary size and higher minimum word frequencyÔºåavoid memory issues\n",
        "count_vectorizer = CountVectorizer(\n",
        "    inputCol=\"tokens_cleaned\", \n",
        "    outputCol=\"raw_features\",\n",
        "    vocabSize=2000,  # Reduce vocabulary size\n",
        "    minDF=10.0       # improve minimum document frequency\n",
        ")\n",
        "\n",
        "print(\"2. trainingCountVectorizer...\")\n",
        "count_model = count_vectorizer.fit(df_sample)\n",
        "df_vectorized = count_model.transform(df_sample)\n",
        "\n",
        "# 3. TF-IDFÔºöCalculate word importance weights\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "print(\"3. trainingIDF...\")\n",
        "idf_model = idf.fit(df_vectorized)\n",
        "df_tfidf = idf_model.transform(df_vectorized)\n",
        "\n",
        "print(f\"Vocabulary size: {len(count_model.vocabulary)}\")\n",
        "print(f\"Feature vector dimensions: {len(count_model.vocabulary)}\")\n",
        "\n",
        "# display some example words in vocabulary\n",
        "print(\"\\nVocabulary examplesÔºàfirst 20 wordsÔºâ:\")\n",
        "for i, word in enumerate(count_model.vocabulary[:20]):\n",
        "    print(f\"{i}: {word}\")\n",
        "\n",
        "# update climate_count to post-sampling count\n",
        "climate_count = sample_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LDA Topic Modeling\n",
        "print(\"=== LDA Topic Modeling ===\")\n",
        "\n",
        "# Set number of topicsÔºàreduce number of topics to fit smaller datasetÔºâ\n",
        "NUM_TOPICS = 5  # reduce number of topics\n",
        "\n",
        "# create LDA model\n",
        "lda = LDA(\n",
        "    featuresCol=\"features\", \n",
        "    topicsCol=\"topic_distribution\",\n",
        "    k=NUM_TOPICS,\n",
        "    maxIter=10,  # reduce iteration count to speed up training\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"start training LDA modelÔºà{NUM_TOPICS}topicsÔºâ...\")\n",
        "lda_model = lda.fit(df_tfidf)\n",
        "\n",
        "print(\"\\n‚úÖ LDA model training completed!\")\n",
        "print(f\"model perplexity: {lda_model.logPerplexity(df_tfidf):.2f}\")\n",
        "print(f\"model log likelihood: {lda_model.logLikelihood(df_tfidf):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and analyze topic keywords\n",
        "print(\"=== Topic Keywords Analysis ===\")\n",
        "\n",
        "# get keywords for each topic\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
        "vocabulary = count_model.vocabulary\n",
        "\n",
        "def get_topic_words(topic_data):\n",
        "    \"\"\"Convert topic word indices to actual words\"\"\"\n",
        "    topics_list = []\n",
        "    \n",
        "    for row in topic_data.collect():\n",
        "        topic_id = row['topic']\n",
        "        term_indices = row['termIndices']\n",
        "        term_weights = row['termWeights']\n",
        "        \n",
        "        # Convert indices to words\n",
        "        words = [vocabulary[idx] for idx in term_indices]\n",
        "        \n",
        "        topics_list.append({\n",
        "            'topic_id': topic_id,\n",
        "            'words': words,\n",
        "            'weights': term_weights\n",
        "        })\n",
        "    \n",
        "    return topics_list\n",
        "\n",
        "topic_words = get_topic_words(topics)\n",
        "\n",
        "# display keywords for each topic\n",
        "print(\"\\n=== Topic Keywords List ===\")\n",
        "for topic in topic_words:\n",
        "    topic_id = topic['topic_id']\n",
        "    words = topic['words'][:10]  # Display top 10 keywords\n",
        "    weights = topic['weights'][:10]\n",
        "    \n",
        "    print(f\"\\nüîç Topic {topic_id}:\")\n",
        "    for word, weight in zip(words, weights):\n",
        "        print(f\"  {word}: {weight:.4f}\")\n",
        "    \n",
        "    # generate topic descriptionÔºàbased on keywordsÔºâ\n",
        "    key_terms = \", \".join(words[:5])\n",
        "    print(f\"  üí° Key terms: {key_terms}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fordocumentÂàÜÈÖçTopic\n",
        "print(\"=== documentTopicÂàÜÈÖç ===\")\n",
        "\n",
        "# convertdocumentÔºåËé∑ÂæóTopicdistribution\n",
        "df_topics = lda_model.transform(df_tfidf)\n",
        "\n",
        "# forÊØèdocumentÂàÜÈÖçdominantTopicÔºàÊ¶ÇÁéámostHighÁöÑTopicÔºâ\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"getdominantTopicID\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return -1\n",
        "    return int(np.argmax(topic_distribution.toArray()))\n",
        "\n",
        "def get_topic_probability(topic_distribution):\n",
        "    \"\"\"getdominantTopicÁöÑÊ¶ÇÁéá\"\"\"\n",
        "    if topic_distribution is None:\n",
        "        return 0.0\n",
        "    return float(np.max(topic_distribution.toArray()))\n",
        "\n",
        "get_dominant_topic_udf = F.udf(get_dominant_topic, IntegerType())\n",
        "get_topic_prob_udf = F.udf(get_topic_probability, DoubleType())\n",
        "\n",
        "# addingdominantTopiccolumns\n",
        "df_topics = df_topics.withColumn(\n",
        "    \"dominant_topic\", \n",
        "    get_dominant_topic_udf(F.col(\"topic_distribution\"))\n",
        ").withColumn(\n",
        "    \"topic_probability\",\n",
        "    get_topic_prob_udf(F.col(\"topic_distribution\"))\n",
        ")\n",
        "\n",
        "print(\"‚úÖ TopicÂàÜÈÖçcompleted\")\n",
        "\n",
        "# displayTopicdistributionÁªüËÆ°\n",
        "print(\"\\nTopicdistributionÁªüËÆ°:\")\n",
        "topic_dist = df_topics.groupBy(\"dominant_topic\").count().orderBy(\"dominant_topic\")\n",
        "topic_dist.show()\n",
        "\n",
        "# Convert to Pandas for detailed analysis\n",
        "topic_dist_pd = topic_dist.toPandas()\n",
        "total_docs = topic_dist_pd['count'].sum()\n",
        "topic_dist_pd['percentage'] = (topic_dist_pd['count'] / total_docs * 100).round(2)\n",
        "\n",
        "print(\"\\ndetailedTopicdistribution:\")\n",
        "for _, row in topic_dist_pd.iterrows():\n",
        "    topic_id = int(row['dominant_topic'])\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentage']\n",
        "    \n",
        "    # getTopickeyËØç\n",
        "    if topic_id >= 0 and topic_id < len(topic_words):\n",
        "        topic_keywords = \", \".join(topic_words[topic_id]['words'][:3])\n",
        "    else:\n",
        "        topic_keywords = \"undefined\"\n",
        "    \n",
        "    print(f\"Topic {topic_id} ({topic_keywords}): {count:,} document ({pct}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyzeÊØètopics‰∏ãÁöÑÊÉÖÊÑüdistribution\n",
        "print(\"=== Topicsentiment analysis ===\")\n",
        "\n",
        "# Check if VADER sentiment scores exist\n",
        "sentiment_cols = [col for col in df_topics.columns if 'vader' in col.lower() or \n",
        "                 col in ['sentiment', 'compound_score', 'pos_score', 'neu_score', 'neg_score']]\n",
        "\n",
        "print(f\"Found sentiment-related columns: {sentiment_cols}\")\n",
        "\n",
        "# ifhaveVADERscoreÔºåusingVADERÔºõotherwiseusingÂéüÂßãsentiment\n",
        "if 'compound_score' in df_topics.columns:\n",
        "    sentiment_col = 'compound_score'\n",
        "    print(\"usingVADER compound scoreperformsentiment analysis\")\n",
        "elif 'sentiment' in df_topics.columns:\n",
        "    sentiment_col = 'sentiment'\n",
        "    print(\"usingÂéüÂßãsentiment scoreperformsentiment analysis\")\n",
        "else:\n",
        "    print(\"‚ùå Sentiment score column not foundÔºåskipsentiment analysis\")\n",
        "    sentiment_col = None\n",
        "\n",
        "if sentiment_col:\n",
        "    # definitionÊÉÖÊÑüclassificationfunction\n",
        "    def classify_sentiment(score):\n",
        "        if score is None:\n",
        "            return \"Unknown\"\n",
        "        elif score > 0.1:\n",
        "            return \"Positive\"\n",
        "        elif score < -0.1:\n",
        "            return \"Negative\"\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "    \n",
        "    classify_sentiment_udf = F.udf(classify_sentiment, StringType())\n",
        "    \n",
        "    # addingÊÉÖÊÑüclassificationcolumns\n",
        "    df_topic_sentiment = df_topics.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        classify_sentiment_udf(F.col(sentiment_col))\n",
        "    )\n",
        "    \n",
        "    # CalculatingÊØètopicsÁöÑÊÉÖÊÑüdistribution\n",
        "    topic_sentiment_dist = df_topic_sentiment.groupBy(\"dominant_topic\", \"sentiment_label\").count().orderBy(\"dominant_topic\", \"sentiment_label\")\n",
        "    \n",
        "    print(\"\\neachTopicÊÉÖÊÑüdistribution:\")\n",
        "    topic_sentiment_dist.show()\n",
        "    \n",
        "    # Convert to pivot table format for analysis\n",
        "    topic_sentiment_pd = topic_sentiment_dist.toPandas()\n",
        "    pivot_sentiment = topic_sentiment_pd.pivot(index='dominant_topic', columns='sentiment_label', values='count').fillna(0)\n",
        "    \n",
        "    # Calculate proportions\n",
        "    pivot_sentiment_pct = pivot_sentiment.div(pivot_sentiment.sum(axis=1), axis=0) * 100\n",
        "    \n",
        "    print(\"\\neachTopicÊÉÖÊÑüdistributionÊØî‰æã(%)Ôºö\")\n",
        "    print(pivot_sentiment_pct.round(2))\n",
        "    \n",
        "    # analyzeÊØètopicsÁöÑÊÉÖÊÑüfeature\n",
        "    print(\"\\n=== TopicÊÉÖÊÑüfeatureanalyze ===\")\n",
        "    for topic_id in range(NUM_TOPICS):\n",
        "        if topic_id in pivot_sentiment_pct.index:\n",
        "            row = pivot_sentiment_pct.loc[topic_id]\n",
        "            pos_pct = row.get('Positive', 0)\n",
        "            neg_pct = row.get('Negative', 0)\n",
        "            neu_pct = row.get('Neutral', 0)\n",
        "            \n",
        "            # getTopickeyËØç\n",
        "            keywords = \", \".join(topic_words[topic_id]['words'][:5])\n",
        "            \n",
        "            # Âà§Êñ≠TopicSentiment tendency\n",
        "            if pos_pct > neg_pct + 10:\n",
        "                tendency = \"ÂÅèPositive\"\n",
        "            elif neg_pct > pos_pct + 10:\n",
        "                tendency = \"ÂÅèNegative\"\n",
        "            else:\n",
        "                tendency = \"Neutral\"\n",
        "            \n",
        "            print(f\"\\nTopic {topic_id} ({keywords}):\")\n",
        "            print(f\"  Sentiment tendency: {tendency}\")\n",
        "            print(f\"  Positive: {pos_pct:.1f}% | Neutral: {neu_pct:.1f}% | Negative: {neg_pct:.1f}%\")\n",
        "            \n",
        "            # Calculate sentiment polarization degree\n",
        "            polarization = abs(pos_pct - neg_pct)\n",
        "            print(f\"  Sentiment polarization degree: {polarization:.1f}% ({'High' if polarization > 20 else 'in' if polarization > 10 else 'Low'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# savingTopicmodelingresults\n",
        "print(\"=== Save Results ===\")\n",
        "\n",
        "# 1. savingÂ∏¶TopicinformationÁöÑdata\n",
        "output_path = \"/home/jovyan/work/data/processed/topic_analyzed_comments.parquet\"\n",
        "\n",
        "# Select columns to save\n",
        "columns_to_save = [\n",
        "    \"id\", \"`subreddit.name`\", \"timestamp\", \"cleaned_body\", \n",
        "    \"dominant_topic\", \"topic_probability\"\n",
        "]\n",
        "\n",
        "# ifhavesentiment analysisresultsÔºå‰πücontainsËøõÂéª\n",
        "if sentiment_col:\n",
        "    columns_to_save.extend([sentiment_col, \"sentiment_label\"])\n",
        "\n",
        "# Ensure columns exist before saving\n",
        "available_cols = [col for col in columns_to_save if col in df_topic_sentiment.columns]\n",
        "\n",
        "df_result = df_topic_sentiment.select(available_cols)\n",
        "df_result.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"‚úÖ Topicanalyzeresultsalreadysavingto: {output_path}\")\n",
        "print(f\"Saved columns: {len(available_cols)}\")\n",
        "print(f\"Saved records: {df_result.count():,}\")\n",
        "\n",
        "# 2. savingTopickeyËØçsummary\n",
        "import json\n",
        "\n",
        "topic_summary = {\n",
        "    \"model_info\": {\n",
        "        \"num_topics\": NUM_TOPICS,\n",
        "        \"vocab_size\": len(count_model.vocabulary),\n",
        "        \"total_documents\": climate_count,\n",
        "        \"log_perplexity\": lda_model.logPerplexity(df_tfidf),\n",
        "        \"log_likelihood\": lda_model.logLikelihood(df_tfidf)\n",
        "    },\n",
        "    \"topics\": []\n",
        "}\n",
        "\n",
        "for topic_id, topic_data in enumerate(topic_words):\n",
        "    topic_info = {\n",
        "        \"topic_id\": topic_id,\n",
        "        \"keywords\": topic_data['words'][:10],\n",
        "        \"weights\": [float(w) for w in topic_data['weights'][:10]],\n",
        "        \"document_count\": int(topic_dist_pd[topic_dist_pd['dominant_topic'] == topic_id]['count'].iloc[0]) if topic_id in topic_dist_pd['dominant_topic'].values else 0\n",
        "    }\n",
        "    \n",
        "    # ifhaveÊÉÖÊÑüdataÔºåaddingÊÉÖÊÑüdistribution\n",
        "    if sentiment_col and topic_id in pivot_sentiment_pct.index:\n",
        "        topic_info[\"sentiment_distribution\"] = {\n",
        "            \"positive\": float(pivot_sentiment_pct.loc[topic_id].get('Positive', 0)),\n",
        "            \"neutral\": float(pivot_sentiment_pct.loc[topic_id].get('Neutral', 0)),\n",
        "            \"negative\": float(pivot_sentiment_pct.loc[topic_id].get('Negative', 0))\n",
        "        }\n",
        "    \n",
        "    topic_summary[\"topics\"].append(topic_info)\n",
        "\n",
        "# savingTopicsummarytoJSONfile\n",
        "summary_path = \"/home/jovyan/work/data/processed/topic_summary.json\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(topic_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Topicsummaryalreadysavingto: {summary_path}\")\n",
        "\n",
        "print(\"\\n=== LDA Topic Modelingcompleted! ===\")\n",
        "print(f\"Identified {NUM_TOPICS} topics\")\n",
        "print(f\"Processed {climate_count:,} climate-related comments\")\n",
        "print(\"TopicanalyzeresultsÂèØforsubsequentÁöÑclassificationmodeling\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
